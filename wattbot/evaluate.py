# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_evaluate.ipynb.

# %% auto 0
__all__ = ['is_numeric', 'score_answer_value', 'score_ref_id', 'score_is_na', 'calculate_wattbot_score', 'evaluate_train',
           'create_submission']

# %% ../nbs/03_evaluate.ipynb 3
import braintrust
import ast
import fastcore.all as fc
from dotenv import load_dotenv
from tqdm import tqdm
from . import retriever, eda, generator, evaluate, utils

# %% ../nbs/03_evaluate.ipynb 6
def is_numeric(value):
    try:
        float(value)
        return True
    except (ValueError, TypeError):
        return False

# %% ../nbs/03_evaluate.ipynb 8
def score_answer_value(predicted, expected):
    if expected == "is_blank":
        return 1.0 if predicted == "is_blank" else 0.0
    
    if isinstance(expected, str) and expected.startswith('['): expected = ast.literal_eval(expected)
    
    if is_numeric(expected) and is_numeric(predicted):
        pred_num, exp_num = map(float, (predicted, expected))
        return 1.0 if abs(pred_num - exp_num) <= abs(exp_num * 0.001) else 0.0
    else:
        return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0

# %% ../nbs/03_evaluate.ipynb 11
def score_ref_id(predicted, expected):
    if expected == "is_blank":
        return 1.0 if predicted == "is_blank" else 0.0
        
    if isinstance(expected, str) and expected.startswith('['): expected = ast.literal_eval(expected)
        
    pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])
    exp_set = set(expected) if isinstance(expected, list) else set([expected])
        
    intersection = len(pred_set.intersection(exp_set))
    union = len(pred_set.union(exp_set))
        
    return intersection / union if union > 0 else 0.0

# %% ../nbs/03_evaluate.ipynb 14
def score_is_na(predicted_answer, expected_answer):
    na_fields = ['answer_value', 'answer_unit', 'ref_id', 'ref_url', 'supporting_materials']
    
    expected_is_na = expected_answer['answer_value'] == 'is_blank'
    predicted_is_na = predicted_answer['answer_value'] == 'is_blank'
    
    if not expected_is_na and not predicted_is_na: return 1.0
    
    if expected_is_na and predicted_is_na:
        all_fields_blank = all(predicted_answer[field] == 'is_blank' for field in na_fields)
        return 1.0 if all_fields_blank else 0.0

    return 0.0

# %% ../nbs/03_evaluate.ipynb 18
def calculate_wattbot_score(predicted_answer, expected_row):
    answer_score = score_answer_value(predicted_answer['answer_value'], expected_row['answer_value'])
    ref_score = score_ref_id(predicted_answer['ref_id'], expected_row['ref_id'])
    na_score = score_is_na(predicted_answer, expected_row)
    score = 0.75 * answer_score + 0.15 * ref_score + 0.10 * na_score
    return fc.NS(score=score, answer_score=answer_score, ref_score=ref_score, na_score=na_score)

# %% ../nbs/03_evaluate.ipynb 23
def evaluate_train(rag, experiment_metadata, n_rc=10):
    experiment_metadata['embedding_model'] = rag.r.model
    experiment_metadata['gen_model'] = rag.model
    experiment = braintrust.init(project="wattbot_v2_evaluate", experiment="evaluation", metadata=experiment_metadata)
    result = 0
    df = eda.train()
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing Rows"):
        question = row['question']
        response = rag.answer(question, n_rc=n_rc)
        expected = eda.get_value(row).__dict__
        answer = response.ans
        wattbot_score = calculate_wattbot_score(answer, expected).__dict__
        context = list(map(lambda x: x.__dict__, response.rc))
        prompt = response.pm
        experiment.log(input=question, output=answer, expected=expected, scores=wattbot_score, metadata = {'context': context, 'prompt': prompt})
        result += wattbot_score['score']
    return result

# %% ../nbs/03_evaluate.ipynb 26
def create_submission(rag, experiment_metadata, n_rc=10):
    df = eda.test()
    questions = df['question'].to_list()
    experiment_metadata['embedding_model'] = rag.r.model
    experiment_metadata['gen_model'] = rag.model
    experiment = braintrust.init(project="wattbot_v2_test", experiment="test", metadata=experiment_metadata)
    for i, question in enumerate(tqdm(questions, desc="Answering question")):
        response = rag.answer(question, n_rc=n_rc)
        answer = response.ans
        df.loc[i, 'answer'] = str(answer['answer'])
        df.loc[i, 'answer_value'] = str(answer['answer_value'])
        df.loc[i, 'answer_unit'] = str(answer['answer_unit'])
        df.loc[i, 'ref_id'] = str(answer['ref_id'])
        df.loc[i, 'ref_url'] = str(answer['ref_url'])
        df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])
        df.loc[i, 'explanation'] = str(answer['explanation'])
        context = list(map(lambda x: x.__dict__, response.rc))
        prompt = response.pm
        experiment.log(input=question, output=answer, scores={'score': 0}, metadata = {'context': context, 'prompt': prompt})
        
    df = df.fillna('is_blank')
    df.to_csv(experiment_metadata['output_path'], index=False)

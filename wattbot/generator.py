"""Generate answer based on retrieved chunks"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_generator.ipynb.

# %% auto #0
__all__ = ['required_keys', 'template', 'system_prompt', 'create_context', 'fill_template', 'mk_msg', 'generate',
           'default_answer', 'replace_missing_keys', 'post_process', 'RAG']

# %% ../nbs/02_generator.ipynb #59bf2748-e9b9-48ad-9115-e295ea460e5d
import os
import openai
import fastcore.all as fc
import braintrust
from dotenv import load_dotenv
from . import retriever, eda, utils

# %% ../nbs/02_generator.ipynb #c815a541-4857-4049-bc94-77cd255e20c7
@fc.patch
def template(self:braintrust.logger.Prompt):
    return self.prompt.messages[0].content

# %% ../nbs/02_generator.ipynb #e606dc95-71ee-43f1-b176-86513394e5bc
@fc.patch
def system_prompt(self:braintrust.logger.Prompt): return self.prompt.messages[1].content

# %% ../nbs/02_generator.ipynb #016d57b4-14e6-49d6-b9a0-4f1a2e50e35b
def create_context(chunks):
    return "\n\n".join([retriever.Nugget(chunk, i+1) for i, chunk in enumerate(chunks)])

# %% ../nbs/02_generator.ipynb #fee2ac94-6bdd-4559-b92a-5f71f2574559
@fc.patch
def fill_template(self:braintrust.logger.Prompt,  question, chunks):
    return self.build(question=question, context=create_context(chunks))['messages'][0]['content']

# %% ../nbs/02_generator.ipynb #83b674e0-96d6-4ddf-a290-9cf1a2b15018
def mk_msg(msg, sys): return [{"role": "system", "content": sys}, {"role": "user", "content": msg}]

# %% ../nbs/02_generator.ipynb #861cd081-0e15-42ce-b0c1-b661ce61d53a
@fc.patch
def generate(self:openai.OpenAI, model, text, sys):
    return self.chat.completions.create(model=model, messages=mk_msg(text, sys), response_format={"type": "json_object"}).choices[0].message.content

# %% ../nbs/02_generator.ipynb #3dbd0091-1e08-4de0-8ce1-df86ee33467d
required_keys = ['answer', 'answer_value', 'answer_unit', 'ref_id', 'ref_url', 'supporting_materials', 'explanation']

# %% ../nbs/02_generator.ipynb #a196fdb6-cbde-49f5-bb34-81ac851c66d4
def default_answer(): return {key: 'is_blank' for key in required_keys}

# %% ../nbs/02_generator.ipynb #0cf37de8-faa4-4f84-93f6-c746233767bd
def replace_missing_keys(answer): 
    return {key: answer.get(key, 'is_blank') for key in required_keys}

# %% ../nbs/02_generator.ipynb #3e243bc3-70c7-4aa8-8b54-0e87a70b734a
def post_process(answer):
    answer = replace_missing_keys(answer)
    if len(answer['ref_id']) == 0 or answer['ref_id'] == ['is_blank']: answer['ref_id'] = 'is_blank'
    if len(answer['ref_url']) == 0 or answer['ref_url'] == ['is_blank']: answer['ref_url'] = 'is_blank'
    return answer

# %% ../nbs/02_generator.ipynb #5141221f-0ca3-4a2c-b78c-83971c1f9fff
class RAG:
    def __init__(self, r, g, model='accounts/fireworks/models/gpt-oss-20b'):
        fc.store_attr()
        self.pt = braintrust.load_prompt(project="wattbot", slug="wattbot-prompt-v2")
        self.sp = self.pt.system_prompt()
        
    def answer(self, q, n_rc=10, json=True):
        rc = self.r.search(q, n=n_rc)
        pm = self.pt.fill_template(q, rc)
        try:
            ans = self.g.generate(self.model, pm, self.sp)
            if json: ans = fc.loads(ans)
        except Exception as e:
            print(e)
            ans = default_answer()
        if json: ans = post_process(ans)
        return fc.NS(ans=ans, rc=rc, pm=pm)

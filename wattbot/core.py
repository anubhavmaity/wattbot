"""Python library for experimentating faster with Wattbot dataset"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/xx_sdk_markdown.ipynb.

# %% auto #0
__all__ = ['Chunk', 'DocumentProcessor', 'process_docs', 'Embedder', 'HybridSearch', 'Generator']

# %% ../nbs/xx_sdk_markdown.ipynb #d045ce9e-068b-40f1-90da-38be00db4287
import requests
import time
import os
import openai
import pandas as pd
import fastcore.all as fc
import contextkit.read as rd
import fireworks as fw
import braintrust
import time
import numpy as np

import ast
from types import SimpleNamespace
from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker
from fastembed import SparseTextEmbedding
from rank_bm25 import BM25Okapi
from sklearn.metrics.pairwise import cosine_similarity
from langchain_text_splitters import MarkdownTextSplitter

# %% ../nbs/xx_sdk_markdown.ipynb #b094edd8-954a-489f-b243-80329cec5c4f
class Chunk(fc.NS):
    def __init__(self, chunk_id=None, doc_id=None, text=None, **metadata):
        super().__init__(chunk_id=chunk_id, doc_id=doc_id, text=text, **metadata)
    
    def __repr__(self):
        meta = "\n".join([f"\033[1;35m{k}\033[0m: {str(v)[:300]}" for k, v in self.__dict__.items()])
        return f"{meta}"

# %% ../nbs/xx_sdk_markdown.ipynb #85861eec-91c1-416e-908f-a6165a276980
class DocumentProcessor:
    def __init__(self, path='../data', chunk_size=375, chunk_overlap=350):
        fc.store_attr()
        self.splitter = MarkdownTextSplitter.from_tiktoken_encoder(encoding_name="cl100k_base", chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        
    def stream(self, doc_id, metadata=None):
        md = (fc.Path(self.path)/f'markdown/{doc_id}.md').read_text()
        chunks = self.splitter.split_text(md)
        for i, c in enumerate(chunks): yield Chunk(text=c, chunk_id=i, doc_id=doc_id, **(metadata or {}))

    def process(self, doc_id, metadata=None):
        return fc.L(self.stream(doc_id, metadata=metadata))

# %% ../nbs/xx_sdk_markdown.ipynb #c3b1d59a-7695-480b-871f-9fc35bcd4d25
def process_docs(data_path='../data'):
    metadata = pd.read_csv(fc.Path(data_path)/'metadata.csv', encoding='latin-1')
    doc_proc = DocumentProcessor()
    all_chunks = metadata.apply(lambda r: doc_proc.process(r['id'], metadata=fc.attrdict(r, 'type', 'url', 'title', 'citation', 'year')), axis=1).tolist()
    return fc.L(all_chunks).concat()

# %% ../nbs/xx_sdk_markdown.ipynb #262a4093-a8b9-465c-b7f3-def0b3d3b5e8
class Embedder:
    def __init__(self, dclient, dmodel, smodel=None, bs=50):
        fc.store_attr()
    
    def create_dembs(self, texts):
        data = fc.mapped(lambda o: self.dclient.embeddings.create(model=self.dmodel, input=o), fc.chunked(texts, chunk_sz=self.bs)).attrgot('data')
        return data.concat().attrgot('embedding')

    def create_sembs(self, texts):
        if not self.smodel: raise ValueError("Sparse model not initialized")
        return list(self.smodel.embed(texts)) 

# %% ../nbs/xx_sdk_markdown.ipynb #89ac1b55-91c9-4d9e-9fa7-1150da41297a
class HybridSearch:
    def __init__(self, all_chunks, dense_embds, tokenized_corpus):
        fc.store_attr()
        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def get_context_chunks(self, idxs):
        """Get chunks with prev/next context, respecting doc boundaries"""
        seen, res = set(), []
        for i in idxs:
            doc_id = self.all_chunks[i].doc_id
            for j in [i-1, i, i+1]:
                if 0 <= j < len(self.all_chunks) and j not in seen and self.all_chunks[j].doc_id == doc_id:
                    res.append((j, self.all_chunks[j]))
                    seen.add(j)
        return fc.L(sorted(res)).itemgot(1)

    def rerank(self, query, chunks, limit=15):
        res = requests.post(
            "https://api.fireworks.ai/inference/v1/rerank",
            json={
                "model": "fireworks/qwen3-reranker-8b",
                "query": query,
                "documents": [c.text for c in chunks],
                "top_n": limit,
                "return_documents": True
            },
            headers={
                "Authorization": f"Bearer {os.environ['FIREWORKS_API_KEY']}",
                "Content-Type": "application/json"
            }
        )
        ranked_idxs = [r['index'] for r in res.json()['data']]
        return fc.L([chunks[i] for i in ranked_idxs])

    def search(self, query, limit=25):
        q_emb = embedder.create_dembs(texts=[query])[0]
        tok_q = query.lower().split()
        
        bm25_res = self.bm25.get_top_n(tok_q, range(len(self.all_chunks)), n=limit)
        dense_scores = cosine_similarity([q_emb], self.dense_embds)[0]
        dense_idxs = np.argsort(dense_scores)[-limit:][::-1].tolist()

        # idxs = list(set(bm25_res + dense_idxs))
        chunks =  self.get_context_chunks(idxs)
        return self.rerank(query, chunks)

# %% ../nbs/xx_sdk_markdown.ipynb #dda88046-6762-4a1f-aa33-ad92b06a4470
class Generator:
    def __init__(self, model, llm_client, prompt_template):
        fc.store_attr()
    
    def create_context(self, retrieved_chunks):
        return "\n\n".join([f"Doc ID: {chunk['doc_id']}\nURL: {chunk['url']}\nTitle: {chunk['title']}\nText: {chunk['text']}" for chunk in retrieved_chunks])
    
    def format_prompt(self, query, context):
        return self.prompt_template.build(context=context, question=query)
    
    def generate(self, query, retrieved_chunks):
        context = self.create_context(retrieved_chunks)
        formatted_prompt = self.format_prompt(query, context)
        # response_format={"type": "json_object"}
        response = self.llm_client.chat.completions.create(model=self.model, messages=formatted_prompt['messages'], response_format={"type": "json_object"})
        time.sleep(1)
        return fc.loads(response.choices[0].message.content)

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_retriever.ipynb.

# %% auto #0
__all__ = ['get_metadata', 'read_doc', 'read_markdown', 'get_content_metadata', 'chunk_doc', 'chunk_markdown', 'Nugget',
           'chunk_all', 'Chunks', 'tokenize', 'bm25chunks', 'LexicalSearch', 'embed', 'embed_chunks', 'SemanticSearch',
           'combine_chunks', 'rerank_chunks', 'HybridSearch']

# %% ../nbs/01_retriever.ipynb #a86386bb-3768-4bdb-851d-beb929196850
import toolslm as tlm
import os
import openai, numpy as np
from . import eda, utils
import fastcore.all as fc
import contextkit.read as rd
from rank_bm25 import BM25Okapi
from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity
from langchain_text_splitters import MarkdownTextSplitter

# %% ../nbs/01_retriever.ipynb #a84952e2-70a2-4889-a13f-d675cc3aa145
def get_metadata(doc_id:str) -> str:
    """Returns the metadata for a given doc_id"""
    meta = eda.metadata()
    return meta[meta['id'] == doc_id].iloc[0].to_dict()

# %% ../nbs/01_retriever.ipynb #9e4d3bb5-2bcd-43e8-b02a-d0d7f784b51e
def read_doc(doc_id:str) -> str: 
    """Returns the content of the pdf along with its metadata for a given doc_id"""
    meta = get_metadata(doc_id)
    content = rd.read_pdf(fc.Path(eda.data_path)/f'{doc_id}.pdf')
    return fc.NS(content=content, **meta)

# %% ../nbs/01_retriever.ipynb #4d66bfa4-a022-43a4-bc8e-295548c01ce6
def read_markdown(doc_id:str) -> str:
    """Returns the markdown content given the doc_id"""
    meta = get_metadata(doc_id)
    md = (fc.Path(eda.data_path)/f'markdown/{doc_id}.md').read_text()
    return fc.NS(content=tlm.download.clean_md(md), **meta)

# %% ../nbs/01_retriever.ipynb #eb27563f-4bac-4217-a0d8-bdace6f45e11
def get_content_metadata(fn, doc_id):
    doc = fn(doc_id)
    content = doc.__dict__.pop('content')
    return content, doc.__dict__

# %% ../nbs/01_retriever.ipynb #e21a4a59-9bc7-44ee-b4b6-319a78ffb66b
def chunk_doc(doc_id:str, start_id:int=0, chunk_size:int=1500, step:int=1400) -> list:
    """Chunks the content of a doc given the doc_id"""
    content, metadata = get_content_metadata(read_doc, doc_id)
    def _chunk(x): return fc.NS(text = content[x[-1]: x[-1] + chunk_size],  chunk_id=x[0] + start_id, **metadata)
    return fc.L.range(0, len(content), step).enumerate().map(_chunk)

# %% ../nbs/01_retriever.ipynb #a69c39a3-916b-4b9b-bf79-166d322705f8
@fc.patch
def chunk_markdown(self:MarkdownTextSplitter, doc_id:str, start_id:int=0):
    """Chunks the markdown of a doc given the doc_id"""
    content, metadata = get_content_metadata(read_markdown, doc_id)
    chunks = fc.L(self.split_text(content))
    return chunks.enumerate().map(lambda x: fc.NS(text = x[1],  chunk_id=x[0] + start_id, **metadata))

# %% ../nbs/01_retriever.ipynb #aa6e21fd-7810-4582-b61f-a735d4fade3d
def Nugget(chunk:object, chunk_no:int=0) -> str:
    """Returns the chunk in a readable format"""
    return f"""### Chunk {chunk_no}
            Text: {chunk['text']}
            Chunk Id: {chunk['chunk_id']}
            Doc ID: {chunk['id']}
            Type: {chunk['type']}
            Title: {chunk['title']}
            Year: {chunk['year']}
            Citation: {chunk['citation']}
            URL: {chunk['url']}"""

# %% ../nbs/01_retriever.ipynb #5efbdf95-e6f2-4f4c-9d11-4b85a65421ee
def chunk_all(fn) -> list:
    """Chunk contents of all docs"""
    doc_ids = eda.metadata()['id'].tolist()
    start_id, all_chunks = 0, fc.L()
    for doc_id in doc_ids:
        chunks = fn(doc_id, start_id=len(all_chunks))
        all_chunks.extend(chunks)
    return all_chunks

# %% ../nbs/01_retriever.ipynb #65ea4355-bfb4-4435-ab75-ae07eeb7cb2e
class Chunks:
    def __init__(self, all_chunks):
        fc.store_attr()

    def get_chunk(self, chunk_id):
        return self.all_chunks.filter(lambda x: x.chunk_id==chunk_id)[0]

    def get_neighbours(self, chunk_id):
        left_chunk, right_chunk = None, None
        left_chunk_id, right_chunk_id = chunk_id - 1, chunk_id + 1
        if left_chunk_id >= 0: left_chunk = self.get_chunk(left_chunk_id)
        if right_chunk_id < len(self.all_chunks): right_chunk = self.get_chunk(right_chunk_id)
        return left_chunk, right_chunk

    @staticmethod
    def unique(chunks):
        unique_chunk_ids = set()
        ans = fc.L()
        for c in chunks:
            if c.chunk_id not in unique_chunk_ids:
                unique_chunk_ids.add(c.chunk_id)
                ans.append(c)
        return ans

    def include_neighbours(self, chunks):
        ans = fc.L()
        for chunk in chunks:
            left_chunk, right_chunk = self.get_neighbours(chunk.chunk_id)
            if left_chunk: ans.append(left_chunk)
            ans.append(chunk)
            if right_chunk: ans.append(right_chunk)
        return self.unique(ans)

# %% ../nbs/01_retriever.ipynb #828419ae-a339-4da7-984f-97dfbcfe3f18
def tokenize(query): return query.lower().split()

# %% ../nbs/01_retriever.ipynb #c61b90b9-1e47-4b5e-bd84-c3c4249296f4
def bm25chunks(chunks:object) -> object: 
    """Indexes the chunks to BM250kapi"""
    return BM25Okapi([tokenize(t) for t in chunks.attrgot('text')])

# %% ../nbs/01_retriever.ipynb #f0aefc93-31c2-417f-946a-4793085b3843
class LexicalSearch:
    def __init__(self, chunks, tokenize_func=tokenize, neighbour_chunks=False):
        fc.store_attr()
        self.model = 'BM25Okapi'
        self.bm25 = bm25chunks(chunks)
        
    def search(self, query, n=10):
        ans = fc.L(self.bm25.get_top_n(self.tokenize_func(query), self.chunks, n=n))
        if self.neighbour_chunks: ans = Chunks(self.chunks).include_neighbours(ans)
        return ans

# %% ../nbs/01_retriever.ipynb #e757a435-3bf7-43d8-94c7-3f5df3c36410
@fc.patch
def embed(self:openai.OpenAI, model, texts, bs=256): 
    if type(texts) == str: texts = [texts]
    texts_chunks = fc.chunked(texts, chunk_sz=bs)
    data = fc.mapped(lambda o: self.embeddings.create(input=o, model=model), texts_chunks).attrgot('data')
    return np.array(data.concat().attrgot('embedding'))

# %% ../nbs/01_retriever.ipynb #50476444-4ab9-4f61-b5e8-ddf20de8802d
def embed_chunks(chunks, model='nomic-ai/nomic-embed-text-v1.5'):
    texts = chunks.attrgot('text')
    embeddings = utils.fw().embed(model, texts)
    chunks_embeddings = fc.L(chunks, embeddings)
    return chunks_embeddings

# %% ../nbs/01_retriever.ipynb #fd235115-48b2-4e09-b0ec-f0c6433eb1d5
class SemanticSearch:
    def __init__(self, chunks, model='nomic-ai/nomic-embed-text-v1.5', neighbour_chunks=False):
        fc.store_attr()
        self.chunks_embeddings = embed_chunks(chunks, model)

    def search(self, query, n=1):
        query_embedding = utils.fw().embed(self.model, query)
        all_chunks, all_embeddings = self.chunks_embeddings
        scores = cosine_similarity(query_embedding, all_embeddings)
        best_k_ind = np.argsort(scores)[0].tolist()[::-1][:n]
        ans = all_chunks[best_k_ind]
        if self.neighbour_chunks: ans = Chunks(self.chunks).include_neighbours(ans)
        return ans

# %% ../nbs/01_retriever.ipynb #a19beec8-0620-4ab0-ab93-d132f72cad8f
def combine_chunks(chunks1:list, chunks2:list) -> list:
    "Returns unique combination of chunks 1 and chunks 2"
    res = chunks1.copy()
    seen_id = set(res.attrgot('chunk_id'))
    for ele in chunks2:
        if ele.chunk_id not in seen_id:
            res.append(ele)
            seen_id.add(ele.chunk_id)
    return res

# %% ../nbs/01_retriever.ipynb #b30fa6de-d2cf-40e3-ac66-c3794058ec7d
@fc.patch
def rerank_chunks(self:utils.Reranker, query, chunks, n=10):
    docs = chunks.attrgot('text')
    ranked_ids = self.rank(query=query, docs=docs, n=n)
    return chunks[ranked_ids]

# %% ../nbs/01_retriever.ipynb #62bf62bc-f9ac-4793-8854-77c94e5d8762
class HybridSearch:
    def __init__(self, lexical_search, semantic_search, neighbour_chunks=False):
        fc.store_attr()
        self.model = [lexical_search.model, semantic_search.model]
        self.ranker = utils.Reranker()

    def search(self, query, n=1):
        lexical_res = self.lexical_search.search(query, n=2*n)
        semantic_res = self.semantic_search.search(query, n=2*n)
        combined_chunks = combine_chunks(lexical_res, semantic_res)
        ans = self.ranker.rerank_chunks(query, combined_chunks,  n=n)
        if self.neighbour_chunks: ans = Chunks(self.lexical_search.chunks).include_neighbours(ans)
        return ans

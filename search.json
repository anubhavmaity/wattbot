[
  {
    "objectID": "submission_v2.html",
    "href": "submission_v2.html",
    "title": "Submissions",
    "section": "",
    "text": "%load_ext autoreload %autoreload 2\nimport braintrust\nimport fastcore.all as fc\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\nfrom wattbot import retriever, eda, evaluate, utils, generator\nload_dotenv()\n\nTrue\nembedding_model = 'accounts/fireworks/models/qwen3-embedding-8b'\nllm_model = 'accounts/fireworks/models/kimi-k2p5'\nall_chunks = retriever.chunk_all(retriever.chunk_doc)",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v2.html#with-lexical-search",
    "href": "submission_v2.html#with-lexical-search",
    "title": "Submissions",
    "section": "With Lexical Search",
    "text": "With Lexical Search\n\nls = retriever.LexicalSearch(all_chunks, neighbour_chunks=True)\nrag = generator.RAG(ls, utils.fw(), model=llm_model)\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'pypdf',\n    'chunking': 'character_level',\n    'chunk_size': 1500,\n    'chunk_step': 1400,\n    'retrieval': 'lexical_search'\n    'neighbour_chunks': True\n}\n\nevaluate.evaluate_train(rag, experiment_metadata, n_rc=10)\n\nProcessing Rows:   2%|‚ñà‚ñà‚ñà‚ñâ                                                                                                                                                            | 1/41 [00:02&lt;01:27,  2.19s/it]Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [01:50&lt;00:00,  2.68s/it]\n\n\n31.475\n\n\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\n\n\n\nexperiment_metadata['output_path'] = 'submission_v11.csv'\nevaluate.create_submission(rag, experiment_metadata)",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v2.html#with-semantic-search",
    "href": "submission_v2.html#with-semantic-search",
    "title": "Submissions",
    "section": "With Semantic Search",
    "text": "With Semantic Search\n\nss = retriever.SemanticSearch(all_chunks, model=embedding_model, neighbour_chunks=True)\nrag = generator.RAG(ss, utils.fw(), model=llm_model)\n\n\nrag.r.chunks_embeddings[-1].shape\n\n(1927, 4096)\n\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'pypdf',\n    'chunking': 'character_level',\n    'chunk_size': 1500,\n    'chunk_step': 1400,\n    'retrieval': 'semantic_search', \n    'neighbour_chunks': True\n}\n\n\nevaluate.evaluate_train(rag, experiment_metadata, n_rc=10)\n\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [02:45&lt;00:00,  4.02s/it]\n\n\n32.375\n\n\n\nexperiment_metadata['output_path'] = 'submission_v6.csv'\nevaluate.create_submission(rag, experiment_metadata, n_rc=10)\n\nAnswering question:   0%|                                                                                                                                                                    | 0/282 [00:00&lt;?, ?it/s]/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:109: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of U.S. data centers increased by about 4% between 2010 and 2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer'] = str(answer['answer'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:110: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer_value'] = str(answer['answer_value'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:112: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['wu2021b']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_id'] = str(answer['ref_id'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:113: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['https://arxiv.org/pdf/2108.06738']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_url'] = str(answer['ref_url'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:114: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of the US data centers increased by about 4% from 2010-2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:115: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Chunk 18 from wu2021b explicitly states that U.S. data center energy consumption increased by about 4% between 2010 and 2014, providing the exact percentage increase requested.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'explanation'] = str(answer['explanation'])\nAnswering question: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [17:44&lt;00:00,  3.77s/it]",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v2.html#hybrid-search",
    "href": "submission_v2.html#hybrid-search",
    "title": "Submissions",
    "section": "Hybrid Search",
    "text": "Hybrid Search\n\nls = retriever.LexicalSearch(all_chunks)\nss = retriever.SemanticSearch(all_chunks, model=embedding_model)\nhs = retriever.HybridSearch(ls, ss, neighbour_chunks=True)\nrag = generator.RAG(hs, utils.fw(), model=llm_model)\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'pypdf',\n    'chunking': 'character_level',\n    'chunk_size': 1500,\n    'chunk_step': 1400,\n    'retrieval': 'hybrid_search', \n    'neighbour_chunks': True\n}\n\n\nevaluate.evaluate_train(rag, experiment_metadata, n_rc=10)\n\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [05:34&lt;00:00,  8.17s/it]\n\n\n33.25\n\n\n\nexperiment_metadata['output_path'] = 'submission_v13.csv'\nevaluate.create_submission(rag, experiment_metadata, n_rc=10)\n\nAnswering question:   0%|                                                                                                                                                                    | 0/282 [00:00&lt;?, ?it/s]/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:103: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4%' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer'] = str(answer['answer'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:104: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer_value'] = str(answer['answer_value'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:106: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['wu2021b']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_id'] = str(answer['ref_id'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:107: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['https://arxiv.org/pdf/2108.06738']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_url'] = str(answer['ref_url'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:108: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '\"The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].\"' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:109: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The answer is explicitly stated in Chunk 2 of the provided context. The text directly states that U.S. data center energy consumption increased by about 4% from 2010-2014, citing Masanet et al., 2020. This is the only mention of the 2010-2014 period in the context, and it provides a clear percentage increase without requiring any calculation.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'explanation'] = str(answer['explanation'])\nAnswering question:   0%|‚ñå                                                                                                                                                           | 1/282 [00:05&lt;26:04,  5.57s/it]Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nAnswering question: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [30:46&lt;00:00,  6.55s/it]",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v3.html",
    "href": "submission_v3.html",
    "title": "Submissions",
    "section": "",
    "text": "import braintrust\nimport fastcore.all as fc\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\nfrom wattbot import retriever, eda, evaluate, utils, generator\nfrom langchain_text_splitters import MarkdownTextSplitter\nload_dotenv()\n\nTrue\nembedding_model = 'accounts/fireworks/models/qwen3-embedding-8b'\nllm_model = 'accounts/fireworks/models/kimi-k2p5'",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v3.html#hybrid-search",
    "href": "submission_v3.html#hybrid-search",
    "title": "Submissions",
    "section": "Hybrid Search",
    "text": "Hybrid Search\n\nchunk_size, chunk_overlap = 375, 350\nmd_splitter = MarkdownTextSplitter.from_tiktoken_encoder(encoding_name=\"cl100k_base\", chunk_size=chunk_size, chunk_overlap=chunk_overlap)\nall_chunks = retriever.chunk_all(md_splitter.chunk_markdown)\nls = retriever.LexicalSearch(all_chunks)\nss = retriever.SemanticSearch(all_chunks, model=embedding_model)\nhs = retriever.HybridSearch(ls, ss, neighbour_chunks=True)\nrag = generator.RAG(hs, utils.fw(), model=llm_model)\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'markdown',\n    'chunking': 'token_level',\n    'chunk_size': chunk_size,\n    'chunk_overlap': chunk_overlap,\n    'retrieval': 'hybrid_search',\n    'neighbour_chunks': True\n}\n\n\nevaluate.evaluate_train(rag, experiment_metadata, n_rc=15)\n\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [03:27&lt;00:00,  5.05s/it]\n\n\n32.75\n\n\n\nexperiment_metadata['output_path'] = 'submission_v15.csv'\nevaluate.create_submission(rag, experiment_metadata, n_rc=15)\n\nAnswering question:   0%|                                                                                                                                                                    | 0/282 [00:00&lt;?, ?it/s]/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:103: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The average increase in U.S. data center electricity consumption between 2010 and 2014 was about 4%' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer'] = str(answer['answer'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:104: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer_value'] = str(answer['answer_value'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:106: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['wu2021b']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_id'] = str(answer['ref_id'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:107: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['https://arxiv.org/pdf/2108.06738']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_url'] = str(answer['ref_url'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:108: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '\"The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020]\"' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:109: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The text explicitly states that U.S. data center energy consumption increased by about 4% from 2010-2014, citing Masanet et al., 2020. This represents the average increase over that 4-year period.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'explanation'] = str(answer['explanation'])\nAnswering question: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [24:55&lt;00:00,  5.30s/it]",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "evaluate.html",
    "href": "evaluate.html",
    "title": "Eval",
    "section": "",
    "text": "load_dotenv()\n\nTrue",
    "crumbs": [
      "Eval"
    ]
  },
  {
    "objectID": "evaluate.html#score-answer-value",
    "href": "evaluate.html#score-answer-value",
    "title": "Eval",
    "section": "Score Answer Value",
    "text": "Score Answer Value\n\nfc.test_eq(is_numeric(\"1.0\"), True)\n\nfc.test_eq(is_numeric(\"a\"), False)\n\n\nfc.test_eq(score_answer_value(\"is_blank\", \"is_blank\"), True)\n\nfc.test_eq(score_answer_value([\"is_blank\"], \"is_blank\"), False)\n\nfc.test_eq(score_answer_value(\"ML.ENERGY Benchmark\", \"The ML.ENERGY Benchmark\"), False)\n\nfc.test_eq(score_answer_value(\"ML.ENERGY Benchmark\", \"ML.ENERGY Benchmark\"), True)\n\nfc.test_eq(score_answer_value(\"4.3\", \"4.3\"), True)\n\nfc.test_eq(score_answer_value(\"4.3\", \"4.13\"), False)",
    "crumbs": [
      "Eval"
    ]
  },
  {
    "objectID": "evaluate.html#score-ref-id",
    "href": "evaluate.html#score-ref-id",
    "title": "Eval",
    "section": "Score Ref ID",
    "text": "Score Ref ID\n\nfc.test_eq(score_ref_id(\"is_blank\", \"is_blank\"), True)\n\nfc.test_eq(score_ref_id([\"is_blank\"], \"is_blank\"), False)\n\nfc.test_eq(score_ref_id([\"patterson2021\"], [\"patterson2021\"]), True)\n\nfc.test_eq(score_ref_id([\"patterson2021\"], '[\"patterson2021\"]'), True)",
    "crumbs": [
      "Eval"
    ]
  },
  {
    "objectID": "evaluate.html#score-is_na",
    "href": "evaluate.html#score-is_na",
    "title": "Eval",
    "section": "Score is_na",
    "text": "Score is_na\n\npredicted_answer = {\n  \"answer\": \"False\",\n  \"answer_unit\": \"is_blank\",\n  \"answer_value\": \"is_blank\",\n  \"explanation\": \"Quote\",\n  \"ref_id\": \"is_blank\",\n  \"ref_url\": \"is_blank\",\n  \"supporting_materials\": \"is_blank\"\n}\n\nexpected_answer = {\n  \"answer\": \"False\",\n  \"answer_unit\": \"is_blank\",\n  \"answer_value\": \"is_blank\",\n  \"explanation\": \"Quote\",\n  \"ref_id\": \"is_blank\",\n  \"ref_url\": \"is_blank\",\n  \"supporting_materials\": \"is_blank\"\n}\n\nfc.test_eq(score_is_na(predicted_answer, expected_answer), True)\n\n\npredicted_answer = {\n  \"answer\": \"False\",\n  \"answer_unit\": \"is_blank\",\n  \"answer_value\": \"0\",\n  \"explanation\": \"Quote\",\n  \"ref_id\": \"is_blank\",\n  \"ref_url\": \"is_blank\",\n  \"supporting_materials\": \"The limited availability of this data significantly reduces transparency and accountability, thereby weakening the potential for public oversight and market responses.\"\n}\n\nfc.test_eq(score_is_na(predicted_answer, expected_answer), False)",
    "crumbs": [
      "Eval"
    ]
  },
  {
    "objectID": "evaluate.html#wattbot-score",
    "href": "evaluate.html#wattbot-score",
    "title": "Eval",
    "section": "Wattbot Score",
    "text": "Wattbot Score\n\npredicted_answer = {\n    \"answer\": \"Unanswerable from the context.\",\n    \"answer_unit\": \"MWh\",\n    \"answer_value\": \"is_blank\",\n    \"explanation\": \"is_blank\",\n    \"ref_id\": \"is_blank\",\n    \"ref_url\": \"is_blank\",\n    \"supporting_materials\": \"is_blank\"\n}\n\nexpected_answer = {\n    \"answer\": \"Unable to answer with confidence based on the ...\",\n    \"answer_unit\": \"MWh\",\n    \"answer_value\": \"is_blank\",\n    \"explanation\": \"is_blank\",\n    \"ref_id\": \"is_blank\",\n    \"ref_url\": \"is_blank\",\n    \"supporting_materials\": \"is_blank\"\n}\n\nws = calculate_wattbot_score(predicted_answer, expected_answer)\nfc.test_eq(ws.na_score, 0.0)\nfc.test_eq(ws.answer_score, 1.0)\nfc.test_eq(ws.ref_score, 1.0)\nfc.test_eq(ws.score, 0.9)\n\n\npredicted_answer = {\n    \"answer\": \"Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.\",\n    \"answer_unit\": \"is_blank\",\n    \"answer_value\": \"1\",\n    \"explanation\": \"is_blank\",\n    \"ref_id\": [\"khan2025\"],\n    \"ref_url\": [\"https://arxiv.org/pdf/2504.06307\"],\n    \"supporting_materials\": \"The proposed framework tackles energy efficiency in LLM deployment through three interconnected components: local inference optimization, the selection of energy-efficient LLMs, and a comprehensive evaluation methodology.\"\n}\n\nexpected_answer = {\n    \"answer\": \"Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.\",\n    \"answer_unit\": \"is_blank\",\n    \"answer_value\": \"1\",\n    \"explanation\": \"is_blank\",\n    \"ref_id\": [\"khan2025\"],\n    \"ref_url\": [\"https://arxiv.org/pdf/2504.06307\"],\n    \"supporting_materials\": \"The proposed framework tackles energy efficiency in LLM deployment through three interconnected components: local inference optimization, the selection of energy-efficient LLMs, and a comprehensive evaluation methodology.\"\n}\n\nws = calculate_wattbot_score(predicted_answer, expected_answer)\nfc.test_eq(ws.na_score, 1.0)\nfc.test_eq(ws.answer_score, 1.0)\nfc.test_eq(ws.ref_score, 1.0)\nfc.test_eq(ws.score, 1.0)",
    "crumbs": [
      "Eval"
    ]
  },
  {
    "objectID": "evaluate.html#evaluation",
    "href": "evaluate.html#evaluation",
    "title": "Eval",
    "section": "Evaluation",
    "text": "Evaluation\n\nall_chunks = retriever.chunk_all(retriever.chunk_doc)\nls = retriever.LexicalSearch(all_chunks)\nrag = generator.RAG(ls, utils.fw(), model='accounts/fireworks/models/kimi-k2p5')\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'pypdf',\n    'chunking': 'character_level',\n    'chunk_size': 1500,\n    'chunk_step': 1400,\n    'retrieval': 'lexical_search'\n}\n\nevaluate_train(rag, experiment_metadata)\n\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [01:35&lt;00:00,  2.33s/it]\n\n\n32.7",
    "crumbs": [
      "Eval"
    ]
  },
  {
    "objectID": "evaluate.html#test",
    "href": "evaluate.html#test",
    "title": "Eval",
    "section": "Test",
    "text": "Test\n\nexperiment_metadata['output_path'] = 'submission_v1.csv'\ncreate_submission(rag, experiment_metadata)\n\nAnswering question:   0%|                                                                                                                                                                    | 0/282 [00:00&lt;?, ?it/s]/var/folders/wh/7kgzwj895cb622q6m52_0v3r0000gn/T/ipykernel_4022/2808968084.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of U.S. data centers increased by about 4% from 2010-2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer'] = str(answer['answer'])\n/var/folders/wh/7kgzwj895cb622q6m52_0v3r0000gn/T/ipykernel_4022/2808968084.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer_value'] = str(answer['answer_value'])\n/var/folders/wh/7kgzwj895cb622q6m52_0v3r0000gn/T/ipykernel_4022/2808968084.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['wu2021b']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_id'] = str(answer['ref_id'])\n/var/folders/wh/7kgzwj895cb622q6m52_0v3r0000gn/T/ipykernel_4022/2808968084.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['https://arxiv.org/pdf/2108.06738']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_url'] = str(answer['ref_url'])\n/var/folders/wh/7kgzwj895cb622q6m52_0v3r0000gn/T/ipykernel_4022/2808968084.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of the US data centers increased by about 4% from 2010-2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])\n/var/folders/wh/7kgzwj895cb622q6m52_0v3r0000gn/T/ipykernel_4022/2808968084.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Chunk 9 from wu2021b explicitly states that U.S. data center energy consumption increased by about 4% between 2010-2014, which directly answers the question about the average increase in electricity consumption during this period.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'explanation'] = str(answer['explanation'])\nAnswering question: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [10:45&lt;00:00,  2.29s/it]",
    "crumbs": [
      "Eval"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html",
    "href": "wattbot_exploration_solveit.html",
    "title": "Exploration with Solveit",
    "section": "",
    "text": "I have the following url links and I have downloaded the pdfs in my local. I want to index the content (images/text) of the pdfs in qdrant database. https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf://arxiv.org/pdf/2405.01814://arxiv.org/pdf/2505.06371://arxiv.org/pdf/2405.21015://arxiv.org/pdf/2206.05229://arxiv.org/pdf/2410.06681://arxiv.org/pdf/2306.03163://arxiv.org/pdf/2504.17674://arxiv.org/pdf/2404.14527://arxiv.org/pdf/2412.06288://arxiv.org/pdf/2505.09598://arxiv.org/pdf/2504.06307://arxiv.org/pdf/2504.11816://arxiv.org/pdf/2309.03852://arxiv.org/pdf/2304.03271://arxiv.org/pdf/2302.08476://arxiv.org/pdf/2311.16863://arxiv.org/pdf/2501.16548://arxiv.org/pdf/2504.00797://arxiv.org/pdf/2506.15572://arxiv.org/pdf/2503.05804://arxiv.org/pdf/2104.10350://arxiv.org/pdf/2501.05899://arxiv.org/pdf/2310.03003://arxiv.org/pdf/1907.10597://arxiv.org/pdf/2404.07413://arxiv.org/pdf/2211.06318://arxiv.org/pdf/1906.02243://arxiv.org/pdf/2111.00364://arxiv.org/pdf/2108.06738://arxiv.org/pdf/2408.04693://arxiv.org/pdf/2508.14170\nAlso I have metadata of these pdfs stored in metadata.csv. I also want to index along with the content of the pdfs. Eventually I want to build a rag database\nHow shall I proceed?",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#wattbot-2025-1",
    "href": "wattbot_exploration_solveit.html#wattbot-2025-1",
    "title": "Exploration with Solveit",
    "section": "WattBot 2025",
    "text": "WattBot 2025\nOverviewDataCodeModelsDiscussionLeaderboardRulesTeamSubmissions",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#overview",
    "href": "wattbot_exploration_solveit.html#overview",
    "title": "Exploration with Solveit",
    "section": "Overview",
    "text": "Overview\nAI systems can consume vast amounts of energy and water, but reliable emissions data remains hard to find and harder to trust. In this challenge, you‚Äôll build a retrieval-augmented generation (RAG) system that extracts credible environmental impact estimates from academic articles and reports. Your model must output concise, citation-backed answers‚Äîor explicitly indicate when the evidence is missing. The goal: turn scattered academic knowledge into transparent, actionable insights for researchers, engineers, and policy makers.",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#background",
    "href": "wattbot_exploration_solveit.html#background",
    "title": "Exploration with Solveit",
    "section": "Background",
    "text": "Background\nThe environmental impact of artificial intelligence (AI) is an emerging concern in both industry and academia. As large models grow in popularity, so do their energy and resource requirements‚Äîraising questions about carbon emissions, water usage, and sustainability practices across the AI lifecycle. However, the knowledge needed to estimate or compare these impacts is often buried in technical literature, scattered across domains like machine learning, energy systems, and environmental science. Retrieval-augmented generation (RAG) offers a promising approach to extracting and synthesizing this information from unstructured documents. By combining document retrieval with generative language models, RAG systems can produce factually grounded answers supported by references.",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#goal",
    "href": "wattbot_exploration_solveit.html#goal",
    "title": "Exploration with Solveit",
    "section": "Goal",
    "text": "Goal\nIn this challenge, you‚Äôll use RAG to tackle real sustainability questions, drawing from a curated corpus of over 30 scholarly articles on AI‚Äôs environmental impact. Your system should generate responses that include:\n\nA concise natural-language answer\nref_id ‚Äì ID(s) of the cited document(s) from metadata.csv\nsupporting_materials ‚Äì Supporting materials from the retrieved reference(s) (e.g., a verbatim quote, table reference, or figure reference)\nexplanation ‚Äì Reasoning that connects the supporting materials to the final answer.\n\nIf no evidence exists, your system must emit the standardized fallback. Scores combine retrieval precision, numerical accuracy, and citation faithfulness.",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#provided-data",
    "href": "wattbot_exploration_solveit.html#provided-data",
    "title": "Exploration with Solveit",
    "section": "Provided Data",
    "text": "Provided Data\n\nmetadata.csv: Contains document IDs, titles, and full citations\ntrain_QA.csv: Sample questions with answers and references provided\ntest_Q.csv: Test questions to be used for evaluation (answers not provided). Your solution file must provide answers to these questions for evaluation.\n\n\nQuestion Types & Examples\nAll questions expect either (i) a numeric value, (ii) a specific term / concept name, or (iii) a True/False answer. Examples include:\n\nQuestion : What is the estimated CO2 emissions (in pounds) from training the BERT‚Äëbase model for 79 hours on 64 V100 GPUs?\nAnswer : 1438 lbs. Supporting refID(s): [strubel2019], Supporting materials: [Table 3].\nQuestion : True or False: New AI data centers often rely on air cooling due to high server power densities.\nAnswer : FALSE. Supporting refID(s): [li2025b], Supporting materials: [‚ÄúIn general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.‚Äù]\nQuestion : What term refers to the amount of water evaporated, transpired, or incorporated into products, defined as ‚Äúwater withdrawal minus water discharge‚Äù?\nAnswer : Water consumption. Supporting refID(s): [li2025b], Supporting materials: [‚ÄúWater consumption: It is defined as ‚Äúwater withdrawal minus water discharge‚Äù, and means the amount of water ‚Äúevaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment [13].‚Äù‚Äù]\nQuestion : How much does an elephant weigh?\nAnswer : Unable to answer with confidence based on the provided documents.\n\nRefer to to the train_QA.csv to explore other Q &A examples.\n\n\nConsiderations\n\nA small portion of questions require combining information from multiple documents (e.g., GPU power specs from one source and emissions rates from another).\nA small portion of questions involve visual reasoning ‚Äî for example, interpreting a graph in a paper to extract values. Participants are encouraged to use OCR techniques to extract information from figures, tables, or scanned pages where necessary.\nA small portion of questions are intentionally unanswerable based on the provided corpus; your system must recognize and handle these appropriately.",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#expected-output-format",
    "href": "wattbot_exploration_solveit.html#expected-output-format",
    "title": "Exploration with Solveit",
    "section": "Expected Output Format",
    "text": "Expected Output Format\nEach system prediction must include the all fields listed in train_QA.csv\n\nid : The question ID (e.g., q001)\nquestion : The original question text\nanswer : A clear natural-language response (e.g., 1438 lbs, Water consumption, TRUE)‚Äô.\n\nIf no answer is possible, use \"Unable to answer with confidence based on the provided documents.\"\n\nanswer_value : The normalized numeric or categorical value (e.g., 1438, Water consumption, 1)\n\nIf no answer is possible, use is_blank\nRanges should be encoded as [low,high]\nDo not include symbols like &lt;, &gt;, ~ here. Those can be left in the clear natural language column.\n\nanswer_unit : Unit of measurement (e.g., lbs, kWh, gCO2, projects, is_blank).\nref_id : One or more document IDs from metadata.csv that support the answer.\nref_url : One or more URL(s) of the cited document(s).\nsupporting_materials : Verbatim justification from the cited document (quote, table reference, figure reference, etc.).\nexplanation : Short reasoning describing why the cited material supports the answer.\n\nRefer to to the train_QA.csv to study the expected formatting conventions.",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#scoring",
    "href": "wattbot_exploration_solveit.html#scoring",
    "title": "Exploration with Solveit",
    "section": "Scoring",
    "text": "Scoring\nSubmissions are scored with a custom WattBot score that evaluates three fields for every question and returns a weighted accuracy between 0 ‚Äì 1 :\n\n\n\n\n\n\n\n\nComponent\nWeight\nWhat counts as correct\n\n\n\n\nanswer_value\n0.75\nMatches the ground truth. Numeric answers must be within ¬±0.1% relative tolerance; categorical values (e.g.¬†TRUE, FALSE, Water consumption) must match exactly after normalization. If a question is unanswerable, this column must contain is_blank.\n\n\nref_id\n0.15\nPartial credit via Jaccard overlap between your ref_id set and the ground-truth set (order ignored, case-insensitive). Use ‚Äòis_blank‚Äô if no evidence is available.\n\n\nis_NA\n0.10\nFor truly unanswerable questions, your submission must mark all relevant fields (answer_value, answer_unit, ref_id, ref_url, supporting_materials) as is_blank. Any other combination scores 0 for this component.\n\n\n\n\nNotes\n\nOnly the answer_value and ref_id columns will be evaluated, but full responses are still expected in the other columns. You‚Äôre encouraged to make the explanation field as useful and detailed as possible to help with debugging and calibrating your RAG pipeline. While short quotes or table references are acceptable, longer natural-language justifications and multi-sentence explanations are also encouraged.\nThe public leaderboard reports the mean WattBot Score on a subset of the test set. A hidden portion is used after the competition deadline for final rankings.",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#submitting-solution",
    "href": "wattbot_exploration_solveit.html#submitting-solution",
    "title": "Exploration with Solveit",
    "section": "Submitting solution",
    "text": "Submitting solution\nUpload a single CSV with the same columns and formatting conventions as train_QA.csv.\nSubmit the file via Make Submission. The platform will run the metric script (ignoring the explanation column for scoring), return your score, and place your team on the public leaderboard.\nQuestions and answers should:\n\nfollow the same conventions as train_QA.csv\nstick to sustainability topics surrounding AI\nbe grounded in explicit evidence (quote, table, or figure) with ref ID(s)\nbe phrased to yield short, checkable answers (numbers, ranges, or categorical names)\nto generate good Q&A pairs (that challenge the best RAG systems), we recommend crafting Q&A pairs that have one or more of the following qualities:\n\ncombine evidence across multiple papers\ninvolve visual reasoning over figures (bonus: without supporting text to back it up so it‚Äôs visual reasoning only)\ninvolve simple math/unit conversions/deductions\n\n\nSend your team‚Äôs Q &A pairs to endemann@wisc.edu by Oct 11.\nI uploaded the competition overview\n\nü§ñReplyü§ñ\nThanks for sharing the competition overview! Now I understand the WattBot 2025 scoring mechanism. It evaluates three components:\n\nanswer_value (75% weight) - Must match ground truth within ¬±0.1% for numbers, exactly for categories\nref_id (15% weight) - Jaccard overlap with ground truth reference IDs\n\nis_NA (10% weight) - Correctly identifying unanswerable questions\n\nTo evaluate your RAG system, we should test it on some questions from train_QA.csv and see how well it retrieves the right documents and extracts accurate answers.\nShould we start by testing your RAG on one question from the training set?\nSure\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs test your RAG system with the first question from the training set:\ntest_question = qa_df.iloc[0]\nprint(f\"Question: {test_question['question']}\")\nprint(f\"Expected answer: {test_question['answer']}\")\nprint(f\"Expected ref_id: {test_question['ref_id']}\")\nShould we run this to see the first test question?\n\ntest_question = qa_df.iloc[0]\nprint(f\"Question: {test_question['question']}\")\nprint(f\"Expected answer: {test_question['answer']}\")\nprint(f\"Expected ref_id: {test_question['ref_id']}\")\n\nQuestion: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\nExpected answer: The ML.ENERGY Benchmark\nExpected ref_id: ['chung2025']\n\n\nYes it works\n\n\nü§ñReplyü§ñ\nGreat! Now let‚Äôs test your RAG system on this question. We need to:\n\nQuery your Qdrant collection with this question\nSee what documents it retrieves\n\nquestion_text = test_question['question']\nquery_vector = fireworks.client.Embedding.create(\n    model=\"nomic-ai/nomic-embed-text-v1.5\", \n    input=[question_text]\n).data[0].embedding\n\nresults = client.query_points(\n    collection_name=new_collection_name, \n    query=query_vector, \n    limit=5\n)\nShould we run this to see what your RAG retrieves for this question?\n\nquestion_text = test_question['question']\nquery_vector = fireworks.client.Embedding.create(\n    model=\"nomic-ai/nomic-embed-text-v1.5\", \n    input=[question_text]\n).data[0].embedding\n\nresults = client.query_points(\n    collection_name=new_collection_name, \n    query=query_vector, \n    limit=5\n)\nlen(results.points)\n\nERROR:asyncio:Unclosed client session\nclient_session: &lt;aiohttp.client.ClientSession object&gt;\n\n\n5\n\n\nIt is giving the right answer.\n\n\nü§ñReplyü§ñ\nExcellent! Your RAG system is retrieving relevant content for the question.\nNow let‚Äôs check if it retrieved the correct document. Can you see what doc_id is in the top result?\nprint(f\"Retrieved doc_id: {results.points[0].payload['doc_id']}\")\nprint(f\"Expected doc_id: chung2025\")\nDoes it match the expected reference?\n\nresults.points[0].payload['doc_id']\n\n'chung2025'\n\n\nYes it matches the expected result\n\n\nü§ñReplyü§ñ\nPerfect! Your RAG system correctly retrieved the right document (chung2025) for this question. This means your retrieval is working well.\nTo fully evaluate your system, you‚Äôd need to: 1. Extract the actual answer from the retrieved text 2. Compare it against the expected answer ‚ÄúThe ML.ENERGY Benchmark‚Äù 3. Calculate the WattBot score\nWould you like to build a simple evaluation function to test more questions, or focus on improving a specific part of your RAG pipeline?\nDont we have to use the LLM for extracting the actual answer from the retrieved text?\n\n\nü§ñReplyü§ñ\nAbsolutely right! You need an LLM to read the retrieved text chunks and extract the specific answer to the question.\nYour current RAG system only does the retrieval part - it finds relevant documents but doesn‚Äôt generate the final answer from that context.\nWould you like to: 1. Add an LLM (like GPT or Claude) to generate answers from retrieved chunks 2. Use a local LLM 3. Try Fireworks.ai for the generation step too\nWhat‚Äôs your preference for the answer generation component?\nLets try the fireworks api for the generation step too\n\n\nü§ñReplyü§ñ\nGreat choice! Fireworks.ai has good chat/completion models for generation.\nFirst, let‚Äôs see what models are available. You can use models like accounts/fireworks/models/llama-v3p1-8b-instruct or accounts/fireworks/models/llama-v3p1-70b-instruct.\nWhich model would you prefer - the faster 8B model or the more capable 70B model?\nLets use the most capable one\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs use the 70B model for better answer quality. Here‚Äôs how to set up the generation step:\nimport fireworks.client\n\ndef generate_answer(question, retrieved_chunks, model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\"):\n    context = \"\\n\\n\".join([chunk.payload['text'] for chunk in retrieved_chunks])\n    \n    prompt = f\"\"\"Based on the following context, answer the question. If you cannot find the answer in the context, respond with \"Unable to answer with confidence based on the provided documents.\"\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n    \n    response = fireworks.client.ChatCompletion.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return response.choices[0].message.content\nShould we test this with our retrieved chunks?\nDont we have to mention the answer format with specific to the competition overview\n\n\nü§ñReplyü§ñ\nAbsolutely right! The competition has very specific output requirements. We need to modify the prompt to generate all the required fields:\n\nanswer (natural language)\nanswer_value (normalized value)\nanswer_unit (units)\nref_id (document IDs)\nsupporting_materials (quotes/references)\nexplanation (reasoning)\n\nShould we update the prompt to include these specific formatting requirements from the competition?\nYes please\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs create a prompt that generates all the required competition fields:\ndef generate_wattbot_answer(question, retrieved_chunks, model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\"):\n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nText: {chunk.payload['text']}\" for chunk in retrieved_chunks])\n    \n    prompt = f\"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\n{{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"normalized value (number, category, or 'is_blank')\",\n  \"answer_unit\": \"unit of measurement or 'is_blank'\",\n  \"ref_id\": [\"doc_id1\", \"doc_id2\"],\n  \"supporting_materials\": \"verbatim quote or reference from documents\",\n  \"explanation\": \"reasoning connecting materials to answer\"\n}}\n\nIf no answer can be found, use \"Unable to answer with confidence based on the provided documents.\" for answer and \"is_blank\" for other fields.\n\nContext: {context}\n\nQuestion: {question}\n\nResponse:\"\"\"\nShould we test this updated prompt structure?\n\nqa_df['explanation'].values\n\narray(['Quote', 'Quote', 'Table 3', 'is_blank',\n       'The &gt;40% statement is explicit in Wu. Patterson\\'s PUE numbers (1.59 vs 1.11) provide a numeric example consistent with \"more than 40% higher efficiency.\"',\n       'The paper states that one 500ml bottle is consumed for every 10 to 50 responses. Therefore, one response consumes 1/50 to 1/10 of a bottle, which is 0.02 to 0.1 bottles.',\n       'Requires calculation (75-20)', 'Quote', 'Figure', 'Quote',\n       'Table', 'Quote', 'Quote', 'Quote', 'Quote', 'Quote', 'is_blank',\n       'Figure', '1438 lbs / 99.8 lbs/day = 14.4 ', 'Figure', 'Quote',\n       'Quote', 'Quote', 'Quote', 'Figure', 'Quote', 'Quote', 'Quote',\n       'Quote', 'Quote', 'Table', 'Quote',\n       '13.8 MWh √∑ 10.7 MWh/yr ‚âà 1.3 household-years.', 'Figure, Quote',\n       'Math: wall_clock_hours ‚âà 30,000 GPUh √∑ 96 GPUs = 312.5 h; days ‚âà 312.5 √∑ 24 ‚âà 13.0 days (pretraining; alignment 60 GPUh is separate).',\n       'Quote', 'Quote', ' 48.66/5.58 ', '0.009‚Üí0.004=55.6%', 'Quote',\n       'Table'], dtype=object)\n\n\nHow to change the prompt to have the explanation in the above format?\n\n\nü§ñReplyü§ñ\nLooking at the explanation formats, they vary from simple ‚ÄúQuote‚Äù to complex calculations. Let‚Äôs update the prompt to guide the LLM to use these specific explanation styles:\nprompt = f\"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"normalized value\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n}}\n\nContext: {context}\nQuestion: {question}\nResponse:\"\"\"\nShould we test this updated format?\n\nprompt = \"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"normalized value\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"ref_url\": [\"ref_ur\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n}}\n\nContext: {context}\nQuestion: {question}\nResponse:\"\"\"\n\n\nprompt.format(context='hi', question='bye')\n\n'You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\\n\\nFor explanation field, use one of these formats:\\n- \"Quote\" - if answer comes directly from text\\n- \"Table\" or \"Figure\" - if from visual elements  \\n- \"Math: [calculation]\" - if requires computation\\n- \"is_blank\" - if unanswerable\\n\\n{\\n  \"answer\": \"natural language answer\",\\n  \"answer_value\": \"normalized value\",\\n  \"answer_unit\": \"unit or \\'is_blank\\'\",\\n  \"ref_id\": [\"doc_id\"],\\n  \"ref_url\": [\"ref_ur\"],\\n  \"supporting_materials\": \"exact quote or reference\",\\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\\n}\\n\\nContext: hi\\nQuestion: bye\\nResponse:'\n\n\n\ndef generate_wattbot_answer(question, retrieved_chunks, model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\"):\n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nText: {chunk.payload['text']}\" for chunk in retrieved_chunks])\n    return prompt.format(context=context, question=question)\n\nWhat is next?\n\n\nü§ñReplyü§ñ\nNow let‚Äôs test the complete RAG pipeline by generating an answer for our test question:\ngenerated_prompt = generate_wattbot_answer(question_text, results.points[:3])\nprint(generated_prompt[:500] + \"...\")\nShould we run this to see the formatted prompt, then send it to the Fireworks API to get the generated answer?\n\ngenerated_prompt = generate_wattbot_answer(question_text, results.points[:3])\nprint(generated_prompt[:500] + \"...\")\n\nYou are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"normalized value\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [...\n\n\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Now let‚Äôs send this prompt to the Fireworks API to get the generated answer:\nresponse = fireworks.client.ChatCompletion.create(\n    model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": generated_prompt}]\n)\n\nprint(response.choices[0].message.content)\nShould we run this to see what answer the LLM generates?\n\nresponse = fireworks.client.ChatCompletion.create(\n    model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": generated_prompt}]\n)\n\nprint(response.choices[0].message.content)\n\nERROR:asyncio:Unclosed client session\nclient_session: &lt;aiohttp.client.ClientSession object&gt;\n\n\n{\n  \"answer\": \"ML.ENERGY Benchmark\",\n  \"answer_value\": \"ML.ENERGY Benchmark\",\n  \"answer_unit\": \"is_blank\",\n  \"ref_id\": [\"chung2025\"],\n  \"ref_url\": [\"https://arxiv.org/abs/2505.06371\"],\n  \"supporting_materials\": \"The ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments\",\n  \"explanation\": \"Quote\"\n}\n\n\n\nqa_df.iloc[0]\n\nid                                                                   q003\nquestion                What is the name of the benchmark suite presen...\nanswer                                            The ML.ENERGY Benchmark\nanswer_value                                          ML.ENERGY Benchmark\nanswer_unit                                                      is_blank\nref_id                                                      ['chung2025']\nref_url                              ['https://arxiv.org/pdf/2505.06371']\nsupporting_materials    We present the ML.ENERGY Benchmark, a benchmar...\nexplanation                                                         Quote\nName: 0, dtype: object\n\n\nOkay, lgtm\n\n\nü§ñReplyü§ñ\nGreat! Your RAG system is working well - it successfully:\n\n‚úÖ Retrieved the correct document (chung2025)\n‚úÖ Generated a proper JSON response with all required fields\n‚úÖ Matched the expected answer format\n\nNow you have a complete RAG pipeline that can: - Query your Qdrant database - Generate competition-formatted answers using Fireworks AI\nWould you like to test it on a few more questions from your training set, or work on automating the evaluation process?\nLets work on automating the evaluation process\n\n\nü§ñReplyü§ñ\nGreat idea! To automate the evaluation, we need to create a function that:\n\nTakes a question from your QA dataset\nRuns it through your RAG pipeline\nCompares the generated answer with the expected answer\nCalculates the WattBot score\n\nShould we start by creating a function that processes one question end-to-end, or would you prefer to begin with the scoring logic first?\nWhatever\n\n\nü§ñReplyü§ñ\nLet‚Äôs start with creating a function that processes one question end-to-end. This will help us test the complete pipeline before building the scoring logic.\ndef evaluate_single_question(question_row):\n    # 1. Get the question\n    question = question_row['question']\n    \n    # 2. Retrieve relevant chunks\n    # 3. Generate answer\n    # 4. Return results\n    pass\nWould you like to build this step by step, or should I write the complete function for you?\nWrite the complete function\n\n\nü§ñReplyü§ñ\nHere‚Äôs the complete function to process one question through your RAG pipeline:\ndef evaluate_single_question(question_row, emb_model=\"nomic-ai/nomic-embed-text-v1.5\", llm=\"accounts/fireworks/models/llama-v3p1-70b-instruct\"):\n    question = question_row['question']\n    \n    query_vector = fireworks.client.Embedding.create(model=model, input=[question]).data[0].embedding\n    \n    results = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\n    \n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\n    formatted_prompt = prompt.format(context=context, question=question)\n    \n    response = fireworks.client.ChatCompletion.create(model=llm, messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n    \n    try:\n        generated_answer = fc.loads(response.choices[0].message.content)\n        return generated_answer\n    except:\n        return {\"error\": \"Failed to parse JSON response\"}\nShould we test this function with your first question?\n\ndef evaluate_single_question(question_row, emb_model=\"nomic-ai/nomic-embed-text-v1.5\", llm=\"accounts/fireworks/models/llama-v3p1-70b-instruct\"):\n    question = question_row['question']\n    \n    query_vector = fireworks.client.Embedding.create(model=emb_model, input=[question]).data[0].embedding\n    \n    results = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\n    \n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\n    formatted_prompt = prompt.format(context=context, question=question)\n    \n    response = fireworks.client.ChatCompletion.create(model=llm, messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n    \n    try:\n        generated_answer = fc.loads(response.choices[0].message.content)\n        return generated_answer\n    except:\n        return {\"error\": \"Failed to parse JSON response\"}\n\n\nevaluate_single_question(qa_df.iloc[7])\n\nERROR:asyncio:Unclosed client session\nclient_session: &lt;aiohttp.client.ClientSession object&gt;\nERROR:asyncio:Unclosed client session\nclient_session: &lt;aiohttp.client.ClientSession object&gt;\n\n\n{'answer': 'False',\n 'answer_value': 'False',\n 'answer_unit': 'is_blank',\n 'ref_id': ['ebert2024'],\n 'ref_url': ['https://doi.org/10.1145/nnnnnnn.nnnnnnn'],\n 'supporting_materials': 'The limited availability of this data significantly reduces transparency and accountability, thereby weakening the potential for public oversight and market responses.',\n 'explanation': 'Quote'}\n\n\nI think in the context we are not passing information like url\n\n\nü§ñReplyü§ñ\nYou‚Äôre absolutely right! The context only includes doc_id and text, but the LLM needs access to the URL and other metadata to generate the ref_url field correctly.\nLet‚Äôs update the context to include all the metadata:\ncontext = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\nShould we update the function with this richer context?\n\ndef evaluate_single_question(question_row, emb_model=\"nomic-ai/nomic-embed-text-v1.5\", llm=\"accounts/fireworks/models/llama-v3p1-70b-instruct\"):\n    question = question_row['question']\n    \n    query_vector = fireworks.client.Embedding.create(model=emb_model, input=[question]).data[0].embedding\n    \n    results = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\n    \n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\n    formatted_prompt = prompt.format(context=context, question=question)\n    \n    response = fireworks.client.ChatCompletion.create(model=llm, messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n    \n    try:\n        generated_answer = fc.loads(response.choices[0].message.content)\n        return generated_answer\n    except:\n        return {\"error\": \"Failed to parse JSON response\"}\n\n\nevaluate_single_question(qa_df.iloc[7])\n\nERROR:asyncio:Unclosed client session\nclient_session: &lt;aiohttp.client.ClientSession object&gt;\nERROR:asyncio:Unclosed client session\nclient_session: &lt;aiohttp.client.ClientSession object&gt;\n\n\n{'answer': 'False',\n 'answer_value': 'False',\n 'answer_unit': 'is_blank',\n 'ref_id': ['ebert2024'],\n 'ref_url': ['https://arxiv.org/pdf/2410.06681'],\n 'supporting_materials': 'According to the text, the AI Act restricts the disclosure of energy consumption to authorities and is not accessible to downstream providers or the general public due to confidentiality clauses.',\n 'explanation': 'Quote'}\n\n\n\nqa_df.iloc[7]\n\nid                                                                   q102\nquestion                True or False: The AI Act makes energy consump...\nanswer                                                              FALSE\nanswer_value                                                            0\nanswer_unit                                                      is_blank\nref_id                                                      ['ebert2024']\nref_url                              ['https://arxiv.org/pdf/2410.06681']\nsupporting_materials    Section 4.3 Transparency: 'Where the Act does ...\nexplanation                                                         Quote\nName: 7, dtype: object\n\n\nCan we make the function return the context along with the answer?\n\n\nü§ñReplyü§ñ\nYes! Let‚Äôs modify the function to return both the generated answer and the context used:\ndef evaluate_single_question(question_row, emb_model=\"nomic-ai/nomic-embed-text-v1.5\", llm=\"accounts/fireworks/models/llama-v3p1-70b-instruct\"):\n    # ... existing code ...\n    \n    try:\n        generated_answer = fc.loads(response.choices[0].message.content)\n        return {\"answer\": generated_answer, \"context\": context}\n    except:\n        return {\"error\": \"Failed to parse JSON response\", \"context\": context}\nShould we update the function with this change?\n\nfrom fireworks import LLM\n\nllm = LLM(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\",  deployment_type=\"auto\")\n\n\nimport os\nimport openai\n\nopenai_client = openai.OpenAI(base_url = \"https://api.fireworks.ai/inference/v1\", api_key=os.environ['FIREWORKS_API_KEY'])\n\n\ndef evaluate_single_question(question_row, emb_model=\"nomic-ai/nomic-embed-text-v1.5\"):\n    question = question_row['question']\n    \n    query_vector = openai_client.embeddings.create(model=emb_model, input=[question]).data[0].embedding\n    \n    results = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\n    \n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\n    formatted_prompt = prompt.format(context=context, question=question)\n    \n    response = llm.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n    \n    try:\n        generated_answer = fc.loads(response.choices[0].message.content)\n        return {\"answer\": generated_answer, \"context\": context}\n    except:\n        return {\"error\": \"Failed to parse JSON response\", \"context\": context}\n\n\nanswer = evaluate_single_question(qa_df.iloc[7])['answer']\nanswer\n\n{'answer': 'False',\n 'answer_value': 'False',\n 'answer_unit': 'is_blank',\n 'ref_id': ['ebert2024'],\n 'ref_url': ['https://arxiv.org/pdf/2410.06681'],\n 'supporting_materials': 'The limited availability of this data significantly reduces transparency and accountability, thereby weakening the potential for public oversight and market responses.',\n 'explanation': 'Quote'}\n\n\nNow lets implement the scoring logic\n\n\nü§ñReplyü§ñ\nPerfect! Now let‚Äôs implement the WattBot scoring logic. Based on the competition overview, we need to score three components:\n\nanswer_value (75% weight) - exact match or ¬±0.1% for numbers\nref_id (15% weight) - Jaccard overlap\nis_NA (10% weight) - correctly identifying unanswerable questions\n\nShould we start with the answer_value scoring function first?\nSure\n\n\nü§ñReplyü§ñ\nLet‚Äôs start with the answer_value scoring function. This needs to handle:\n\nNumeric values: ¬±0.1% relative tolerance\nCategorical values: exact match after normalization\n\n‚Äúis_blank‚Äù for unanswerable questions\n\ndef score_answer_value(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    # Add numeric and categorical logic here\nShould we implement the numeric comparison logic first, or the categorical matching?\nYes\n\n\nü§ñReplyü§ñ\nLet‚Äôs implement the numeric comparison logic first:\ndef score_answer_value(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    # Try numeric comparison with ¬±0.1% tolerance\n    try:\n        pred_num = float(predicted)\n        exp_num = float(expected)\n        tolerance = abs(exp_num * 0.001)  # 0.1% relative tolerance\n        return 1.0 if abs(pred_num - exp_num) &lt;= tolerance else 0.0\n    except ValueError:\n        # Fall back to categorical matching\n        return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0\nShould we test this function with a few examples?\nCan we check differentiate categorical and numerical value without try except?\n\n\nü§ñReplyü§ñ\nYes! We can use type checking or regex patterns to differentiate them more cleanly. Here are a few approaches:\n\nCheck if it‚Äôs a number first:\n\nimport re\nif re.match(r'^-?\\d+\\.?\\d*$', str(expected)):\n    # numeric logic\nelse:\n    # categorical logic\n\nUse isinstance() with conversion:\n\ndef is_numeric(value):\n    try:\n        float(value)\n        return True\n    except:\n        return False\nWhich approach do you prefer for detecting numeric vs categorical values?\nLets use isinstance\n\n\nü§ñReplyü§ñ\nGreat choice! Let‚Äôs implement the isinstance() approach:\ndef is_numeric(value):\n    try:\n        float(value)\n        return True\n    except (ValueError, TypeError):\n        return False\n\ndef score_answer_value(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    if is_numeric(expected) and is_numeric(predicted):\n        # Numeric comparison with ¬±0.1% tolerance\n        pred_num = float(predicted)\n        exp_num = float(expected)\n        tolerance = abs(exp_num * 0.001)\n        return 1.0 if abs(pred_num - exp_num) &lt;= tolerance else 0.0\n    else:\n        # Categorical comparison\n        return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0\nShould we test this with a few examples from your QA data?\n\ndef is_numeric(value):\n    try:\n        float(value)\n        return True\n    except (ValueError, TypeError):\n        return False\n\ndef score_answer_value(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    if is_numeric(expected) and is_numeric(predicted):\n        # Numeric comparison with ¬±0.1% tolerance\n        pred_num = float(predicted)\n        exp_num = float(expected)\n        tolerance = abs(exp_num * 0.001)\n        return 1.0 if abs(pred_num - exp_num) &lt;= tolerance else 0.0\n    else:\n        # Categorical comparison\n        return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0\n\n\nscore_answer_value(answer['answer_value'], qa_df.iloc[7]['answer_value'])\n\n0.0\n\n\nFrom the llm the generated answer has answer_value as False whereas in the dataframe it is 0\n\n\nü§ñReplyü§ñ\nAh, I see the issue! The LLM generated ‚ÄúFalse‚Äù but the expected value is 0. For boolean values, we need to normalize them before comparison.\nLet‚Äôs add boolean normalization to handle True/False ‚ÜîÔ∏é 1/0 conversions:\ndef normalize_boolean(value):\n    if str(value).lower() in ['true', '1', 1]:\n        return '1'\n    elif str(value).lower() in ['false', '0', 0]:\n        return '0'\n    return str(value)\nShould we update the scoring function to use this normalization?\nInstead of modifying the evaluation logic why cant we modify the prompt to answer as 0 and 1 incase of true and false\n\n\nü§ñReplyü§ñ\nThat‚Äôs a much cleaner solution! You‚Äôre right - it‚Äôs better to fix the prompt to generate the expected format rather than complicate the evaluation logic.\nLet‚Äôs update the prompt to specify that boolean answers should be 0/1:\nprompt = \"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor answer_value field:\n- Use 0 for False, 1 for True\n- Use exact numbers for numeric answers\n- Use exact text for categorical answers\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"ref_url\": [\"ref_url\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n}}\n\nContext: {context}\nQuestion: {question}\nResponse:\"\"\"\nShould we test this updated prompt?\n\nprompt = \"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor answer_value field:\n- Use 0 for False, 1 for True\n- Use exact numbers for numeric answers\n- Use exact text for categorical answers\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"ref_url\": [\"ref_url\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n}}\n\nContext: {context}\nQuestion: {question}\nResponse:\"\"\"\n\n\nanswer = evaluate_single_question(qa_df.iloc[7])['answer']\nanswer\n\n{'answer': 'False',\n 'answer_value': '0',\n 'answer_unit': 'is_blank',\n 'ref_id': ['ebert2024'],\n 'ref_url': ['https://arxiv.org/pdf/2410.06681'],\n 'supporting_materials': 'According to the text, \"Where the Act does mandate the disclosure of energy consumption, this information is restricted to authorities and is not accessible to downstream providers (unless the proposed interpretation from 2) is applied) or the general public, due to confidentiality clauses in Articles 21(3), 53(7), and 78(1) [4].\"',\n 'explanation': 'Quote'}\n\n\n\nscore_answer_value(answer['answer_value'], qa_df.iloc[7]['answer_value'])\n\n1.0\n\n\nLets continue implementing scoring logic\n\n\nü§ñReplyü§ñ\nPerfect! The boolean scoring is now working correctly. Let‚Äôs implement the ref_id scoring using Jaccard overlap:\ndef score_ref_id(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    # Convert to sets for Jaccard calculation\n    pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])\n    exp_set = set(expected) if isinstance(expected, list) else set([expected])\n    \n    intersection = len(pred_set.intersection(exp_set))\n    union = len(pred_set.union(exp_set))\n    \n    return intersection / union if union &gt; 0 else 0.0\nShould we test this function?\n\ndef score_ref_id(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    # Convert to sets for Jaccard calculation\n    pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])\n    exp_set = set(expected) if isinstance(expected, list) else set([expected])\n    \n    intersection = len(pred_set.intersection(exp_set))\n    union = len(pred_set.union(exp_set))\n    \n    return intersection / union if union &gt; 0 else 0.0\n\n\nscore_ref_id(answer['ref_id'], qa_df.iloc[7]['ref_id'])\n\n0.0\n\n\n\nanswer['ref_id']\n\n['ebert2024']\n\n\n\nqa_df.iloc[7]['ref_id']\n\n\"['ebert2024']\"\n\n\nI think we need to update the score_ref_id as one of them is string and another one is list\n\n\nü§ñReplyü§ñ\nYou‚Äôre absolutely right! The expected ref_id is stored as a string representation of a list \"['ebert2024']\", but the predicted one is an actual list ['ebert2024'].\nWe need to parse the string first:\nimport ast\n\ndef score_ref_id(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    # Parse string representation if needed\n    if isinstance(expected, str) and expected.startswith('['):\n        expected = ast.literal_eval(expected)\nShould we update the function with this parsing logic?\nYes\n\n\nü§ñReplyü§ñ\nLet‚Äôs update the function with the parsing logic:\nimport ast\n\ndef score_ref_id(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    # Parse string representation if needed\n    if isinstance(expected, str) and expected.startswith('['):\n        expected = ast.literal_eval(expected)\n    \n    # Convert to sets for Jaccard calculation\n    pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])\n    exp_set = set(expected) if isinstance(expected, list) else set([expected])\n    \n    intersection = len(pred_set.intersection(exp_set))\n    union = len(pred_set.union(exp_set))\n    \n    return intersection / union if union &gt; 0 else 0.0\nShould we test this updated version?\n\nimport ast\n\ndef score_ref_id(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    # Parse string representation if needed\n    if isinstance(expected, str) and expected.startswith('['):\n        expected = ast.literal_eval(expected)\n    \n    # Convert to sets for Jaccard calculation\n    pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])\n    exp_set = set(expected) if isinstance(expected, list) else set([expected])\n    \n    intersection = len(pred_set.intersection(exp_set))\n    union = len(pred_set.union(exp_set))\n    \n    return intersection / union if union &gt; 0 else 0.0\n\n\nscore_ref_id(answer['ref_id'], qa_df.iloc[7]['ref_id'])\n\n1.0\n\n\nLets continue with the scoring logic\n\n\nü§ñReplyü§ñ\nPerfect! Now let‚Äôs implement the final component - the is_NA scoring (10% weight). This checks if unanswerable questions are correctly identified:\ndef score_is_na(predicted_answer, expected_answer):\n    # Check if expected is unanswerable\n    expected_is_na = expected_answer.get('answer_value') == 'is_blank'\n    \n    # Check if predicted is unanswerable  \n    predicted_is_na = predicted_answer.get('answer_value') == 'is_blank'\n    \n    return 1.0 if expected_is_na == predicted_is_na else 0.0\nShould we test this function?\n\ndef score_is_na(predicted_answer, expected_answer):\n    # Check if expected is unanswerable\n    expected_is_na = expected_answer['answer_value'] == 'is_blank'\n    \n    # Check if predicted is unanswerable  \n    predicted_is_na = predicted_answer['answer_value'] == 'is_blank'\n    \n    return 1.0 if expected_is_na == predicted_is_na else 0.0\n\n\nscore_is_na(answer, qa_df.iloc[7])\n\n1.0\n\n\nIt is working fine\n\n\nü§ñReplyü§ñ\nExcellent! Now let‚Äôs combine all three scoring components into the final WattBot score function:\ndef calculate_wattbot_score(predicted_answer, expected_row):\n    answer_score = score_answer_value(predicted_answer['answer_value'], expected_row['answer_value'])\n    ref_score = score_ref_id(predicted_answer['ref_id'], expected_row['ref_id'])\n    na_score = score_is_na(predicted_answer, expected_row)\n    \n    return 0.75 * answer_score + 0.15 * ref_score + 0.10 * na_score\nShould we test this complete scoring function?\n\ndef calculate_wattbot_score(predicted_answer, expected_row):\n    answer_score = score_answer_value(predicted_answer['answer_value'], expected_row['answer_value'])\n    ref_score = score_ref_id(predicted_answer['ref_id'], expected_row['ref_id'])\n    na_score = score_is_na(predicted_answer, expected_row)\n    \n    return 0.75 * answer_score + 0.15 * ref_score + 0.10 * na_score\n\ncalculate_wattbot_score(answer, qa_df.iloc[7])\n\n1.0\n\n\nNow how shall we calculate the score for the whole dataframe?\n\n\nü§ñReplyü§ñ\nNow we can create a function to evaluate the entire dataframe. We need to:\n\nProcess each question through your RAG pipeline\nCalculate the WattBot score for each\nReturn the average score\n\ndef evaluate_full_dataset(qa_dataframe):\n    scores = []\n    for i, row in qa_dataframe.iterrows():\n        # Process question and calculate score\n        pass\n    return sum(scores) / len(scores)\nWould you like me to write the complete function, or would you prefer to build it step by step?\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*Unclosed client session.*\")\n\n\ndef evaluate_full_dataset(qa_dataframe):\n    scores = []\n    for i, row in qa_dataframe.iterrows():\n        answer = evaluate_single_question(row)['answer']\n        scores.append(calculate_wattbot_score(answer, row))\n    return sum(scores) / len(scores)\n\n\nevaluate_full_dataset(qa_df)\n\n0.600609756097561\n\n\n\ntst_df = pd.read_csv(data_path/'test_Q.csv')\ntst_df.head(n=2)\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq001\nWhat was the average increase in U.S. data cen...\nNaN\nNaN\npercent\nNaN\nNaN\nNaN\nNaN\n\n\n1\nq002\nIn 2023, what was the estimated amount of cars...\nNaN\nNaN\ncars\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nNow I want to generate the answer of tst_df\n\n\nü§ñReplyü§ñ\nGreat! Your RAG system achieved a score of ~0.61 on the training data. Now let‚Äôs generate answers for the test dataset.\nSince the test dataset has NaN values in the answer columns, we need to create a function that fills in all the required fields:\ndef generate_test_answers(test_dataframe):\n    results = []\n    for i, row in test_dataframe.iterrows():\n        answer = evaluate_single_question(row)['answer']\n        # Add the generated answer to the row\n        # Fill in all required columns\n    return results_df\nWould you like me to write the complete function to generate the test answers?\n\ndef generate_test_answers(test_dataframe):\n    for i, row in test_dataframe.iterrows():\n        answer_block = evaluate_single_question(row)\n        if 'answer' in answer_block:\n            row['answer'] = answer['answer']\n            row['answer_value'] = answer['answer_value']\n            row['answer_unit'] = answer['answer_unit']\n            row['ref_id'] = answer['ref_id']\n            row['ref_url'] = answer['ref_url']\n            row['supporting_materials'] = answer['supporting_materials']\n            row['explanation'] = answer['explanation']\n    return test_dataframe.copy(deep=True)\n\n\nlen(tst_df), len(qa_df)\n\n(282, 41)\n\n\nThere are too many rows for tst_df, lets parallelize this\n\n\nü§ñReplyü§ñ\nYou‚Äôre right! Processing 282 questions sequentially will take a very long time. Let‚Äôs parallelize it using fc.parallel:\ndef generate_test_answers_parallel(test_dataframe):\n    def process_row(row):\n        answer_block = evaluate_single_question(row)\n        if 'answer' in answer_block:\n            answer = answer_block['answer']\n            return {\n                'id': row['id'],\n                'question': row['question'],\n                'answer': answer['answer'],\n                'answer_value': answer['answer_value'],\n                'answer_unit': answer['answer_unit'],\n                'ref_id': answer['ref_id'],\n                'ref_url': answer['ref_url'],\n                'supporting_materials': answer['supporting_materials'],\n                'explanation': answer['explanation']\n            }\n    \n    results = fc.parallel(process_row, test_dataframe.iterrows(), threadpool=True)\n    return pd.DataFrame([r for r in results if r is not None])\nShould we try this parallel approach?\n\nnext(qa_df.iterrows())\n\n(0,\n id                                                                   q003\n question                What is the name of the benchmark suite presen...\n answer                                            The ML.ENERGY Benchmark\n answer_value                                          ML.ENERGY Benchmark\n answer_unit                                                      is_blank\n ref_id                                                      ['chung2025']\n ref_url                              ['https://arxiv.org/pdf/2505.06371']\n supporting_materials    We present the ML.ENERGY Benchmark, a benchmar...\n explanation                                                         Quote\n Name: 0, dtype: object)\n\n\niterrows returns both index and value. I think you need to correct the function\n\n\nü§ñReplyü§ñ\nYou‚Äôre absolutely right! iterrows() returns (index, row) tuples. Let me fix the function:\ndef process_row(index_row_tuple):\n    index, row = index_row_tuple\n    # rest of the processing logic\nShould we update the parallel function with this correction?\n\ndef generate_test_answers_parallel(test_dataframe):\n    def process_row(index_row_tuple):\n        idx, row = index_row_tuple\n        answer_block = evaluate_single_question(row)\n        if 'answer' in answer_block:\n            answer = answer_block['answer']\n            return {\n                'id': row['id'],\n                'question': row['question'],\n                'answer': answer['answer'],\n                'answer_value': answer['answer_value'],\n                'answer_unit': answer['answer_unit'],\n                'ref_id': answer['ref_id'],\n                'ref_url': answer['ref_url'],\n                'supporting_materials': answer['supporting_materials'],\n                'explanation': answer['explanation']\n            }\n    \n    results = fc.parallel(process_row, test_dataframe.iterrows(), threadpool=True)\n    return pd.DataFrame([r for r in results if r is not None])\n\n\ntst_ans_df = generate_test_answers_parallel(tst_df)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\ntst_ans_df.head(n=2)\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq001\nWhat was the average increase in U.S. data cen...\nThere is no information about the average incr...\nis_blank\nis_blank\n[patterson2021]\n[https://arxiv.org/pdf/2104.10350]\nAccording to a recent paper in Science [Mas20]...\nis_blank\n\n\n1\nq002\nIn 2023, what was the estimated amount of cars...\n13,900\n13900\ncars\n[amazon2023]\n[https://sustainability.aboutamazon.com/2023-a...\nThe Amazon Solar Farm Maryland‚ÄìCPV Backbone wi...\nQuote\n\n\n\n\n\n\n\n\ntst_ans_df[tst_ans_df['answer_value'].isnull()]\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n\n\n\n\n\n\ntst_ans_df.to_csv(data_path/'tst_ans_df.csv', index=False)\n\nHow to check for nan values in tst_ans_df?\n\n\nü§ñReplyü§ñ\nYou can check for NaN values in several ways:\n# Check for any NaN values in the entire dataframe\ntst_ans_df.isnull().sum()\nThis will show you the count of NaN values in each column. Would you like to try this?\n\ntst_ans_df.isnull().sum()\n\nid                      0\nquestion                0\nanswer                  0\nanswer_value            0\nanswer_unit             0\nref_id                  0\nref_url                 0\nsupporting_materials    0\nexplanation             0\ndtype: int64\n\n\nWhen I submitting the csv file in the kaggle, I am getting the error Submission contains null values\n\n\nü§ñReplyü§ñ\nThe issue might be that some values are being stored as strings like ‚ÄúNone‚Äù or empty strings rather than actual NaN values. Let‚Äôs check for these:\n# Check for string representations of null values\ntst_ans_df[tst_ans_df['answer_value'] == 'None']\nOr check for empty strings:\ntst_ans_df[tst_ans_df['answer_value'] == '']\nWhich one should we try first to identify the problematic rows?\n\nlen(tst_ans_df)\n\n274\n\n\nRows are missing\n\n\nü§ñReplyü§ñ\nYou‚Äôre right! The test dataframe has 282 rows but your results only have 274 rows. This means 8 questions failed to process and returned None.\nLet‚Äôs check which questions are missing:\nmissing_ids = set(tst_df['id']) - set(tst_ans_df['id'])\nprint(f\"Missing {len(missing_ids)} questions: {missing_ids}\")\nShould we run this to identify the problematic questions?\n\nmissing_ids = set(tst_df['id']) - set(tst_ans_df['id'])\nmissing_ids\n\n{'q012', 'q058', 'q087', 'q160', 'q172', 'q220', 'q247', 'q273'}\n\n\nYes lets run for problematic ones\n\n\nü§ñReplyü§ñ\nLet‚Äôs process just those 8 missing questions to see what‚Äôs causing the failures:\nmissing_rows = tst_df[tst_df['id'].isin(missing_ids)]\nprint(f\"Processing {len(missing_rows)} missing questions...\")\nShould we try processing one of these problematic questions individually first to debug the issue?\nYes\n\n\nü§ñReplyü§ñ\nLet‚Äôs try processing one of the problematic questions to see what error occurs:\ntest_row = tst_df[tst_df['id'] == 'q012'].iloc[0]\nresult = evaluate_single_question(test_row)\nprint(result)\nShould we run this to see what‚Äôs causing the failure?\n\ntest_row = tst_df[tst_df['id'] == 'q012'].iloc[0]\nresult = evaluate_single_question(test_row)\n\nIt is giving the error to fail the JSON parsing\n\n\nü§ñReplyü§ñ\nThe issue is that the LLM isn‚Äôt generating valid JSON. Let‚Äôs look at what it actually returned by checking the raw response:\n# Let's see what the LLM actually generated\ntest_row = tst_df[tst_df['id'] == 'q012'].iloc[0]\nquestion = test_row['question']\nprint(f\"Question: {question}\")\nShould we check what question q012 is asking, and then see the raw LLM response to understand why JSON parsing is failing?\n\ntest_row = tst_df[tst_df['id'] == 'q012'].iloc[0]\nquestion = test_row['question']\nquestion\n\n'What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?'\n\n\nLets see the raw LLM response\n\n\nü§ñReplyü§ñ\nLet‚Äôs check the raw LLM response to see why JSON parsing is failing:\n# Get the raw response without JSON parsing\nquery_vector = openai_client.embeddings.create(model=\"nomic-ai/nomic-embed-text-v1.5\", input=[question]).data[0].embedding\nresults = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\ncontext = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\nformatted_prompt = prompt.format(context=context, question=question)\nresponse = llm.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\nprint(response.choices[0].message.content)\nShould we run this to see the raw response?\nLets create a function\n\n\nü§ñReplyü§ñ\nLet‚Äôs create a function to get the raw LLM response for debugging:\ndef get_raw_llm_response(question):\n    query_vector = openai_client.embeddings.create(model=\"nomic-ai/nomic-embed-text-v1.5\", input=[question]).data[0].embedding\n    results = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\n    formatted_prompt = prompt.format(context=context, question=question)\n    response = llm.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n    return response.choices[0].message.content\nShould we test this function with the problematic question?\n\ndef get_raw_llm_response(question):\n    query_vector = openai_client.embeddings.create(model=\"nomic-ai/nomic-embed-text-v1.5\", input=[question]).data[0].embedding\n    results = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\n    formatted_prompt = prompt.format(context=context, question=question)\n    response = llm.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n    return response.choices[0].message.content\nget_raw_llm_response(question)\n\n'Unfortunately, the provided context does not contain the specific information required to answer the question about the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model. The question seems to be asking for information that is not present in the given text.\\n\\nHere is the response in the required JSON format:\\n\\n```\\n{\\n  \"answer\": \"The information is not available in the provided context.\",\\n  \"answer_value\": \"is_blank\",\\n  \"answer_unit\": \"is_blank\",\\n  \"ref_id\": [],\\n  \"ref_url\": [],\\n  \"supporting_materials\": \"is_blank\",\\n  \"explanation\": \"is_blank\"\\n}\\n```'\n\n\nAbove is the response\n\n\nü§ñReplyü§ñ\nI see the issue! The LLM is wrapping the JSON in markdown code blocks with triple backticks, but your parsing function expects raw JSON.\nWe need to extract the JSON from between the ``json blocks. Should we modify theevaluate_single_question` function to handle this markdown formatting?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs modify the parsing to handle markdown code blocks:\ndef extract_json_from_response(response_text):\n    if \"```\" in response_text:\n        # Extract JSON from markdown code blocks\n        start = response_text.find(\"```\") + 3\n        if response_text[start:start+4] == \"json\":\n            start += 4\n        end = response_text.find(\"```\", start)\n        return response_text[start:end].strip()\n    return response_text.strip()\nShould we test this extraction function first?\nAre you certain that this extraction logic will work for all responses?\n\n\nü§ñReplyü§ñ\nYou raise a good point! The extraction logic might not handle all edge cases. For example:\n\nWhat if there are multiple code blocks?\nWhat if the JSON is malformed inside the blocks?\nWhat if there are no backticks but the JSON is still valid?\n\nWould you like to test it with a few different response formats first, or should we make it more robust to handle these edge cases?\nI was thinking of making the prompt more strict to return the json even if there is no related context. Can we use prefill?\n\n\nü§ñReplyü§ñ\nGreat idea! Using prefill to force the model to start with JSON structure is much more reliable than trying to parse markdown afterwards.\nWith prefill, you can start the assistant‚Äôs response with { to force it to generate valid JSON directly. This way the model has no choice but to continue with proper JSON format.\nWould you like to try modifying the chat completion call to use prefill, or explore how to implement this with the Fireworks API?\nLets explore how the fireworks api implement this?\n\n\nü§ñReplyü§ñ\nWith Fireworks API, you can implement prefill by adding an assistant message that starts the response. Here‚Äôs the approach:\nmessages = [\n    {\"role\": \"user\", \"content\": formatted_prompt},\n    {\"role\": \"assistant\", \"content\": \"{\"}\n]\nThis forces the model to continue from the opening brace, ensuring JSON format.\nWould you like to try this approach, or would you prefer to see how to modify your existing function to use this prefill technique?\nLets try this approach\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs modify your evaluate_single_question function to use prefill:\nresponse = llm.chat.completions.create(\n    messages=[\n        {\"role\": \"user\", \"content\": formatted_prompt},\n        {\"role\": \"assistant\", \"content\": \"{\"}\n    ]\n)\nThen we‚Äôll need to add the opening brace back to the response before parsing. Should we update your function with this change?\n\nprompt = \"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor answer_value field:\n- Use 0 for False, 1 for True\n- Use exact numbers for numeric answers\n- Use exact text for categorical answers\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"ref_url\": [\"ref_url\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n}}\n\nAlways start with {{. \nContext: {context}\nQuestion: {question}\nResponse: \"\"\"\n\n\ndef get_raw_llm_response(question):\n    query_vector = openai_client.embeddings.create(model=\"nomic-ai/nomic-embed-text-v1.5\", input=[question]).data[0].embedding\n    results = client.query_points(collection_name=new_collection_name, query=query_vector, limit=5)\n    context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in results.points])\n    formatted_prompt = prompt.format(context=context, question=question)\n    response = llm.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n    return response.choices[0].message.content\nget_raw_llm_response(question)\n\n'{\\n  \"answer\": \"is_blank\",\\n  \"answer_value\": \"is_blank\",\\n  \"answer_unit\": \"is_blank\",\\n  \"ref_id\": [\"samsi2024\", \"chung2025\"],\\n  \"ref_url\": [\"https://arxiv.org/pdf/2310.03003\", \"https://arxiv.org/pdf/2505.06371\"],\\n  \"supporting_materials\": \"None\",\\n  \"explanation\": \"is_blank\"\\n}'\n\n\nLets try with other questions where it failed\n\n\nü§ñReplyü§ñ\nGreat! The prompt modification worked - now it‚Äôs returning proper JSON. Let‚Äôs test with the other failing questions:\ntest_ids = ['q058', 'q064', 'q087']\nfor test_id in test_ids:\n    test_row = tst_df[tst_df['id'] == test_id].iloc[0]\n    response = get_raw_llm_response(test_row['question'])\n    print(f\"{test_id}: {response[:100]}...\")\nShould we test these to see if they now generate valid JSON?\n\ntest_ids = ['q058', 'q064', 'q087']\nfor test_id in test_ids:\n    test_row = tst_df[tst_df['id'] == test_id].iloc[0]\n    response = get_raw_llm_response(test_row['question'])\n    print(response[:100])\n\n{\n  \"answer\": \"False\",\n  \"answer_value\": \"0\",\n  \"answer_unit\": \"is_blank\",\n  \"ref_id\": [],\n  \"ref_ur\n{\n  \"answer\": \"is_blank\",\n  \"answer_value\": \"is_blank\",\n  \"answer_unit\": \"is_blank\",\n  \"ref_id\": [\"c\n{\n  \"answer\": \"475 gCO2eq/kWh\",\n  \"answer_value\": \"475\",\n  \"answer_unit\": \"gCO2eq/kWh\",\n  \"ref_id\": \n\n\n\ntst_ans_df = generate_test_answers_parallel(tst_df)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\nlen(tst_ans_df)\n\n281\n\n\n\ntst_ans_df.to_csv(data_path/'tst_ans_df.csv', index=False)\n\n\ntst_ans_df.isnull().sum()\n\nid                      0\nquestion                0\nanswer                  0\nanswer_value            0\nanswer_unit             0\nref_id                  0\nref_url                 0\nsupporting_materials    0\nexplanation             0\ndtype: int64\n\n\n\nlen(tst_df), len(tst_ans_df)\n\n(282, 281)\n\n\n\nmissing_ids = set(tst_df['id']) - set(tst_ans_df['id'])\nmissing_ids\n\n{'q066'}\n\n\n\ntst_ans_df.head(n=2)\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq001\nWhat was the average increase in U.S. data cen...\nThe text does not provide the average increase...\nis_blank\nis_blank\n[patterson2021]\n[https://arxiv.org/pdf/2104.10350]\nThe text does not provide the average increase...\nis_blank\n\n\n1\nq002\nIn 2023, what was the estimated amount of cars...\nThe Amazon Solar Farm Maryland-CPV Backbone wo...\n13900\ncars\n[amazon2023]\n[https://sustainability.aboutamazon.com/2023-a...\nThe Amazon Solar Farm Maryland‚ÄìCPV Backbone wi...\nQuote\n\n\n\n\n\n\n\n\ntst_ans_df['supporting_materials'] = tst_ans_df['supporting_materials'].replace([None, '', 'None', 'null'], 'is_blank')\n\n\ntst_ans_df_local = pd.read_csv(data_path/'tst_ans_df.csv')\ntst_ans_df_local.head(n=2)\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq001\nWhat was the average increase in U.S. data cen...\nThe text does not provide the average increase...\nis_blank\nis_blank\n['patterson2021']\n['https://arxiv.org/pdf/2104.10350']\nThe text does not provide the average increase...\nis_blank\n\n\n1\nq002\nIn 2023, what was the estimated amount of cars...\nThe Amazon Solar Farm Maryland-CPV Backbone wo...\n13900\ncars\n['amazon2023']\n['https://sustainability.aboutamazon.com/2023-...\nThe Amazon Solar Farm Maryland‚ÄìCPV Backbone wi...\nQuote\n\n\n\n\n\n\n\nHow does tst_ans_df_local has 20 null values whereas tst_ans_df has 0 null values for supporting materials?\n\n\nü§ñReplyü§ñ\nGood observation! The difference suggests there might be inconsistency between what‚Äôs in memory (tst_ans_df) versus what was saved to the CSV file (tst_ans_df_local).\nThis could happen if: 1. The CSV was saved from an earlier version before all issues were fixed 2. Some responses had empty strings \"\" that got converted to NaN when reading the CSV 3. There were processing differences between runs\nShould we check what the actual values look like in the rows with null supporting_materials?\nyes\n\n\nü§ñReplyü§ñ\nLet‚Äôs check the rows with null supporting_materials:\nnull_rows = tst_ans_df_local[tst_ans_df_local['supporting_materials'].isnull()]\nprint(f\"Found {len(null_rows)} rows with null supporting_materials\")\nprint(null_rows[['id', 'supporting_materials']].head())\nShould we run this to see which questions have the null values?\n\nnull_rows = tst_ans_df_local[tst_ans_df_local['supporting_materials'].isnull()]\nnull_rows[['id', 'supporting_materials']].head(n=2)\n\n\n\n\n\n\n\n\nid\nsupporting_materials\n\n\n\n\n9\nq012\nNaN\n\n\n17\nq020\nNaN\n\n\n\n\n\n\n\n\ntst_ans_df.iloc[9]\n\nid                                                                   q012\nquestion                What is the estimated GPU Power Usage in kWh f...\nanswer                                                           is_blank\nanswer_value                                                     is_blank\nanswer_unit                                                      is_blank\nref_id                                                                 []\nref_url                                                                []\nsupporting_materials                                             is_blank\nexplanation                                                      is_blank\nName: 9, dtype: object\n\n\n\ntst_ans_df.iloc[59]\n\nid                                                                   q064\nquestion                What is the estimated cost, in USD, of trainin...\nanswer                  The estimated cost is not provided in the give...\nanswer_value                                                     is_blank\nanswer_unit                                                      is_blank\nref_id                                                      [cottier2024]\nref_url                                [https://arxiv.org/pdf/2405.21015]\nsupporting_materials                                             is_blank\nexplanation                                                      is_blank\nName: 59, dtype: object\n\n\n\ntst_ans_df.isnull().sum()\n\nid                      0\nquestion                0\nanswer                  0\nanswer_value            0\nanswer_unit             0\nref_id                  0\nref_url                 0\nsupporting_materials    0\nexplanation             0\ndtype: int64\n\n\nCan we replace the empty string ones with is_blank?\n\n\nü§ñReplyü§ñ\nYes! We can replace empty strings with ‚Äúis_blank‚Äù. Here‚Äôs how:\ntst_ans_df['supporting_materials'] = tst_ans_df['supporting_materials'].replace('', 'is_blank')\nShould we run this to fix the empty supporting_materials values?\n\nnull_rows = tst_ans_df[tst_ans_df['supporting_materials'].isnull()]\nnull_rows[['id', 'supporting_materials']].head()\n\n\n\n\n\n\n\n\nid\nsupporting_materials\n\n\n\n\n\n\n\n\n\n\ntst_ans_df.to_csv(data_path/'tst_ans_df.csv', index=False)\n\n\ntst_ans_df_local = pd.read_csv(data_path/'tst_ans_df.csv')\ntst_ans_df_local.isnull().sum()\n\nid                      0\nquestion                0\nanswer                  0\nanswer_value            0\nanswer_unit             0\nref_id                  0\nref_url                 0\nsupporting_materials    0\nexplanation             0\ndtype: int64\n\n\nCan we update the prompt to answer supporting material as is_blank if no reference or exact quote\n\n\nü§ñReplyü§ñ\nYes! Let‚Äôs update the prompt to be more explicit about when to use ‚Äúis_blank‚Äù for supporting_materials:\nprompt = \"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor supporting_materials field:\n- Use exact verbatim quote if answer comes from text\n- Use \"Table X\" or \"Figure Y\" if from visual elements\n- Use \"is_blank\" if no specific reference or when unanswerable\n\nAlways start with {{. \nContext: {context}\nQuestion: {question}\nResponse: \"\"\"\nShould we test this updated prompt?\n\nprompt = \"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor answer_value field:\n- Use 0 for False, 1 for True\n- Use exact numbers for numeric answers\n- Use exact text for categorical answers\n\nFor supporting_materials field:\n- Use exact verbatim quote if answer comes from text\n- Use \"Table X\" or \"Figure Y\" if from visual elements\n- Use \"is_blank\" if no specific reference or when unanswerable\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"ref_url\": [\"ref_url\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n}}\n\nAlways start with {{. \nContext: {context}\nQuestion: {question}\nResponse: \"\"\"\n\nHow shall I improve the performance?\n\n\nü§ñReplyü§ñ\nGreat question! To improve your RAG system‚Äôs performance, here are some key areas to explore:\n\nRetrieval quality - Are you retrieving the most relevant chunks?\nChunk parameters - Could different chunk sizes or overlap work better?\nNumber of retrieved chunks - Currently using 5, might need more or fewer\nPrompt engineering - Add few-shot examples to guide the LLM better\nRe-ranking - Add a re-ranking step after initial retrieval\n\nWhich area would you like to focus on first? Or would you like to analyze where your current system is struggling most?\nHow do you measure the retrieval quality?\n\n\nü§ñReplyü§ñ\nGood question! To measure retrieval quality, you can check if the correct documents are being retrieved. Here are a few approaches:\n\nRecall@k - For each question in your training set, check if the expected ref_id appears in your top-k retrieved chunks\nMean Reciprocal Rank (MRR) - Measure the rank position of the first correct document\nManual inspection - Look at specific cases where your system scored poorly\n\nWould you like to calculate Recall@5 on your training set to see how often the correct documents are being retrieved?\nHow is the Recall@k different from precision@k?\n\n\nü§ñReplyü§ñ\nGreat question! Here‚Äôs the key difference:\nRecall@k - Of all the correct documents that should be retrieved, how many appear in the top-k results? - Formula: (# of correct docs in top-k) / (total # of correct docs) - Answers: ‚ÄúDid I find what I needed?‚Äù\nPrecision@k - Of the top-k documents retrieved, how many are actually correct? - Formula: (# of correct docs in top-k) / k - Answers: ‚ÄúHow accurate are my results?‚Äù\nFor your RAG system, if a question has 1 correct document and you retrieve 5 chunks: - If the correct doc appears anywhere in those 5: Recall@5 = 1.0 - If only 1 of the 5 is correct: Precision@5 = 0.2\nWhich metric would you like to calculate first for your system?\nFirst I want to create a sdk/library for what I have till develop for faster experimentation with the above key ares\n\n\nü§ñReplyü§ñ\nGreat idea! Creating a reusable SDK will make it much easier to experiment with different configurations.\nTo organize your code into a library, we could structure it with classes like:\n\nWattBotRAG - Main class to handle retrieval and generation\nWattBotEvaluator - For scoring and evaluation\nWattBotConfig - To manage settings (chunk size, model names, etc.)\n\nWould you like to start by creating a config class to store all your parameters (like chunk_size, step, embedding model, LLM model, etc.), or would you prefer to begin with the main RAG class?\nLets start with WattBotRAG\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs start designing the WattBotRAG class. Here are the key methods it should have:\n\n__init__() - Initialize with config (Qdrant client, collection name, models, etc.)\nretrieve() - Get relevant chunks for a question\ngenerate() - Generate answer from retrieved chunks\nanswer_question() - Complete pipeline (retrieve + generate)\n\nShould we start by defining the __init__ method with the parameters you‚Äôll need, or would you prefer to sketch out all the method signatures first?\nI want this library to work with any rag application. Also I want to include the chunkng process and its embedding process\n\n\nü§ñReplyü§ñ\nExcellent! Making it generic for any RAG application is a smart design choice. So your library should handle:\n\nDocument processing - PDF reading, chunking\nEmbedding - Generate and store embeddings in Qdrant\nRetrieval - Query the vector database\nGeneration - LLM-based answer generation\nEvaluation - Scoring (optional, for your competition)\n\nShould we create separate classes for each component (like DocumentProcessor, Embedder, Retriever, Generator) or keep them as methods within WattBotRAG? Which structure do you prefer?\nLets create separate classes for each component\n\n\nü§ñReplyü§ñ\nGreat choice! Separate classes will make your library more modular and flexible. Let‚Äôs start with the first component.\nWhich class would you like to build first:\n\nDocumentProcessor - Handles PDF reading and chunking\nEmbedder - Creates embeddings and stores in Qdrant\nRetriever - Queries the vector database\nGenerator - LLM-based answer generation\n\nWhich one should we start with?\nDocumentProcessor\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs design the DocumentProcessor class. It should handle:\n\nReading PDFs\nChunking text with configurable size and overlap\nSupporting different chunking strategies (fixed-size, sliding window, etc.)\n\nHere‚Äôs a basic structure to start with:\nclass DocumentProcessor:\n    def __init__(self, chunk_size=1500, step=1400):\n        self.chunk_size = chunk_size\n        self.step = step\n    \n    def read_pdf(self, pdf_path):\n        # Read PDF and return text\n        pass\n    \n    def chunk_text(self, text):\n        # Chunk text using sliding window\n        pass\n    \n    def process_document(self, pdf_path):\n        # Complete pipeline: read + chunk\n        pass\nWould you like to implement these methods, or should we discuss the design further first?\n\nimport fastcore.all as fc\nimport contextkit.read as rd\n\nclass DocumentProcessor:\n    def __init__(self, chunk_size=1500, step=1400):\n        fc.store_attr()\n    \n    def read_pdf(self, pdf_path):\n        return rd.read_pdf(pdf_path)\n    \n    def chunk_text(self, text):\n        return fc.L(text[i:i+self.chunk_size] for i in range(0, len(text), self.step))\n    \n    def process_document(self, pdf_path):\n        return self.chunk_text(self.read_pdf(pdf_path))\n\nHow does it look?\n\n\nü§ñReplyü§ñ\nLooks great! Your DocumentProcessor is clean and concise. The use of fc.store_attr() is nice for automatically storing the init parameters.\nOne small consideration: Should process_document also accept a document ID or metadata? This might be useful when you need to track which document each chunk came from later.\nWould you like to add metadata handling, or move on to building the Embedder class next?\nLets do that\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs add metadata handling to DocumentProcessor. You could modify it to accept and return metadata alongside chunks:\ndef process_document(self, pdf_path, doc_id=None, metadata=None):\n    text = self.read_pdf(pdf_path)\n    chunks = self.chunk_text(text)\n    # Attach metadata to each chunk?\n    return chunks\nShould the method return just chunks, or return chunks with their associated metadata (like doc_id, chunk_index, etc.)?\nLet it return with associated metadata\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs modify the method to return chunks with their metadata:\ndef process_document(self, pdf_path, doc_id=None, metadata=None):\n    text = self.read_pdf(pdf_path)\n    chunks = self.chunk_text(text)\n    return [{'text': chunk, 'chunk_id': i, 'doc_id': doc_id, **(metadata or {})} for i, chunk in enumerate(chunks)]\nDoes this structure work for you, or would you prefer a different format for the chunk metadata?\n\nimport fastcore.all as fc\nimport contextkit.read as rd\n\nclass DocumentProcessor:\n    def __init__(self, data_path='watt_bot_data', chunk_size=1500, step=1400):\n        fc.store_attr()\n    \n    def read_pdf(self, doc_id):\n        return rd.read_pdf(fc.Path(self.data_path)/f'{doc_id}.pdf')\n    \n    def chunk_text(self, text):\n        for i in range(0, len(text), self.step): yield text[i:i+self.chunk_size]\n\n    def process_document(self, doc_id, metadata=None):\n        text = self.read_pdf(doc_id)\n        return fc.L({'text': chunk, 'chunk_id': i, 'doc_id': doc_id, **(metadata or {})} for i, chunk in enumerate(self.chunk_text(text)))\n\nShould we not text this class?\n\n\nü§ñReplyü§ñ\nYou‚Äôre absolutely right! We should test the DocumentProcessor class before moving forward. Let‚Äôs make sure it works correctly:\nprocessor = DocumentProcessor()\nresult = processor.process_document(data_path/'amazon2023.pdf', doc_id='amazon2023', metadata={'type': 'report'})\nprint(f\"Number of chunks: {len(result)}\")\nprint(f\"First chunk: {result[0]}\")\nShould we run this test?\n\nmetadata = df.iloc[0]\ndoc_id = metadata['id']\nprocessor = DocumentProcessor()\nresult = processor.process_document(doc_id, metadata={'type': 'report', 'url': metadata['url'], 'title': metadata['title']})\nprint(f\"Number of chunks: {len(result)}\")\nprint(f\"First chunk: {result[0]['text'][:200]}\")\n\nNumber of chunks: 322\nFirst chunk: Amazon \nSustainability \nReport\n2023 Contents\nOverview\n3 Introduction\n4 A Letter from Our Chief \nSustainability Officer¬†\n5 How We Work\n6 Goals Summary\n7 2023 Year in Review ¬†\nEnvironment\n9 Carbon\n24 Ca\n\n\nIt is working what is next?\n\n\nü§ñReplyü§ñ\nExcellent! The DocumentProcessor is working perfectly. Now let‚Äôs move on to the Embedder class. This should handle:\n\nCreating embeddings from text chunks\nStoring embeddings in Qdrant\nSupporting different embedding models\n\nShould we start designing the Embedder class structure?\nYes\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs design the Embedder class. It should handle:\n\nInitialize with embedding model and Qdrant client\nCreate embeddings in batches (for efficiency)\nStore embeddings with metadata in Qdrant\n\nHere‚Äôs a basic structure:\nclass Embedder:\n    def __init__(self, model_name, qdrant_client, collection_name, batch_size=50):\n        # Initialize embedding model and Qdrant client\n        pass\n    \n    def create_embeddings(self, chunks):\n        # Generate embeddings in batches\n        pass\n    \n    def store_in_qdrant(self, chunks, embeddings):\n        # Store embeddings with metadata\n        pass\nDoes this structure work for you, or would you like to adjust the methods?\nI want this embedder to work with any vector database\n\n\nü§ñReplyü§ñ\nExcellent design decision! To make it work with any vector database, we should separate the embedding creation from storage.\nHere are two approaches:\n\nAbstract base class - Create a VectorStore interface that different databases (Qdrant, Pinecone, etc.) can implement\nDependency injection - Pass a storage function/object to the Embedder\n\nWhich approach appeals to you more?\n2nd option\n\n\nü§ñReplyü§ñ\nPerfect! With dependency injection, the Embedder will just create embeddings, and you pass in a storage handler separately. Here‚Äôs the structure:\nclass Embedder:\n    def __init__(self, embedding_client, model_name, batch_size=50):\n        self.embedding_client = embedding_client\n        self.model_name = model_name\n        self.batch_size = batch_size\n    \n    def create_embeddings(self, texts):\n        # Batch embed the texts\n        pass\nThen you‚Äôd have a separate VectorStore class that handles storage. Does this separation make sense for your use case?\nYes\n\n\nü§ñReplyü§ñ\nGreat! So you‚Äôll have:\n\nEmbedder - Creates embeddings from text\nVectorStore - Handles storage (you can create QdrantStore, PineconeStore, etc.)\n\nShould we implement the Embedder class first, or would you like to design the VectorStore interface first?\nYes lets do the embedder\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs implement the Embedder class:\nclass Embedder:\n    def __init__(self, client, model_name, bs=50):\n        fc.store_attr()\n        self.gen_emb = self.client.embeddings.create\n    \n    def create_embeddings(self, texts):\n        embeddings = fc.L()\n        for batch in fc.chunked(texts, self.bs):\n            response = self.gen_emb(model=self.model_name, input=list(batch)).data\n            embeddings.extend([data.embedding for data in response])\n        return embeddings\nThis assumes your embedding_client has an OpenAI-compatible interface (like your Fireworks client). Does this work for your needs?\n\nclass Embedder:\n    def __init__(self, client, model_name, bs=50):\n        fc.store_attr()\n        self.gen_emb = self.client.embeddings.create\n    \n    def create_embeddings(self, texts):\n        embeddings = fc.L()\n        for batch in fc.chunked(texts, self.bs):\n            response = self.gen_emb(model=self.model_name, input=list(batch)).data\n            embeddings.extend([data.embedding for data in response])\n        return embeddings\n\n\nimport os\nimport openai\n\nopenai_client = openai.OpenAI(base_url = \"https://api.fireworks.ai/inference/v1\", api_key=os.environ['FIREWORKS_API_KEY'])\n\nembdr = Embedder(openai_client, \"nomic-ai/nomic-embed-text-v1.5\")\nembds = embdr.create_embeddings(result.attrgot('text'))\nlen(embds), len(embds[0])\n\n(322, 768)\n\n\nLets implement the Vector Store now\n\n\nü§ñReplyü§ñ\nPerfect! The Embedder is working well. Now let‚Äôs design the VectorStore class for Qdrant:\nclass QdrantStore:\n    def __init__(self, client, collection_name):\n        fc.store_attr()\n    \n    def create_collection(self, vector_size, distance=\"Cosine\"):\n        # Create collection if it doesn't exist\n        pass\n    \n    def upsert(self, points):\n        # Store points in Qdrant\n        pass\n    \n    def search(self, query_vector, limit=5):\n        # Search for similar vectors\n        pass\nDoes this structure look good to you?\nIf it is QdrantStore let it accept the host and create the client\n\n\nü§ñReplyü§ñ\nGood point! Let‚Äôs make QdrantStore handle its own client creation:\nclass QdrantStore:\n    def __init__(self, url, api_key, collection_name):\n        fc.store_attr()\n        self.client = QdrantClient(url=url, api_key=api_key)\n    \n    def create_collection(self, vector_size, distance=Distance.COSINE):\n        if not self.client.collection_exists(self.collection_name):\n            self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(size=vector_size, distance=distance)\n            )\nShould we continue implementing the upsert and search methods?\nI think the point of creating the points is to insert them into the database. Do you think there are any other use cases for creating the points?\n\n\nü§ñReplyü§ñ\nYou raise a great point! In most cases, points are created specifically to insert into the database. However, there could be some edge cases:\n\nDry-run/validation - Creating points to validate format before actual insertion\nBatch processing - Creating points in memory to accumulate before bulk insert\nTesting - Creating mock points without database interaction\n\nBut you‚Äôre right - for most practical purposes, point creation and insertion go together. Should we combine them into a single upsert method that creates and stores points in one step?\nIs chaining of methods possible we create points first then we upsert it?\n\n\nü§ñReplyü§ñ\nYes, method chaining is definitely possible! You could design it like this:\nclass QdrantStore:\n    def __init__(self, url, api_key, collection_name):\n        # ... initialization\n        self.points = []\n    \n    def create_points(self, chunks, embeddings):\n        self.points = [...]  # create points\n        return self  # Return self for chaining\n    \n    def upsert(self):\n        self.client.upsert(collection_name=self.collection_name, points=self.points)\n        return self\nThen you could use it like: store.create_points(chunks, embeddings).upsert()\nDoes this chaining approach appeal to you?\nYes\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs implement the QdrantStore class with method chaining:\nclass QdrantStore:\n    def __init__(self, url, api_key, cname):\n        fc.store_attr()\n        self.client = QdrantClient(url=url, api_key=api_key)\n        self.points = []\n    \n    def create_collection(self, vs, distance=Distance.COSINE):\n        if not self.client.collection_exists(self.cname):\n            self.client.create_collection(collection_name=self.cname, vectors_config=VectorParams(size=vs, distance=distance))\n        return self\n    \n    def create_points(self, chunks, embeddings, start_id=0):\n        self.points = [PointStruct(id=start_id+i, vector=embeddings[i], payload=chunk) for i, chunk in enumerate(chunks)]\n        return self\n    \n    def upsert(self):\n        self.client.upsert(collection_name=self.collection_name, points=self.points)\n        return self\nShould we test this implementation?\nWhat about search?\n\n\nü§ñReplyü§ñ\nGood catch! We need to add the search method too. Here‚Äôs what it should look like:\ndef search(self, query_vector, limit=5):\n    results = self.client.query_points(collection_name=self.cname, query=query_vector, limit=limit)\n    return results.points\nShould we add this method and then test the complete QdrantStore class?\n\nimport fastcore.all as fc\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\n\nclass QdrantStore:\n    def __init__(self, url, api_key, cname):\n        fc.store_attr()\n        self.client = QdrantClient(url=url, api_key=api_key)\n        self.points = []\n    \n    def create_collection(self, vs, distance=Distance.COSINE):\n        if not self.client.collection_exists(self.cname):\n            self.client.create_collection(collection_name=self.cname, vectors_config=VectorParams(size=vs, distance=distance))\n        return self\n    \n    def create_points(self, chunks, embeddings, start_id=0):\n        self.points = [PointStruct(id=start_id+i, vector=embeddings[i], payload=chunk) for i, chunk in enumerate(chunks)]\n        return self\n    \n    def upsert(self):\n        self.client.upsert(collection_name=self.cname, points=self.points)\n        return self\n\n    def search(self, query_vector, limit=5):\n        results = self.client.query_points(collection_name=self.cname, query=query_vector, limit=limit)\n        return results.points\n\n\nimport os\nQDRANT_HOST = \"https://76eb75ff-5ca3-4a68-93d4-ae19a9c169a4.us-west-2-0.aws.cloud.qdrant.io:6333\"\napi_key = os.getenv(\"QDRANT_API_KEY\")\ncname = 'wattbot_docs_nomic_768'\nqs = QdrantStore(QDRANT_HOST, api_key, cname).create_collection(768)\n\n\nqs.client.get_collections()\n\nCollectionsResponse(collections=[CollectionDescription(name='wattbot_docs_nomic_768'), CollectionDescription(name='wattbot_documents_768'), CollectionDescription(name='wattbot_documents')])\n\n\n\nlen(result), len(embds)\n\n(322, 322)\n\n\n\nqs = qs.create_points(result, embds)\nqs = qs.upsert()\n\nI want to search for some item in the qdrant store. Please provide some text which I can embed and pass to the search\n\n\nü§ñReplyü§ñ\nSure! You can search for something related to the Amazon sustainability report. Here‚Äôs a sample query:\nquery_text = \"What are Amazon's carbon emissions goals?\"\nquery_emb = embdr.create_embeddings([query_text])[0]\nresults = qs.search(query_emb, limit=3)\nprint(results[0].payload['text'][:200])\nWould you like to try this search?\n\nquery_text = \"What are Amazon's carbon emissions goals?\"\nquery_emb = embdr.create_embeddings([query_text])[0]\nresults = qs.search(query_emb, limit=3)\nprint(results[0].payload['text'][:200])\n\ncacy for carbon-free energy \nsolutions: We engage in partnerships, industry \ninitiatives, and public policy advocacy to advance access \nto and the expansion of carbon-free energy for Amazon, \nour cust\n\n\n\nquery_text = '5,842 labeled entries'\nquery_emb = embdr.create_embeddings([query_text])[0]\nresults = qs.search(query_emb, limit=3)\nprint(results[0].payload['text'][:200])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 query_text = '5,842 labeled entries'\n----&gt; 2 query_emb = embdr.create_embeddings([query_text])[0]\n      3 results = qs.search(query_emb, limit=3)\n      4 print(results[0].payload['text'][:200])\n\nNameError: name 'embdr' is not defined\n\n\n\nWhat is next?\n\n\nü§ñReplyü§ñ\nExcellent! Your QdrantStore is working perfectly. Now let‚Äôs build the Retriever class. This should:\n\nTake a query text\nCreate embeddings for it\nSearch the vector store\nReturn relevant chunks\n\nShould we design the Retriever class structure?Great! Your QdrantStore is working perfectly. Now let‚Äôs move on to the Retriever class. This will tie together the Embedder and VectorStore to handle the complete retrieval pipeline.\nShould we design the Retriever class next, or would you like to build the Generator class for LLM-based answer generation?\nLets build the Retriever class\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs design the Retriever class. It should connect the Embedder and VectorStore:\nclass Retriever:\n    def __init__(self, embedder, vector_store):\n        fc.store_attr()\n    \n    def retrieve(self, query, limit=5):\n        # Embed query and search\n        pass\nShould the retrieve method return just the raw results from the vector store, or should it format them in a specific way (like extracting just the text and metadata)?\nLets return it in the raw way, we can change it later if we want to\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs implement the Retriever class to return raw results:\nclass Retriever:\n    def __init__(self, embedder, vector_store):\n        fc.store_attr()\n    \n    def retrieve(self, query, limit=5):\n        query_emb = self.embedder.create_embeddings([query])[0]\n        return self.vector_store.search(query_emb, limit=limit)\nShould we test this Retriever class with your existing embedder and vector store?\n\nclass Retriever:\n    def __init__(self, embedder, vector_store):\n        fc.store_attr()\n    \n    def retrieve(self, query, limit=5):\n        query_emb = self.embedder.create_embeddings([query])[0]\n        return self.vector_store.search(query_emb, limit=limit)\n\n\nretriever = Retriever(embdr, qs)\nretriever.retrieve(query_text, limit=1)[0].payload['text'][:200]\n\n' important ways we lower electricity-\\nrelated carbon emissions are by improving energy efficiency \\nand transitioning to carbon-free energy. To date, we have \\nfocused on scaling renewable energy; going'\n\n\nWhat is next?\n\n\nü§ñReplyü§ñ\nGreat! The Retriever is working perfectly. Now let‚Äôs build the Generator class to handle LLM-based answer generation. This should:\n\nTake a query and retrieved chunks\nFormat them into a prompt\nCall the LLM to generate an answer\n\nShould we design the Generator class structure?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs design the Generator class. It should handle:\n\nLLM client initialization\nPrompt formatting with context and question\nAnswer generation with proper JSON parsing\n\nHere‚Äôs a basic structure:\nclass Generator:\n    def __init__(self, llm_client, prompt_template):\n        # Initialize LLM and prompt\n        pass\n    \n    def generate(self, query, retrieved_chunks):\n        # Format prompt and generate answer\n        pass\nShould the prompt_template be passed in at initialization, or should we have a default one that can be overridden?\nLets the pass the prompt template at the initialization\n\n\nü§ñReplyü§ñ\nGood choice! That gives maximum flexibility. Here‚Äôs the structure:\nclass Generator:\n    def __init__(self, llm_client, prompt_template):\n        fc.store_attr()\n    \n    def generate(self, query, retrieved_chunks):\n        # Format context from chunks\n        # Create prompt\n        # Call LLM\n        # Parse JSON response\n        pass\nHow should the generate method format the context from retrieved chunks? Should it extract specific fields like text, doc_id, url?\nWe can create the context context = ‚Äú‚Äù.join([f‚ÄùDoc ID: {chunk.payload[‚Äòdoc_id‚Äô]}: {chunk.payload[‚Äòurl‚Äô]}: {chunk.payload[‚Äòtitle‚Äô]}: {chunk.payload[‚Äòtext‚Äô]}‚Äù for chunk in results.points])\n\n\nü§ñReplyü§ñ\nPerfect! So the generate method will format the context like that. Here‚Äôs the implementation:\nclass Generator:\n    def __init__(self, llm_client, prompt_template):\n        fc.store_attr()\n    \n    def generate(self, query, retrieved_chunks):\n        context = \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in retrieved_chunks])\n        formatted_prompt = self.prompt_template.format(context=context, question=query)\n        response = self.llm_client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n        return fc.loads(response.choices[0].message.content)\nShould we test this Generator class with your existing LLM client and prompt?\nCan we have a create context and formating the prompt as separate function?\n\n\nü§ñReplyü§ñ\nGreat idea! Separating concerns makes the code more modular and testable. Here‚Äôs the updated structure:\nclass Generator:\n    def __init__(self, llm_client, prompt_template):\n        fc.store_attr()\n    \n    def create_context(self, retrieved_chunks):\n        return \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in retrieved_chunks])\n    \n    def format_prompt(self, query, context):\n        return self.prompt_template.format(context=context, question=query)\n    \n    def generate(self, query, retrieved_chunks):\n        context = self.create_context(retrieved_chunks)\n        formatted_prompt = self.format_prompt(query, context)\n        response = self.llm_client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n        return fc.loads(response.choices[0].message.content)\nDoes this separation work for you?\n\nclass Generator:\n    def __init__(self, llm_client, prompt_template):\n        fc.store_attr()\n    \n    def create_context(self, retrieved_chunks):\n        return \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in retrieved_chunks])\n    \n    def format_prompt(self, query, context):\n        return self.prompt_template.format(context=context, question=query)\n    \n    def generate(self, query, retrieved_chunks):\n        context = self.create_context(retrieved_chunks)\n        formatted_prompt = self.format_prompt(query, context)\n        response = self.llm_client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": formatted_prompt}])\n        return fc.loads(response.choices[0].message.content)\n\n\nquery_text = 'What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?'\nretrieved_chunks = Retriever(embdr, qs).retrieve(query_text, limit=3)\ngenerator = Generator(llm, prompt)\ngenerator.generate(query_text, retrieved_chunks)\n\n{'answer': 'Inferentia2',\n 'answer_value': 'Inferentia2',\n 'answer_unit': 'is_blank',\n 'ref_id': ['amazon2023'],\n 'ref_url': ['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],\n 'supporting_materials': 'is_blank',\n 'explanation': 'is_blank'}\n\n\nOkay it is working fine. Now what is next?\n\n\nü§ñReplyü§ñ\nExcellent! Now you have all the core components working. Let‚Äôs tie them together with a main RAG class that orchestrates everything:\nclass RAG:\n    def __init__(self, retriever, generator):\n        fc.store_attr()\n    \n    def answer_question(self, query, limit=5):\n        retrieved_chunks = self.retriever.retrieve(query, limit=limit)\n        return self.generator.generate(query, retrieved_chunks)\nShould we create this main RAG class to complete your pipeline?\n\nclass RAG:\n    def __init__(self, retriever, generator):\n        fc.store_attr()\n    \n    def answer_question(self, query, limit=5):\n        retrieved_chunks = self.retriever.retrieve(query, limit=limit)\n        return self.generator.generate(query, retrieved_chunks)\n\n\nrag = RAG(retriever, generator)\nrag.answer_question(query_text)\n\n{'answer': 'Inferentia2',\n 'answer_value': 'Inferentia2',\n 'answer_unit': 'is_blank',\n 'ref_id': ['amazon2023'],\n 'ref_url': ['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],\n 'supporting_materials': 'Inferentia2 is up to 50% more energy-efficient and can reduce costs by up to 40% against comparable Amazon Elastic Compute Cloud (EC2) instances.',\n 'explanation': 'Quote'}\n\n\nI also want an evaluator class\n\n\nü§ñReplyü§ñ\nGreat idea! The Evaluator class should handle the WattBot scoring logic. It should include:\n\nscore_answer_value() - Score with ¬±0.1% tolerance\nscore_ref_id() - Jaccard overlap scoring\nscore_is_na() - Check if unanswerable questions are handled correctly\ncalculate_wattbot_score() - Combine all scores with weights\n\nShould we implement this class with the scoring functions you already created?\nAlso it should idenity the n samples which has the lowest scores. This will be for debbuging purpose\n\n\nü§ñReplyü§ñ\nGreat addition! So the Evaluator class should also:\n\nStore individual question scores\nIdentify the N lowest-scoring samples for debugging\n\nShould we add a method like get_worst_predictions(n=5) that returns the questions with the lowest scores along with their predicted and expected answers?\nAlso if possible the retreived chunks and context\n\n\nü§ñReplyü§ñ\nPerfect! So for debugging, the Evaluator should store:\n\nIndividual scores per question\nPredicted vs expected answers\nRetrieved chunks\nGenerated context\n\nShould we design the Evaluator to store all this information during evaluation, then provide a get_worst_predictions(n=5) method that returns a detailed breakdown of the lowest-scoring questions?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs design the Evaluator class structure:\nclass Evaluator:\n    def __init__(self):\n        self.results = []  # Store all evaluation results\n    \n    def evaluate_dataset(self, rag, qa_dataframe):\n        # Run RAG on each question and store detailed results\n        pass\n    \n    def get_worst_predictions(self, n=5):\n        # Return n lowest-scoring samples with full details\n        pass\nShould we start implementing the evaluate_dataset method first, or would you like to sketch out what information should be stored in self.results?\nLets start implementing the evaluate_dataset first\n\n\nü§ñReplyü§ñ\nGreat! For the evaluate_dataset method, we need to decide what information to store for each question. Should each result include:\n\nQuestion ID and text\nPredicted answer (full JSON)\nExpected answer\nIndividual component scores (answer_value, ref_id, is_na)\nTotal WattBot score\nRetrieved chunks\nGenerated context\n\nIs this the right set of information, or would you like to add/remove anything?\nThis is the right choice, lets store it as SimpleNamespace\n\n\nü§ñReplyü§ñ\nPerfect choice! Using SimpleNamespace makes it easy to access the stored data with dot notation. Here‚Äôs the implementation:\nfrom types import SimpleNamespace\n\ndef evaluate_dataset(self, rag, qa_dataframe):\n    for _, row in qa_dataframe.iterrows():\n        # Get retrieved chunks and answer\n        # Calculate scores\n        # Store as SimpleNamespace\n        result = SimpleNamespace(\n            question_id=row['id'],\n            question=row['question'],\n            predicted=predicted_answer,\n            expected=row,\n            scores={'answer_value': av_score, 'ref_id': ref_score, 'is_na': na_score, 'total': total_score},\n            retrieved_chunks=chunks,\n            context=context\n        )\n        self.results.append(result)\nShould we implement the complete method with the scoring logic?\nInstead of the iterrows we can use the lambda with pandas right?\n\n\nü§ñReplyü§ñ\nYes! You can use apply() with a lambda function, which is often more efficient than iterrows(). Here‚Äôs the approach:\ndef evaluate_dataset(self, rag, qa_dataframe):\n    def evaluate_row(row):\n        # Evaluation logic here\n        return result\n    \n    self.results = qa_dataframe.apply(evaluate_row, axis=1).tolist()\nWould you like to implement the evaluate_row function with all the scoring logic?\nBefore evaluating I want to store the prompts in braintrust\n\n\nü§ñReplyü§ñ\nGreat idea! Braintrust is excellent for tracking prompts and experiments. To integrate it, you‚Äôll need to:\n\nInstall braintrust: pip install braintrust\nInitialize a braintrust logger in your Evaluator\nLog prompts, responses, and scores during evaluation\n\nDo you already have a Braintrust account set up, or would you like help getting started with that first?\nHow shall I create a prompt template and do versioning in braintrust?\n\n\nü§ñReplyü§ñ\nTo create and version prompts in Braintrust, you‚Äôll use their Prompts feature:\n\nCreate a prompt - Define it in the Braintrust UI or via their SDK\nVersion automatically - Braintrust tracks versions when you update prompts\nPull in code - Reference the prompt by name to use the latest or specific version\n\nYou can create prompts using:\nfrom braintrust import load_prompt\nprompt = load_prompt(\"my-prompt-name\")\nWould you like to create your WattBot prompt in Braintrust‚Äôs UI first, or would you prefer to do it programmatically via their SDK?\nHow shall I create a prompt using SDK?\n\n\nü§ñReplyü§ñ\nTo create a prompt using the Braintrust SDK, you‚Äôll use their API to define and store your prompt:\nimport braintrust\n\n# Create or update a prompt\nbraintrust.update_prompt(\n    project=\"your-project-name\",\n    slug=\"wattbot-prompt\",\n    prompt={\n        \"prompt\": your_prompt_template,\n        \"model\": \"llama-v3p1-70b-instruct\"\n    }\n)\nDo you have a Braintrust project already created, or would you like to set that up first?\nWhy the model name is necessary?\n\n\nü§ñReplyü§ñ\nGood question! The model name in Braintrust prompts is optional - it‚Äôs metadata to help you track which model a prompt was designed for. It doesn‚Äôt actually control which model runs the prompt.\nYou can omit it if you prefer:\nbraintrust.update_prompt(\n    project=\"your-project-name\",\n    slug=\"wattbot-prompt\",\n    prompt={\"prompt\": your_prompt_template}\n)\nWould you like to try creating your WattBot prompt in Braintrust now?\n\nprompt_template = \"\"\"You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor answer_value field:\n- Use 0 for False, 1 for True\n- Use exact numbers for numeric answers\n- Use exact text for categorical answers\n\nFor supporting_materials field:\n- Use exact verbatim quote if answer comes from text\n- Use \"Table X\" or \"Figure Y\" if from visual elements\n- Use \"is_blank\" if no specific reference or when unanswerable\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"ref_url\": [\"ref_url\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n}\n\nAlways start with {. \nContext: {{context}}\nQuestion: {{question}}\nResponse:\n\"\"\"\n\nCan the above prompt be coverted to mustache templating syntax?\n\n\nü§ñReplyü§ñ\nYes! Mustache uses {variable} for variables, but since your prompt already has {{ for JSON examples, you‚Äôll need to escape them or use different delimiters.\nIn Mustache, you can: 1. Use {{variable}} (triple braces) for unescaped content 2. Escape literal braces as \\{{ or by wrapping in verbatim sections\nFor your prompt, you‚Äôd replace {context} and {question} with {context} and {question}, and escape the JSON example braces.\nWould you like me to show you the converted Mustache version?\nYes\n\n\nü§ñReplyü§ñ\nHere‚Äôs your prompt converted to Mustache syntax:\nYou are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\n\nFor answer_value field:\n- Use 0 for False, 1 for True\n- Use exact numbers for numeric answers\n- Use exact text for categorical answers\n\nFor supporting_materials field:\n- Use exact verbatim quote if answer comes from text\n- Use \"Table X\" or \"Figure Y\" if from visual elements\n- Use \"is_blank\" if no specific reference or when unanswerable\n\nFor explanation field, use one of these formats:\n- \"Quote\" - if answer comes directly from text\n- \"Table\" or \"Figure\" - if from visual elements  \n- \"Math: [calculation]\" - if requires computation\n- \"is_blank\" - if unanswerable\n\n\\{\n  \"answer\": \"natural language answer\",\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\n  \"answer_unit\": \"unit or 'is_blank'\",\n  \"ref_id\": [\"doc_id\"],\n  \"ref_url\": [\"ref_url\"],\n  \"supporting_materials\": \"exact quote or reference\",\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\n\\}\n\nAlways start with \\{. \nContext: {{context}}\nQuestion: {{question}}\nResponse:\nThe key changes: {context} ‚Üí {context}, {question} ‚Üí {question}, and literal { ‚Üí \\{\nWould you like to create this prompt in Braintrust now?\n\nimport braintrust\n\nHow shall I resolve the error? And is the slug used for versioning? How shall I refer versions?\n\n\nü§ñReplyü§ñ\nLet me search for the correct Braintrust API for managing prompts.Perfect! I found the correct method. * To create a prompt in Braintrust, you use project.prompts.create() method. Here‚Äôs how to fix your code:\nproject = braintrust.projects.create(name=\"WattBot2025\")\nprompt = project.prompts.create(\n    name=\"WattBot Prompt\",\n    slug=\"wattbot-prompt\",\n    prompt={\"prompt\": prompt_template},\n    if_exists=\"replace\"\n)\n* The slug is a unique identifier you use to reference the prompt in your code - as you change the prompt‚Äôs content, the slug stays constant.\n* For versioning, Braintrust automatically tracks versions, and you can load a specific version using loadPrompt() with a version parameter. Each update creates a new version transaction ID automatically.\nShould we try this corrected approach?\nStill getting error\n\n\nü§ñReplyü§ñ\nThe issue is that braintrust.init() returns an Experiment object, not a project object with prompts. Let me search for the correct Braintrust API for creating prompts with an existing project ID.I found the correct way! * According to Braintrust docs, you use braintrust.projects.create() to get a project object, then call project.prompts.create() to create a prompt. Here‚Äôs the correct code:\nproject = braintrust.projects.create(name=\"WattBot2025\")\nprompt = project.prompts.create(\n    name=\"WattBot Prompt\",\n    slug=\"wattbot-prompt\",\n    prompt={\"prompt\": prompt_template},\n    if_exists=\"replace\"\n)\n* The slug is how you‚Äôll reference the prompt later - Braintrust automatically handles versioning when you update prompts with the same slug.\nShould we try this approach with your existing project?\nWhat if I have already a project id?\n\n\nü§ñReplyü§ñ\nIf you have an existing project ID, you can use * braintrust.projects.create(name=\"your-project-name\") to reference your project (it won‚Äôt create a duplicate if a project with that name already exists), then create prompts within it:\nproject = braintrust.projects.create(name=\"WattBot2025\")  # References existing project\nprompt = project.prompts.create(\n    name=\"WattBot Prompt\",\n    slug=\"wattbot-prompt\",\n    prompt={\"prompt\": prompt_template},\n    if_exists=\"replace\"\n)\nThe name parameter identifies which project to use - if it exists, it returns the existing project. Would you like to try this approach?\n\nimport lisette as lz\n\n\nmessages = lz.mk_msgs(prompt_template); messages\n\n[{'role': 'user',\n  'content': 'You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\\n\\nFor answer_value field:\\n- Use 0 for False, 1 for True\\n- Use exact numbers for numeric answers\\n- Use exact text for categorical answers\\n\\nFor supporting_materials field:\\n- Use exact verbatim quote if answer comes from text\\n- Use \"Table X\" or \"Figure Y\" if from visual elements\\n- Use \"is_blank\" if no specific reference or when unanswerable\\n\\nFor explanation field, use one of these formats:\\n- \"Quote\" - if answer comes directly from text\\n- \"Table\" or \"Figure\" - if from visual elements  \\n- \"Math: [calculation]\" - if requires computation\\n- \"is_blank\" - if unanswerable\\n\\n{\\n  \"answer\": \"natural language answer\",\\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\\n  \"answer_unit\": \"unit or \\'is_blank\\'\",\\n  \"ref_id\": [\"doc_id\"],\\n  \"ref_url\": [\"ref_url\"],\\n  \"supporting_materials\": \"exact quote or reference\",\\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\\n}\\n\\nAlways start with {. \\nContext: {{context}}\\nQuestion: {{question}}\\nResponse:\\n'}]\n\n\n\nllm_model = \"accounts/fireworks/models/llama-v3p1-70b-instruct\"\nproject = braintrust.projects.create(name=\"wattbot\")\nprompt = project.prompts.create(name=\"WattBot Prompt\", model = llm_model, slug=\"wattbot-prompt-v1\", messages=messages, if_exists= \"replace\")\nproject.publish()\n\n{'status': 'success'}",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "wattbot_exploration_solveit.html#ai-prompt",
    "href": "wattbot_exploration_solveit.html#ai-prompt",
    "title": "Exploration with Solveit",
    "section": "AI Prompt",
    "text": "AI Prompt\nHow to load the prompt ## AI Response üß†üß†üß†üß†üß†üß†\nTo load a prompt from Braintrust, you use the load_prompt() function with your project and slug:\nloaded_prompt = braintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt\")\nThis will load the latest version of your prompt. Would you like to try loading it?\n\nloaded_prompt = braintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt-v1\")\n\n\nloaded_prompt.prompt\n\nPromptChatBlock(messages=[PromptMessage(content='You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\\n\\nFor answer_value field:\\n- Use 0 for False, 1 for True\\n- Use exact numbers for numeric answers\\n- Use exact text for categorical answers\\n\\nFor supporting_materials field:\\n- Use exact verbatim quote if answer comes from text\\n- Use \"Table X\" or \"Figure Y\" if from visual elements\\n- Use \"is_blank\" if no specific reference or when unanswerable\\n\\nFor explanation field, use one of these formats:\\n- \"Quote\" - if answer comes directly from text\\n- \"Table\" or \"Figure\" - if from visual elements  \\n- \"Math: [calculation]\" - if requires computation\\n- \"is_blank\" - if unanswerable\\n\\n{\\n  \"answer\": \"natural language answer\",\\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\\n  \"answer_unit\": \"unit or \\'is_blank\\'\",\\n  \"ref_id\": [\"doc_id\"],\\n  \"ref_url\": [\"ref_url\"],\\n  \"supporting_materials\": \"exact quote or reference\",\\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\\n}\\n\\nAlways start with {. \\nContext: {{context}}\\nQuestion: {{question}}\\nResponse:\\n', role='user', name=None, function_call=None, tool_calls=None)], tools=None, type='chat')\n\n\nNow how shall I prefill the context and question for this prompt?\n\nü§ñReplyü§ñ\nTo fill in the {context} and {question} variables in your Braintrust prompt, you use the build() method:\nfilled_prompt = loaded_prompt.build(context=your_context, question=your_question)\nThis will replace the mustache variables with your actual values. Would you like to try this with a sample context and question?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs try filling in the prompt with sample data:\nsample_context = \"Doc ID: amazon2023\\nText: Amazon's carbon emissions goal is net-zero by 2040.\"\nsample_question = \"What is Amazon's carbon goal?\"\nfilled_prompt = loaded_prompt.build(context=sample_context, question=sample_question)\nprint(filled_prompt)\nShould we run this to see how the variables get filled in?\n\nsample_context = \"Doc ID: amazon2023\\nText: Amazon's carbon emissions goal is net-zero by 2040.\"\nsample_question = \"What is Amazon's carbon goal?\"\nfilled_prompt = loaded_prompt.build(context=sample_context, question=sample_question)\nprint(filled_prompt)\n\n{'model': 'accounts/fireworks/models/llama-v3p1-70b-instruct', 'span_info': {'metadata': {'prompt': {'variables': {'context': \"Doc ID: amazon2023\\nText: Amazon's carbon emissions goal is net-zero by 2040.\", 'question': \"What is Amazon's carbon goal?\"}, 'id': '92048cd8-fd55-4775-9b23-ca4d8cf697e6', 'project_id': '6c094e85-403b-4ea5-ae5c-03c7fad1c8bb', 'version': '1000195995678157075'}}}, 'messages': [{'content': 'You are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format:\\n\\nFor answer_value field:\\n- Use 0 for False, 1 for True\\n- Use exact numbers for numeric answers\\n- Use exact text for categorical answers\\n\\nFor supporting_materials field:\\n- Use exact verbatim quote if answer comes from text\\n- Use \"Table X\" or \"Figure Y\" if from visual elements\\n- Use \"is_blank\" if no specific reference or when unanswerable\\n\\nFor explanation field, use one of these formats:\\n- \"Quote\" - if answer comes directly from text\\n- \"Table\" or \"Figure\" - if from visual elements  \\n- \"Math: [calculation]\" - if requires computation\\n- \"is_blank\" - if unanswerable\\n\\n{\\n  \"answer\": \"natural language answer\",\\n  \"answer_value\": \"0/1 for booleans, numbers for numeric, exact text for categorical\",\\n  \"answer_unit\": \"unit or \\'is_blank\\'\",\\n  \"ref_id\": [\"doc_id\"],\\n  \"ref_url\": [\"ref_url\"],\\n  \"supporting_materials\": \"exact quote or reference\",\\n  \"explanation\": \"Quote/Table/Figure/Math: calculation/is_blank\"\\n}\\n\\nAlways start with {. \\nContext: Doc ID: amazon2023\\nText: Amazon\\'s carbon emissions goal is net-zero by 2040.\\nQuestion: What is Amazon\\'s carbon goal?\\nResponse:\\n', 'role': 'user'}]}\n\n\nNow shall I use the prompt to call the llm?\n\n\nü§ñReplyü§ñ\nYes! The build() method returns a dictionary with messages that you can pass directly to your LLM client:\nresponse = llm.chat.completions.create(**filled_prompt)\nprint(response.choices[0].message.content)\nShould we try calling the LLM with the filled prompt?\n\nimport fastcore.all as fc\nresponse = llm.chat.completions.create(filled_prompt['messages']); fc.loads(response.choices[0].message.content)\n\n{'answer': \"Amazon's carbon emissions goal is to be net-zero by 2040.\",\n 'answer_value': 'net-zero by 2040',\n 'answer_unit': 'is_blank',\n 'ref_id': ['amazon2023'],\n 'ref_url': ['is_blank'],\n 'supporting_materials': \"Amazon's carbon emissions goal is net-zero by 2040.\",\n 'explanation': 'Quote'}\n\n\nNow I want to implement the Evaluator class with the calculate_wattbot_score function\n\n\nü§ñReplyü§ñ\nPerfect! Now let‚Äôs implement the Evaluator class. It should include:\n\nThe scoring functions you already created (score_answer_value, score_ref_id, score_is_na)\ncalculate_wattbot_score to combine them\nevaluate_dataset to run evaluation on all questions\nget_worst_predictions for debugging\n\nShould we start by putting your existing scoring functions into the Evaluator class?\nYes\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs start building the Evaluator class with your existing scoring functions:\nclass Evaluator:\n    def __init__(self):\n        self.results = []\n    \n    def score_answer_value(self, predicted, expected):\n        # Your existing implementation\n        pass\n    \n    def score_ref_id(self, predicted, expected):\n        # Your existing implementation\n        pass\n    \n    def score_is_na(self, predicted_answer, expected_answer):\n        # Your existing implementation\n        pass\n    \n    def calculate_wattbot_score(self, predicted_answer, expected_row):\n        # Combine the three scores with weights\n        pass\nShould we copy your existing scoring function implementations into this class structure?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Here‚Äôs the Evaluator class with your existing scoring functions:\nimport ast\nfrom types import SimpleNamespace\n\nclass Evaluator:\n    def __init__(self):\n        self.results = []\n    \n    def is_numeric(self, value):\n        try:\n            float(value)\n            return True\n        except (ValueError, TypeError):\n            return False\n    \n    def score_answer_value(self, predicted, expected):\n        if expected == \"is_blank\":\n            return 1.0 if predicted == \"is_blank\" else 0.0\n        \n        if self.is_numeric(expected) and self.is_numeric(predicted):\n            pred_num, exp_num = map(float, (predicted, expected))\n            return 1.0 if abs(pred_num - exp_num) &lt;= abs(exp_num * 0.001) else 0.0\n        else:\n            return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0\n    \n    def score_ref_id(self, predicted, expected):\n        if expected == \"is_blank\":\n            return 1.0 if predicted == \"is_blank\" else 0.0\n        \n        if isinstance(expected, str) and expected.startswith('['): expected = ast.literal_eval(expected)\n        \n        pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])\n        exp_set = set(expected) if isinstance(expected, list) else set([expected])\n        \n        intersection = len(pred_set.intersection(exp_set))\n        union = len(pred_set.union(exp_set))\n        \n        return intersection / union if union &gt; 0 else 0.0\n    \n    def score_is_na(self, predicted_answer, expected_answer):\n        expected_is_na = expected_answer['answer_value'] == 'is_blank'\n        predicted_is_na = predicted_answer['answer_value'] == 'is_blank'\n        return 1.0 if expected_is_na == predicted_is_na else 0.0\n    \n    def calculate_wattbot_score(self, predicted_answer, expected_row):\n        answer_score = self.score_answer_value(predicted_answer['answer_value'], expected_row['answer_value'])\n        ref_score = self.score_ref_id(predicted_answer['ref_id'], expected_row['ref_id'])\n        na_score = self.score_is_na(predicted_answer, expected_row)\n        \n        return 0.75 * answer_score + 0.15 * ref_score + 0.10 * na_score\nNow should we implement the evaluate_dataset method?\n\nimport ast\nfrom types import SimpleNamespace\n\nclass EvalUtil:\n    \n    @staticmethod\n    def is_numeric(value):\n        try:\n            float(value)\n            return True\n        except (ValueError, TypeError):\n            return False\n    \n    @staticmethod\n    def score_answer_value(predicted, expected):\n        if expected == \"is_blank\":\n            return 1.0 if predicted == \"is_blank\" else 0.0\n        \n        if EvalUtil.is_numeric(expected) and EvalUtil.is_numeric(predicted):\n            pred_num, exp_num = map(float, (predicted, expected))\n            return 1.0 if abs(pred_num - exp_num) &lt;= abs(exp_num * 0.001) else 0.0\n        else:\n            return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0\n    \n    @staticmethod\n    def score_ref_id(predicted, expected):\n        if expected == \"is_blank\":\n            return 1.0 if predicted == \"is_blank\" else 0.0\n        \n        if isinstance(expected, str) and expected.startswith('['): expected = ast.literal_eval(expected)\n        \n        pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])\n        exp_set = set(expected) if isinstance(expected, list) else set([expected])\n        \n        intersection = len(pred_set.intersection(exp_set))\n        union = len(pred_set.union(exp_set))\n        \n        return intersection / union if union &gt; 0 else 0.0\n    \n    @staticmethod\n    def score_is_na(predicted_answer, expected_answer):\n        expected_is_na = expected_answer['answer_value'] == 'is_blank'\n        predicted_is_na = predicted_answer['answer_value'] == 'is_blank'\n        return 1.0 if expected_is_na == predicted_is_na else 0.0\n    \n    @staticmethod\n    def calculate_wattbot_score(predicted_answer, expected_row):\n        answer_score = EvalUtil.score_answer_value(predicted_answer['answer_value'], expected_row['answer_value'])\n        ref_score = EvalUtil.score_ref_id(predicted_answer['ref_id'], expected_row['ref_id'])\n        na_score = EvalUtil.score_is_na(predicted_answer, expected_row)\n        \n        return 0.75 * answer_score + 0.15 * ref_score + 0.10 * na_score\n\nI created evalutil class. Is this looking good?\n\n\nü§ñReplyü§ñ\nYes, looks great! Using static methods is a clean choice since these scoring functions don‚Äôt need instance state.\nNow should we create the Evaluator class that uses EvalUtil and implements the evaluate_dataset and get_worst_predictions methods?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs create the Evaluator class that uses EvalUtil. Here‚Äôs the structure:\nclass Evaluator:\n    def __init__(self, rag):\n        fc.store_attr()\n        self.results = []\n    \n    def evaluate_dataset(self, qa_dataframe, limit=5):\n        def evaluate_row(r):\n            predicted_answer = self.rag.answer_question(r['question'])\n            score = EvalUtil.calculate_wattbot_score(predicted_answer, r)\n            return fc.NS(qid=r['id'], q=r['question'], predicted=predicted_answer, expected=r, **score, context=self.rag.retrieved_chunks)\n            \n        self.results = qa_dataframe.apply(evaluate_row, axis=1).tolist()\n        return sum(r.scores['total'] for r in self.results) / len(self.results)\n    \n    def get_worst_predictions(self, n=5):\n        sorted_results = sorted(self.results, key=lambda r: r.scores['total'])\n        return sorted_results[:n]\nShould we implement the evaluate_row function inside evaluate_dataset?\n\nclass Evaluator:\n    def __init__(self, rag):\n        fc.store_attr()\n        self.results = []\n    \n    def evaluate_dataset(self, qa_dataframe, limit=5):\n        def evaluate_row(r):\n            predicted_answer = self.rag.answer_question(r['question'])\n            score = EvalUtil.calculate_wattbot_score(predicted_answer, r)\n            return fc.NS(qid=r['id'], q=r['question'], predicted=predicted_answer, expected=r, **score, context=self.rag.retrieved_chunks)\n            \n        self.results = qa_dataframe.apply(evaluate_row, axis=1).tolist()\n        return sum(r.scores['total'] for r in self.results) / len(self.results)\n    \n    def get_worst_predictions(self, n=5):\n        sorted_results = sorted(self.results, key=lambda r: r.scores['total'])\n        return sorted_results[:n]\n\nHow is it now?\n\n\nü§ñReplyü§ñ\nGood progress! A few things to consider:\n\nRetrieved chunks - Your RAG class doesn‚Äôt currently store retrieved_chunks as an attribute. Should we modify the RAG class to save the chunks it retrieves?\nScore structure - calculate_wattbot_score returns a single float, but you‚Äôre trying to unpack it with **score. Should we modify it to return a dictionary with individual scores?\n\nWhich would you like to fix first?\nI want to create a class/function which does prediction on the test file\n\n\nü§ñReplyü§ñ\nGood idea! You want a separate component for making predictions on test data (without ground truth answers).\nShould this be: 1. A method in your existing RAG class (like predict_dataset())? 2. A separate Predictor class that wraps your RAG system?\nWhich approach do you prefer?\nA method in the existing RAG class\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs add a predict_dataset method to your RAG class:\ndef predict_dataset(self, test_dataframe, limit=5):\n    def predict_row(row):\n        return self.answer_question(row['question'], limit=limit)\n    \n    return test_dataframe.apply(predict_row, axis=1)\nThis would return a Series of predictions. Does this structure work for you, or would you like it to return a full DataFrame with the predictions merged in?\nI want to log the evals into braintrust\n\n\nü§ñReplyü§ñ\nGreat idea! To log evaluations to Braintrust, you‚Äôll need to use their Eval API. You typically:\n\nCreate an evaluation experiment\nLog each question with its input, output, expected answer, and score\n\nShould we integrate Braintrust logging into your Evaluator.evaluate_dataset() method, or create a separate method for it?\nLets integrate braintrust logging into Evaulator.evaluate_dataset\n\n\nü§ñReplyü§ñ\nPerfect! To integrate Braintrust logging into evaluate_dataset, you‚Äôll need to:\n\nInitialize a Braintrust experiment at the start\nLog each evaluation with experiment.log() including input, output, expected, and scores\n\nHere‚Äôs the basic structure:\ndef evaluate_dataset(self, qa_dataframe, limit=5):\n    experiment = braintrust.init(project=\"wattbot\", experiment=\"evaluation\")\n    \n    def evaluate_row(r):\n        # ... existing evaluation logic ...\n        experiment.log(input=r['question'], output=predicted_answer, expected=r, scores={'wattbot_score': score})\n        return result\nWould you like me to show you the complete integration, or would you prefer to implement it step by step?\nCan we log some metadata\n\n\nü§ñReplyü§ñ\nYes, absolutely! Braintrust allows you to log metadata alongside your evaluations. You can include things like:\n\nModel name and parameters\nRetrieval settings (limit, chunk size)\nPrompt version\nAny other configuration details\n\nWhat specific metadata would you like to log for your evaluations?\nThe context, model name, chunksize etc\n\n\nü§ñReplyü§ñ\nGreat choices! You can log all of those as metadata in Braintrust. Here‚Äôs how you could structure it:\nexperiment.log(\n    input=r['question'],\n    output=predicted_answer,\n    expected=r,\n    scores={'wattbot_score': score},\n    metadata={\n        'model': model_name,\n        'chunk_size': chunk_size,\n        'context': context,\n        'retrieval_limit': limit\n    }\n)\nShould we update your Evaluator class to accept these configuration parameters so they can be logged as metadata?\nCan you suggest a better embedding model?\n\n\nü§ñReplyü§ñ\nGood question! For better retrieval quality, you could consider:\n\nnomic-embed-text-v1.5 (what you‚Äôre using) - 768 dims, good balance\ntext-embedding-3-large (OpenAI) - 3072 dims, higher quality but slower/costlier\nvoyage-large-2-instruct - Optimized for RAG tasks\nBGE-large-en-v1.5 - Strong open-source option\n\nThe best choice depends on your priorities. Would you like to experiment with a different model to see if it improves your retrieval scores?\nI prefer a better open source model with 768 dimension\n\n\nü§ñReplyü§ñ\nFor 768-dimensional open-source embeddings with better quality than nomic, consider:\n\nbge-large-en-v1.5 - Strong retrieval performance, widely used\ngte-large - Good for general text embedding tasks\ne5-large-v2 - Microsoft‚Äôs model, performs well on benchmarks\n\nbge-large-en-v1.5 is probably your best bet - it consistently ranks high on retrieval benchmarks and is specifically optimized for semantic search.\nWould you like to try switching to bge-large-en-v1.5 to see if it improves your retrieval quality?\nI think bge-large-en-v1.5 is not there in fireworks\n\n\nü§ñReplyü§ñ\nYou‚Äôre right! Fireworks.ai has a limited set of embedding models. Let me check what embedding models are actually available on Fireworks.\nFor Fireworks.ai, the main embedding options are: - nomic-ai/nomic-embed-text-v1.5 (what you‚Äôre using - 768 dims) - nomic-ai/nomic-embed-text-v1 (older version) - WhereIsAI/UAE-Large-V1 (1024 dims)\nIf you want to stick with Fireworks and 768 dimensions, nomic-embed-text-v1.5 is actually your best option. To try better models like bge-large, you‚Äôd need to switch to a different provider or run locally.\nWould you like to stick with Fireworks and nomic, or explore other embedding providers?\nNow how can I improve this RAG application\n\n\nü§ñReplyü§ñ\nGreat question! Here are some key areas to improve your RAG application:\n\nRetrieval quality - Test different chunk sizes, add re-ranking, or try hybrid search (semantic + keyword)\nPrompt engineering - Add few-shot examples to guide the LLM better\nEvaluation-driven iteration - Use your Evaluator to identify where it‚Äôs failing and iterate\nMetadata filtering - Filter by document type or year before retrieval\nQuery expansion - Rephrase questions to improve retrieval\n\nWhich area interests you most, or would you like to start by analyzing your current performance with the Evaluator to see where improvements are needed?\nCan you please help with hybrid search?\n\n\nü§ñReplyü§ñ\nOf course! I‚Äôd be happy to help you implement hybrid search for your RAG application.\nTo get started with hybrid search in Qdrant, we need to enable sparse vectors alongside your existing dense vectors. Qdrant supports this through their hybrid search feature.\nFirst question: Would you like me to explain the concept and steps first, or would you prefer I write the code to implement it for you?\nLets go through the concept\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs understand hybrid search step by step.\nWhat is Hybrid Search?\nHybrid search combines two retrieval methods:\n\nDense/Semantic search (what you currently have) - Uses embeddings to find semantically similar content. Great for concepts and meaning.\nSparse/Keyword search (BM25-like) - Uses exact term matching and frequency. Great for specific names, numbers, and technical terms.\n\nWhy does it help?\nFor your WattBot questions, you might ask ‚ÄúWhat is the ML.ENERGY Benchmark?‚Äù - semantic search might miss the exact name, but keyword search will catch it. Combining both gives you the best of both worlds.\nDoes this make sense so far? Should I explain how Qdrant implements this next?\nYes please\n\n\nü§ñReplyü§ñ\nGreat! Here‚Äôs how Qdrant implements hybrid search:\nQdrant‚Äôs Approach:\n\nSparse Vectors - Qdrant stores both dense (your 768-dim embeddings) and sparse vectors (keyword-based) for each chunk\nSparse Vector Generation - You need a model that creates sparse vectors from text. Common options:\n\nBM25 (traditional keyword scoring)\nSPLADE (learned sparse representations)\nQdrant‚Äôs built-in sparse vector support\n\nQuery Time - You send both dense and sparse query vectors, and Qdrant combines the results using a fusion algorithm (like RRF - Reciprocal Rank Fusion)\n\nKey Implementation Steps: - Modify your collection to support both vector types - Generate sparse vectors for your chunks - Update queries to include both vector types\nWould you like to start implementing this, beginning with updating your QdrantStore class to support sparse vectors?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs start by updating your QdrantStore class to support sparse vectors.\nFirst, when creating the collection, you‚Äôll need to configure both dense and sparse vectors:\ndef create_collection(self, vs, distance=Distance.COSINE):\n    if not self.client.collection_exists(self.cname):\n        self.client.create_collection(\n            collection_name=self.cname,\n            vectors_config={\n                \"dense\": VectorParams(size=vs, distance=distance)\n            },\n            sparse_vectors_config={\n                \"sparse\": models.SparseVectorParams()\n            }\n        )\n    return self\nThis sets up two vector types: ‚Äúdense‚Äù for your embeddings and ‚Äúsparse‚Äù for keyword-based vectors.\nShould we update your QdrantStore.create_collection() method with this configuration?\nLets have variable to introduce hybrid search or dense search\n\n\nü§ñReplyü§ñ\nGreat idea! Adding a configuration option makes your code more flexible. You could add a parameter to control whether to use hybrid or dense-only search:\ndef __init__(self, url, api_key, cname, use_hybrid=False):\n    fc.store_attr()\n    # ... rest of init\nThen in create_collection, you‚Äôd conditionally add sparse vectors based on use_hybrid. Does this approach work for you, or would you prefer a different way to configure it?\nWhat is the models in models.SparseVectorParams?\n\n\nü§ñReplyü§ñ\nGood catch! models refers to qdrant_client.models. You need to import it:\nfrom qdrant_client import models\nOr you can use the full path: qdrant_client.models.SparseVectorParams()\nWhich import style do you prefer for your code?\nI am getting error AttributeError: ‚ÄòQdrantClient‚Äô object has no attribute ‚Äòmodels‚Äô\n\n\nü§ñReplyü§ñ\nThe error is because models is not an attribute of QdrantClient - it‚Äôs a separate module you need to import directly.\nYou should import it like this:\nfrom qdrant_client import models\nThen use models.SparseVectorParams() in your code. Have you added this import at the top of your file?\nI tried with that but it is not able to find simple text match\n\n\nü§ñReplyü§ñ\nIf your hybrid search isn‚Äôt finding simple text matches, there could be a few issues:\n\nSparse vectors not generated properly - Are you creating sparse vectors for your chunks when indexing?\nQuery sparse vector missing - Are you generating a sparse vector for your query too?\nFusion scoring - The way dense and sparse scores are combined might be weighted too heavily toward dense\n\nWhich part have you implemented so far - the indexing with sparse vectors, the querying, or both?\nOkat after creating the collection what to do next?\n\n\nü§ñReplyü§ñ\nGood! After creating the collection with sparse vector support, the next step is to generate sparse vectors for your text chunks.\nYou‚Äôll need a sparse vector encoder. Qdrant recommends using models like: - fastembed library with their sparse models - SPLADE models - Or implement BM25-style sparse vectors\nHave you chosen which sparse vector model you‚Äôd like to use, or would you like a recommendation?\nWhich one is good?\n\n\nü§ñReplyü§ñ\nFor your use case, I‚Äôd recommend fastembed with their sparse models because:\n\nEasy integration - Works well with Qdrant\nGood performance - Balances speed and accuracy\nSimple API - Similar to what you‚Äôre already using for dense embeddings\n\nSpecifically, fastembed‚Äôs Splade model is a solid choice for hybrid search.\nWould you like to try implementing fastembed for sparse vectors?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! To get started with fastembed for sparse vectors, you‚Äôll need to:\n\nInstall fastembed: pip install fastembed\nImport and initialize the sparse embedding model\nGenerate sparse vectors alongside your dense embeddings\n\nHere‚Äôs how to initialize it:\nfrom fastembed import SparseTextEmbedding\nsparse_model = SparseTextEmbedding(model_name=\"prithvida/Splade_PP_en_v1\")\nWould you like to try installing fastembed and creating a sparse embedder, or would you prefer I show you how to integrate it into your existing Embedder class?\nI installed what is next?\n\n\nü§ñReplyü§ñ\nGreat! Now you need to create a sparse embedder similar to your dense Embedder class.\nShould we: 1. Create a separate SparseEmbedder class 2. Modify your existing Embedder class to handle both dense and sparse\nWhich approach do you prefer?\nLets modify the embedder class to handle both\n\n\nü§ñReplyü§ñ\nGood choice! To modify your Embedder class to handle both dense and sparse embeddings, you‚Äôll need to:\n\nAdd a sparse model parameter to __init__\nCreate a method to generate sparse embeddings\nOptionally, create a method that generates both at once\n\nShould we start by updating the __init__ method to accept an optional sparse model parameter?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs update the __init__ method to accept an optional sparse model:\ndef __init__(self, client, model_name, sparse_model=None, bs=50):\n    fc.store_attr()\n    self.gen_emb = self.client.embeddings.create\nNow you‚Äôll have self.sparse_model available. Should we add a method called create_sparse_embeddings() next that uses this sparse model?\nYes\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs add a method to create sparse embeddings:\ndef create_sparse_embeddings(self, texts):\n    if not self.sparse_model:\n        raise ValueError(\"Sparse model not initialized\")\n    return list(self.sparse_model.embed(texts))\nThis will generate sparse vectors for your texts. Should we test this method once you‚Äôve initialized an Embedder with a sparse model?\nI‚Äôve deployed an end point to huggingface hase and now I want to call that endpoint to generate the sparse embendings instead of the local model because that way it will be faster.\n\n\nü§ñReplyü§ñ\nGreat idea! Using a deployed endpoint will be much faster than running locally.\nTo call your Hugging Face endpoint, you‚Äôll need to use the requests library or Hugging Face‚Äôs InferenceClient.\nDo you have the endpoint URL ready? And would you prefer to use requests or the Hugging Face client to call it?\nI am running into some issue when deploying the model to the hugging phase endpoint. Is there any other model that generates better and bor sparse? embeddings that you can suggest which is also deployable to the hugging face endpoint.\n\n\nü§ñReplyü§ñ\nFor sparse embeddings that work well with Hugging Face endpoints, here are some good alternatives:\n\nSPLADE v2 - naver/splade-cocondenser-ensembledistil - Very effective for retrieval\nSPLADE++ - Improved version with better performance\ndistilsplade-max - Lighter and faster variant\n\nThese models are specifically designed for sparse retrieval and should deploy smoothly to Hugging Face endpoints.\nWhat issue are you running into with your current deployment? That might help me suggest the best alternative.\nI am running into the following issue. Endpoint encountered an error. You can try restarting it using the ‚Äúretry‚Äù button above. Check logs for more details. [Server message]Endpoint failed to start Exit code: 3. Reason: /neuron/modeling_traced.py‚Äù, line 358, in _export main_export( File ‚Äú/usr/local/lib/python3.10/dist-packages/optimum/exporters/neuron/main.py‚Äù, line 624, in main_export models_and_neuron_configs, output_model_names = load_models_and_neuron_configs( File ‚Äú/usr/local/lib/python3.10/dist-packages/optimum/exporters/neuron/main.py‚Äù, line 551, in load_models_and_neuron_configs model = TasksManager.get_model_from_task(**model_kwargs) File ‚Äú/usr/local/lib/python3.10/dist-packages/optimum/exporters/tasks.py‚Äù, line 2093, in get_model_from_task model = model_class( File ‚Äú/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py‚Äù, line 308, in init modules, self.module_kwargs = self._load_sbert_model( File ‚Äú/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py‚Äù, line 1661, in _load_sbert_model module_class = self._load_module_class_from_ref( File ‚Äú/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py‚Äù, line 1551, in _load_module_class_from_ref return import_from_string(class_ref) File ‚Äú/usr/local/lib/python3.10/dist-packages/sentence_transformers/util.py‚Äù, line 1126, in import_from_string module = importlib.import_module(module_path) File ‚Äú/usr/lib/python3.10/importlib/init.py‚Äù, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File ‚Äú&lt;frozen importlib._bootstrap&gt;‚Äù, line 1050, in _gcd_import File ‚Äú&lt;frozen importlib._bootstrap&gt;‚Äù, line 1027, in _find_and_load File ‚Äú&lt;frozen importlib._bootstrap&gt;‚Äù, line 992, in _find_and_load_unlocked File ‚Äú&lt;frozen importlib._bootstrap&gt;‚Äù, line 241, in _call_with_frames_removed File ‚Äú&lt;frozen importlib._bootstrap&gt;‚Äù, line 1050, in _gcd_import File ‚Äú&lt;frozen importlib._bootstrap&gt;‚Äù, line 1027, in _find_and_load File ‚Äú&lt;frozen importlib._bootstrap&gt;‚Äù, line 1004, in _find_and_load_unlocked ModuleNotFoundError: No module named ‚Äòsentence_transformers.sparse_encoder‚Äô\nApplication startup failed. Exiting.\n\n\nü§ñReplyü§ñ\nThe error shows that the Hugging Face endpoint is trying to load a sentence_transformers.sparse_encoder module that doesn‚Äôt exist. This suggests the SPLADE model you‚Äôre trying to deploy has a custom architecture that isn‚Äôt compatible with Hugging Face‚Äôs standard deployment.\nFor sparse embeddings on Hugging Face endpoints, try using a simpler approach:\nOption 1: Use a standard dense model endpoint and generate sparse vectors client-side using BM25 Option 2: Deploy a standard BERT-based model and use token-level embeddings to create sparse representations\nWould you like to explore generating sparse vectors locally using BM25 (which is fast and doesn‚Äôt need a model), or would you prefer to find a different deployable sparse model?\nLet‚Äôs try this Splade V2 which you are suggesting. If it doesn‚Äôt work then we might go for BM25.\n\n\nü§ñReplyü§ñ\nGood plan! Let‚Äôs try SPLADE v2 (naver/splade-cocondenser-ensembledistil).\nTo deploy it on Hugging Face, you‚Äôll need to: 1. Create a new inference endpoint 2. Select the model naver/splade-cocondenser-ensembledistil 3. Choose the task type (feature extraction or text embeddings)\nHave you started creating the endpoint, or would you like guidance on the deployment settings?\nThis splade v2 is also failing. I would request for another model which works in the Huggingface endpoint.\n\n\nü§ñReplyü§ñ\nI understand the frustration with deployment issues. Since SPLADE models are having compatibility problems with Hugging Face endpoints, let‚Äôs switch to BM25 for sparse vectors instead.\nBM25 is: - Fast and runs locally (no endpoint needed) - Proven effective for keyword matching - Easy to implement with libraries like rank-bm25\nWould you like to implement BM25 for your sparse embeddings? It‚Äôs actually simpler than deploying a model endpoint.\nLet‚Äôs go for it.\n\n\nü§ñReplyü§ñ\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\nPerfect! Let‚Äôs implement BM25 for sparse embeddings.\nFirst, you‚Äôll need to install the library:\npip install rank-bm25\nOnce installed, we‚Äôll need to modify your Embedder class to generate BM25-style sparse vectors.\nHave you installed rank-bm25, or would you like to do that first?\n\n!pip install rank-bm25\n\nCollecting rank-bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from rank-bm25) (2.3.3)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nInstalling collected packages: rank-bm25\nSuccessfully installed rank-bm25-0.2.2\n\n\nI install the BM25. Now what is next?\n\n\nü§ñReplyü§ñ\nGreat! Now we need to understand how BM25 works with Qdrant‚Äôs sparse vectors.\nBM25 generates term-frequency based scores, but Qdrant expects sparse vectors in a specific format: a list of (index, value) pairs where index represents token IDs.\nFor this, we have two approaches: 1. Use a tokenizer to convert text to token IDs and compute BM25 scores 2. Use fastembed‚Äôs BM25 which handles this conversion automatically\nWhich approach would you prefer to explore?\nLet‚Äôs use the easier one.\n\n\nü§ñReplyü§ñ\nThe easier approach is fastembed‚Äôs BM25 - it handles all the tokenization and sparse vector formatting for you automatically.\nYou can use it like this:\nfrom fastembed import SparseTextEmbedding\nsparse_model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\nThis creates BM25 sparse vectors in the exact format Qdrant expects. Would you like to try initializing this model?\n\nfrom fastembed import SparseTextEmbedding\nsparse_model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\n\n/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nFetching 18 files:   0%|          | 0/18 [00:00&lt;?, ?it/s]Fetching 18 files:  11%|‚ñà         | 2/18 [00:00&lt;00:01, 15.96it/s]Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00&lt;00:00, 83.86it/s]\n\n\n\nsparse_model\n\n&lt;fastembed.sparse.sparse_text_embedding.SparseTextEmbedding&gt;\n\n\nNow how shall I have all the points generated using this sparse embedding model and insert into the quadrant database?\n\n\nü§ñReplyü§ñ\nGood question! Now you need to:\n\nGenerate sparse embeddings for your text chunks\nUpdate your create_points method in QdrantStore to include both dense and sparse vectors\n\nLet‚Äôs start by testing the sparse model first. Can you try generating a sparse embedding for a sample text to see what format it returns?\nsample_text = [\"Amazon's carbon goal is net-zero by 2040\"]\nsparse_result = list(sparse_model.embed(sample_text))\nprint(sparse_result[0])\nWould you like to run this to see the sparse vector format?\n\nsample_text = [\"Amazon's carbon goal is net-zero by 2040\"]\nsparse_result = list(sparse_model.embed(sample_text))\nprint(sparse_result[0])\n\nSparseEmbedding(values=array([1.66528681, 1.66528681, 1.66528681, 1.66528681, 1.66528681,\n       1.66528681]), indices=array([ 373664883, 1322814068, 1117092987, 1902323900, 2006536704,\n        350969189]))\n\n\nAnd now how shall we deal with create endpoints?\n\n\nü§ñReplyü§ñ\nPerfect! The sparse embedding returns SparseEmbedding objects with values and indices arrays - exactly what Qdrant needs.\nNow you need to update your QdrantStore.create_points() method to handle both dense and sparse vectors.\nWhen creating points, Qdrant expects this format:\nPointStruct(\n    id=...,\n    vector={\"dense\": dense_vector, \"sparse\": sparse_vector},\n    payload=...\n)\nShould we modify your create_points method to accept both dense and sparse embeddings?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs update the create_points method to handle both dense and sparse vectors.\nThe method should now accept both embeddings (dense) and sparse_embeddings parameters. When both are provided, it creates points with hybrid vectors:\ndef create_points(self, chunks, embeddings, sparse_embeddings=None, start_id=0):\n    if sparse_embeddings:\n        # Create hybrid points with both dense and sparse\n        self.points = [PointStruct(\n            id=start_id+i, \n            vector={\"dense\": embeddings[i], \"sparse\": models.SparseVector(values=sparse_embeddings[i].values.tolist(), indices=sparse_embeddings[i].indices.tolist())},\n            payload=chunk\n        ) for i, chunk in enumerate(chunks)]\n    else:\n        # Dense only (existing behavior)\n        self.points = [PointStruct(id=start_id+i, vector=embeddings[i], payload=chunk) for i, chunk in enumerate(chunks)]\n    return self\nShould we update your QdrantStore class with this modification?\nCan you please suggest the prompt with few shot examples?\n\n\nü§ñReplyü§ñ\nGreat idea! Few-shot examples can significantly improve your LLM‚Äôs performance by showing it exactly what you expect.\nFor your WattBot prompt, you could add 2-3 examples showing: 1. A numeric answer with quote 2. A True/False answer 3. An unanswerable question (is_blank)\nWould you like me to draft a few example Q&A pairs to add to your prompt, or would you prefer to select specific examples from your training data?\n\nqa = pd.read_csv(\"wattbot_data/train_QA.csv\")\nqa.head(n=3)\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq003\nWhat is the name of the benchmark suite presen...\nThe ML.ENERGY Benchmark\nML.ENERGY Benchmark\nis_blank\n['chung2025']\n['https://arxiv.org/pdf/2505.06371']\nWe present the ML.ENERGY Benchmark, a benchmar...\nQuote\n\n\n1\nq009\nWhat were the net CO2e emissions from training...\n4.3 tCO2e\n4.3\ntCO2e\n['patterson2021']\n['https://arxiv.org/pdf/2104.10350']\n\"Training GShard-600B used 24 MWh and produced...\nQuote\n\n\n2\nq054\nWhat is the model size in gigabytes (GB) for t...\n64.7 GB\n64.7\nGB\n['chen2024']\n['https://arxiv.org/pdf/2405.01814']\nTable 3: Large language models used for evalua...\nTable 3\n\n\n\n\n\n\n\nlets draft the prompt from the qa pair\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs draft a few-shot prompt using examples from your training data. We can use the 3 examples you just displayed - they cover different types well:\n\nCategorical answer (q003 - benchmark name)\nNumeric answer (q009 - CO2 emissions)\nTable reference (q054 - model size)\n\nShould I draft the complete prompt with these examples included, showing the exact format you want the LLM to follow?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Here‚Äôs a draft prompt with few-shot examples from your training data:\nYou are answering questions for the WattBot 2025 competition. Based on the provided context, generate a response in this exact JSON format.\n\nEXAMPLES:\n\nExample 1 - Categorical Answer:\nQuestion: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\nContext: We present the ML.ENERGY Benchmark, a benchmark suite for measuring inference energy consumption...\nAnswer:\n{\n  \"answer\": \"The ML.ENERGY Benchmark\",\n  \"answer_value\": \"ML.ENERGY Benchmark\",\n  \"answer_unit\": \"is_blank\",\n  \"ref_id\": [\"chung2025\"],\n  \"ref_url\": [\"https://arxiv.org/pdf/2505.06371\"],\n  \"supporting_materials\": \"We present the ML.ENERGY Benchmark, a benchmark suite for measuring inference energy consumption...\",\n  \"explanation\": \"Quote\"\n}\n\nExample 2 - Numeric Answer:\nQuestion: What were the net CO2e emissions from training GShard-600B?\nContext: Training GShard-600B used 24 MWh and produced 4.3 tCO2e...\nAnswer:\n{\n  \"answer\": \"4.3 tCO2e\",\n  \"answer_value\": \"4.3\",\n  \"answer_unit\": \"tCO2e\",\n  \"ref_id\": [\"patterson2021\"],\n  \"ref_url\": [\"https://arxiv.org/pdf/2104.10350\"],\n  \"supporting_materials\": \"Training GShard-600B used 24 MWh and produced 4.3 tCO2e\",\n  \"explanation\": \"Quote\"\n}\n\nExample 3 - Table Reference:\nQuestion: What is the model size in gigabytes (GB) for the model Llama-3-70B-Instruct?\nContext: [Table showing model sizes]\nAnswer:\n{\n  \"answer\": \"64.7 GB\",\n  \"answer_value\": \"64.7\",\n  \"answer_unit\": \"GB\",\n  \"ref_id\": [\"chen2024\"],\n  \"ref_url\": [\"https://arxiv.org/pdf/2405.01814\"],\n  \"supporting_materials\": \"Table 3: Large language models used for evaluation\",\n  \"explanation\": \"Table 3\"\n}\n\nINSTRUCTIONS:\n- Use 0 for False, 1 for True\n- Use exact numbers for numeric answers\n- Use \"is_blank\" when no evidence exists\n- Always start with {\n\nContext: {{context}}\nQuestion: {{question}}\nResponse:\nDoes this structure look good, or would you like to adjust the examples or format?",
    "crumbs": [
      "Exploration with Solveit"
    ]
  },
  {
    "objectID": "retriever.html",
    "href": "retriever.html",
    "title": "Retriever",
    "section": "",
    "text": "load_dotenv()\n\nTrue",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#load-data",
    "href": "retriever.html#load-data",
    "title": "Retriever",
    "section": "Load Data",
    "text": "Load Data\nLoading the files - metadata - train - test\nand viewing them\n\nmd = eda.metadata()\nmd.head()\n\n\n\n\n\n\n\n\nid\ntype\ntitle\nyear\ncitation\nurl\n\n\n\n\n0\namazon2023\nreport\n2023 Amazon Sustainability Report\n2023\nAmazon Staff. (2023). Amazon Sustainability Re...\nhttps://sustainability.aboutamazon.com/2023-am...\n\n\n1\nchen2024\npaper\nEfficient Heterogeneous Large Language Model D...\n2024\nShaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...\nhttps://arxiv.org/pdf/2405.01814\n\n\n2\nchung2025\npaper\nThe ML.ENERGY Benchmark: Toward Automated Infe...\n2025\nJae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...\nhttps://arxiv.org/pdf/2505.06371\n\n\n3\ncottier2024\npaper\nThe Rising Costs of Training Frontier AI Models\n2024\nBen Cottier, Robi Rahman, Loredana Fattorini, ...\nhttps://arxiv.org/pdf/2405.21015\n\n\n4\ndodge2022\npaper\nMeasuring the Carbon Intensity of AI in Cloud ...\n2022\nJesse Dodge, Taylor Prewitt, Remi Tachet Des C...\nhttps://arxiv.org/pdf/2206.05229\n\n\n\n\n\n\n\n\nqa = eda.train()\nqa.head()\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq003\nWhat is the name of the benchmark suite presen...\nThe ML.ENERGY Benchmark\nML.ENERGY Benchmark\nis_blank\n['chung2025']\n['https://arxiv.org/pdf/2505.06371']\nWe present the ML.ENERGY Benchmark, a benchmar...\nQuote\n\n\n1\nq009\nWhat were the net CO2e emissions from training...\n4.3 tCO2e\n4.3\ntCO2e\n['patterson2021']\n['https://arxiv.org/pdf/2104.10350']\n\"Training GShard-600B used 24 MWh and produced...\nQuote\n\n\n2\nq054\nWhat is the model size in gigabytes (GB) for t...\n64.7 GB\n64.7\nGB\n['chen2024']\n['https://arxiv.org/pdf/2405.01814']\nTable 3: Large language models used for evalua...\nTable 3\n\n\n3\nq062\nWhat was the total electricity consumption of ...\nUnable to answer with confidence based on the ...\nis_blank\nMWh\nis_blank\nis_blank\nis_blank\nis_blank\n\n\n4\nq075\nTrue or False: Hyperscale data centers in 2020...\nTRUE\n1\nis_blank\n['wu2021b','patterson2021']\n['https://arxiv.org/abs/2108.06738','https://a...\nWu 2021, body text near Fig. 1: \"‚Ä¶between trad...\nThe &gt;40% statement is explicit in Wu. Patterso...\n\n\n\n\n\n\n\n\ntst = eda.test()\ntst.head()\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq001\nWhat was the average increase in U.S. data cen...\nNaN\nNaN\npercent\nNaN\nNaN\nNaN\nNaN\n\n\n1\nq002\nIn 2023, what was the estimated amount of cars...\nNaN\nNaN\ncars\nNaN\nNaN\nNaN\nNaN\n\n\n2\nq004\nHow many data centers did AWS begin using recy...\nNaN\nNaN\ndata centers\nNaN\nNaN\nNaN\nNaN\n\n\n3\nq005\nSince NVIDIA doesn't release the embodied carb...\nNaN\nNaN\nkg/GPU\nNaN\nNaN\nNaN\nNaN\n\n\n4\nq006\nBy what factor was the estimated amortized tra...\nNaN\nNaN\nratio\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe have to fill up the answer, answer_value, answer_unit, ref_id, ref_url, supporting_materials and explanation here.\nFrom the competition following values are expected\n\nanswer: A clear natural-language response (e.g., 1438 lbs, Water consumption, TRUE)‚Äô. If no answer is possible, use ‚ÄúUnable to answer with confidence based on the provided documents.‚Äù\nanswer_value: The normalized numeric or categorical value (e.g., 1438, Water consumption, 1)\n\nIf no answer is possible, use is_blank\nRanges should be encoded as [low,high]\nDo not include symbols like &lt;, &gt;, ~ here. Those can be left in the clear natural language column.\n\nanswer_unit: Unit of measurement (e.g., lbs, kWh, gCO2, projects, is_blank).\nref_id: One or more document IDs from metadata.csv that support the answer.\nref_url: One or more URL(s) of the cited document(s).\nsupporting_materials: Verbatim justification from the cited document (quote, table reference, figure reference, etc.).\nexplanation: Short reasoning describing why the cited material supports the answer.",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#read-pdf",
    "href": "retriever.html#read-pdf",
    "title": "Retriever",
    "section": "Read pdf",
    "text": "Read pdf\nI already downloaded all the pdfs, please refer the notebook 00_eda\nWe will extract the content from the pdfs here using answerdotai‚Äôs contextkit library which uses pypdf underneath.\npypdf does a decent job of text extraction from pdf but it does not preserve the layouts, table structure and reading order.\n\nget_metadata('chen2024')\n\n{'id': 'chen2024',\n 'type': 'paper',\n 'title': 'Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n 'year': 2024,\n 'citation': 'Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n 'url': 'https://arxiv.org/pdf/2405.01814'}\n\n\n\ndoc_id = 'chen2024'\nfc.test_eq(get_metadata(doc_id)['id'], doc_id)\n\n\ndoc = read_doc('chen2024')\ndoc.content[:100], doc.id\n\n('Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan C',\n 'chen2024')\n\n\n\ndoc_id='chen2024'\ndoc = read_doc(doc_id)\nfc.test_ne(len(doc.content), 0)\nfc.test_eq(doc.id, doc_id)",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#read-markdown",
    "href": "retriever.html#read-markdown",
    "title": "Retriever",
    "section": "Read Markdown",
    "text": "Read Markdown\n\nlen(read_markdown('chen2024').content), len(read_doc('chen2024').content)\n\n(75025, 69175)",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#total-content-size",
    "href": "retriever.html#total-content-size",
    "title": "Retriever",
    "section": "Total content size",
    "text": "Total content size\n\nfc.L(eda.metadata()['id'].to_list()).map(lambda x: len(read_doc(x).content)).sum()\n\n2673613\n\n\nI dont think any open source models can handle that many characters in their context window as of November 2025.\nA RAG based system will be good where we chunk the content, retrieve the relavent chunk and generate answer with those relevant chunk",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#document-chunks",
    "href": "retriever.html#document-chunks",
    "title": "Retriever",
    "section": "Document Chunks",
    "text": "Document Chunks\n\ncontent, metadata = get_content_metadata(read_doc, 'chen2024')\ncontent[:100], metadata\n\n('Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan C',\n {'id': 'chen2024',\n  'type': 'paper',\n  'title': 'Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n  'year': 2024,\n  'citation': 'Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n  'url': 'https://arxiv.org/pdf/2405.01814'})\n\n\n\ndoc = read_doc('chen2024')\nlen(doc.content)\n\n69175\n\n\n\nchunks = chunk_doc('chen2024')\nlen(chunks), chunks[0]['text'][-200:]\n\n(50,\n ' performance and cost efficiency. Our com-\\nprehensive analysis and experiments confirm the viability\\nof splitting the attention computation over multiple devices.\\nAlso, the communication bandwidth req')\n\n\n\ndoc_id = 'chen2024'\nchunks = chunk_doc(doc_id)\nfc.test_ne(len(chunks), 0)\nfc.test_eq(chunks[0].id, doc_id)\n\n\nchunks[0]['text'][:200]\n\n'Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan Chen1 Wencong Xiao2 Yutong Lin1 Mingxing Zhang1 Yingdi Shan1 Jinlei Jiang1\\nKang Chen1 Yongwei Wu1\\n1Ts'\n\n\n\nchunks[0]\n\nnamespace(text='Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan Chen1 Wencong Xiao2 Yutong Lin1 Mingxing Zhang1 Yingdi Shan1 Jinlei Jiang1\\nKang Chen1 Yongwei Wu1\\n1Tsinghua University\\n2ByteDance\\nAbstract\\nTransformer-based large language models (LLMs) exhibit\\nimpressive performance in generative tasks but also intro-\\nduce significant challenges in real-world serving due to in-\\nefficient use of the expensive, computation-optimized accel-\\nerators. Although disaggregated serving architectures have\\nbeen proposed to split different phases of LLM inference, the\\nefficiency of decoding phase is still low. This is caused by\\nthe varying resource demands of different operators in the\\ntransformer-based LLMs. Specifically, the attention operator\\nis memory-intensive, exhibiting a memory access pattern that\\nclashes with the strengths of modern accelerators, especially\\nfor long context requests.\\nTo enhance the efficiency of LLM decoding, we introduce\\nmodel-attention disaggregation. This approach leverages a\\ncollection of cheap, memory-optimized devices for the atten-\\ntion operator while still utilizing high-end accelerators for\\nother parts of the model. This heterogeneous setup ensures\\nthat each component is tailored to its specific workload, max-\\nimizing overall performance and cost efficiency. Our com-\\nprehensive analysis and experiments confirm the viability\\nof splitting the attention computation over multiple devices.\\nAlso, the communication bandwidth req',\n          chunk_id=0,\n          id='chen2024',\n          type='paper',\n          title='Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n          year=2024,\n          citation='Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n          url='https://arxiv.org/pdf/2405.01814')\n\n\n\nchunks[-1]\n\nnamespace(text='i Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\\nand Luke Zettlemoyer. Opt: Open pre-trained trans-\\nformer language models, 2022.\\n[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-\\nServe: Disaggregating prefill and decoding for goodput-\\noptimized large language model serving. In 18th\\nUSENIX Symposium on Operating Systems Design and\\nImplementation (OSDI 24), pages 193‚Äì210, 2024.\\n16',\n          chunk_id=49,\n          id='chen2024',\n          type='paper',\n          title='Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n          year=2024,\n          citation='Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n          url='https://arxiv.org/pdf/2405.01814')",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#markdown-chunks",
    "href": "retriever.html#markdown-chunks",
    "title": "Retriever",
    "section": "Markdown Chunks",
    "text": "Markdown Chunks\n\nchunk_size = 375\nchunk_overlap = 125\nmd_splitter = MarkdownTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n\n\nmd_splitter\n\n&lt;langchain_text_splitters.markdown.MarkdownTextSplitter&gt;\n\n\n\ndoc_id = 'chen2024'\nmd_content = read_markdown(doc_id).content\nchunks = md_splitter.split_text(md_content)\n\n\nchunks[0][-1200:]\n\n'# Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation\\n\\nShaoyuan Chen&lt;sup&gt;1&lt;/sup&gt; Wencong Xiao&lt;sup&gt;2&lt;/sup&gt; Yutong Lin&lt;sup&gt;1&lt;/sup&gt; Mingxing Zhang&lt;sup&gt;1&lt;/sup&gt; Yingdi Shan&lt;sup&gt;1&lt;/sup&gt; Jinlei Jiang&lt;sup&gt;1&lt;/sup&gt;  \\nKang Chen&lt;sup&gt;1&lt;/sup&gt; Yongwei Wu&lt;sup&gt;1&lt;/sup&gt;\\n\\n&lt;sup&gt;1&lt;/sup&gt;Tsinghua University\\n\\n&lt;sup&gt;2&lt;/sup&gt;ByteDance\\n\\n## Abstract\\n\\nTransformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.'\n\n\n\nchunks[1][:1200]\n\n'Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.\\n\\nTo enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model. This heterogeneous setup ensures that each component is tailored to its specific workload, maximizing overall performance and cost efficiency. Our comprehensive analysis and experiments confirm the viability of splitting the attention computation over mult'\n\n\n\nchunks = md_splitter.chunk_markdown(doc_id)\n\n\nchunks[0].text[-900:]\n\n'p&gt; Yutong Lin&lt;sup&gt;1&lt;/sup&gt; Mingxing Zhang&lt;sup&gt;1&lt;/sup&gt; Yingdi Shan&lt;sup&gt;1&lt;/sup&gt; Jinlei Jiang&lt;sup&gt;1&lt;/sup&gt;  \\nKang Chen&lt;sup&gt;1&lt;/sup&gt; Yongwei Wu&lt;sup&gt;1&lt;/sup&gt;\\n\\n&lt;sup&gt;1&lt;/sup&gt;Tsinghua University\\n\\n&lt;sup&gt;2&lt;/sup&gt;ByteDance\\n\\n## Abstract\\n\\nTransformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.'\n\n\n\nchunks[1].text[:900]\n\n'Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.\\n\\nTo enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end'",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#human-readable-chunk",
    "href": "retriever.html#human-readable-chunk",
    "title": "Retriever",
    "section": "Human readable Chunk",
    "text": "Human readable Chunk\nThis will help later in creating context for the prompt\n\nprint(Nugget(chunks[42]))\n\n### Chunk 0\n            Text: ![](_page_8_Figure_0.jpeg)\n\n(a) Request-level partition.\n\n(b) Head-level partition.\n\nFigure 9: Work partition methods of the attention operator.\n\nstore the KV caches and compute the attention operators. As depicted in Figure 9, the attention operators can be parallelized among memory devices in various ways. One method is to distribute different requests across different devices; an alternative strategy is to partition and distribute the attention heads, which can also be computed independently, to different devices. The head-level partitioning approach ensures a balanced workload distribution, whereas the request-level partitioning may result in load imbalance due to the differences in sequence lengths and therefore the KV cache sizes among requests. However, head-level partitioning has limited flexibility, as it requires the number of memory devices to be divisible by the number of attention heads. We opt for head-level partitioning in Lamina, which offers optimal load balancing.\n\n## 6 Evaluation\n\n**Testbed.** We deploy Lamina on a real heterogeneous cluster with two kinds of GPU nodes. Each node consists of either eight H100 or H20 GPUs, and each GPU is paired with a dedicated ConnectX-7 NIC via PCIe switch. The GPU nodes are interconnected with 400 Gbps RoCE network. We use H100 as compute-optimized GPUs and H20 as memory-optimized GPUs for Lamina.\n            Chunk Id: 42\n            Doc ID: chen2024\n            Type: paper\n            Title: Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation\n            Year: 2024\n            Citation: Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814\n            URL: https://arxiv.org/pdf/2405.01814",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#chunks-all-the-docs",
    "href": "retriever.html#chunks-all-the-docs",
    "title": "Retriever",
    "section": "Chunks all the docs",
    "text": "Chunks all the docs\n\nall_chunks = chunk_all(chunk_doc)\nlen(all_chunks)\n\n1927\n\n\n\nall_chunks[0]\n\nnamespace(text='Amazon \\nSustainability \\nReport\\n2023 Contents\\nOverview\\n3 Introduction\\n4 A Letter from Our Chief \\nSustainability Officer\\xa0\\n5 How We Work\\n6 Goals Summary\\n7 2023 Year in Review \\xa0\\nEnvironment\\n9 Carbon\\n24 Carbon-Free Energy\\n29 Packaging \\n34 Waste and Circularity\\n40 Water\\nValue Chain\\n45 Human Rights \\n50 Responsible Supply Chain\\n58 Sustainable Products and \\nMaterials \\n64 Supplier Diversity \\n67 Community Impact\\nPeople\\n75 Employee Experience\\n81 Health and Safety\\n86 Inclusive Experiences\\nAppendix\\n94  Sustainability Reporting Topic \\nAssessment\\n95  Endnotes\\n96 Assurance Statements \\n97 Disclaimer and Forward-Looking \\nStatements \\nOn the cover  \\nThe Baldy Mesa Solar and Storage Project (developed \\nand operated by AES), located in Adelanto, California. Employees inside one of our newest office buildings in Bellevue, \\nWashington.\\nIntroduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Letter\\nAbout This Report\\nThis is our sixth annual report detailing progress against \\nour goals\\xa0  and environmental, social, and governance \\ntopics. All financial figures are reported in U.S. dollars ($), \\nunless otherwise stated. The data within this report reflects \\nprogress from January 1 through December 31, 2023, unless \\notherwise indicated. This report includes information about \\nmany business units and subsidiaries including AWS, Devices, \\nFresh, Whole Foods Market, Amazon Private Brands, Twitch, \\nMGM Studios, and Ring.\\nOur 2023 Sustainability Report is structured into three \\nmain categories: Environment',\n          chunk_id=0,\n          id='amazon2023',\n          type='report',\n          title='2023 Amazon Sustainability Report',\n          year=2023,\n          citation='Amazon Staff. (2023). Amazon Sustainability Report. https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf',\n          url='https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf')\n\n\n\nall_chunks[-1]\n\nnamespace(text='\\nWeidinger, L., Mellor, J., et al.: Ethical and social risks of harm from language models.\\narXiv preprint arXiv:2112.04359 (2021)\\n25',\n          chunk_id=1926,\n          id='zschache2025',\n          type='paper',\n          title='Comparing energy consumption and accuracy in text classification inference',\n          year=2025,\n          citation='Johannes Zschache, & Tilman Hartwig (2025). Comparing energy consumption and accuracy in text classification inference arXiv. https://arxiv.org/pdf/2508.14170',\n          url='https://arxiv.org/pdf/2508.14170 ')",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#neighbour-chunks",
    "href": "retriever.html#neighbour-chunks",
    "title": "Retriever",
    "section": "Neighbour Chunks",
    "text": "Neighbour Chunks\n\nChunks(all_chunks).get_chunk(1850)\n\nnamespace(text='ne-tune the full BlackMamba model (i.e.,\\noriginal weight matrices), whereas employed QLoRA [15]\\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\\nGPU memory capacity budget. For QLoRA, we target the\\nMoE layers, including the routers, and set the rank of the\\nLoRA modules to 16. We enable FlashAttention2 [17] during\\nMixtral fine-tuning for enhanced efficiency. Moreover, we use\\ngradient checkpointing [18] to save memory usage.\\nDatasets. Our fine-tuning process is implemented in Py-\\nTorch using the LLaMA-Factory framework [19], with a\\nlearning rate of 5e-5 and 10 epochs. Both models were fine-\\ntuned on two datasets focused on different tasks: common-\\nsense 15k (CS) and Math 14k (MATH), which address com-\\nmonsense reasoning and arithmetic reasoning respectively\\n(provided by LLM-adapters [20]). The details of datasets\\nare used in Table II. For evaluation, we tested the models\\non GSM8K [21] for arithmetic reasoning and HE [22] for\\ncommonsense reasoning. Each dataset consists of thousands\\nof queries. We define a query as the concatenation of a\\nprompt and its ground-truth answer, which is feed to LLMs\\nfor fine-tuning.\\nProfiling experiments. We evaluate the fine-tuning pro-\\ncess from both software and hardware perspectives. The\\nsoftware evaluation includes an end-to-end assessment of\\nthe fine-tuning process and measures the performance of\\nthe two models on various tasks post-fine-tuning. Using\\nPyTorch, we provide essential algorithm-level information\\nsuch as test accuracy, t',\n          chunk_id=1850,\n          id='xia2024',\n          type='paper',\n          title='Understanding the Performance and Estimating the Cost of LLM Fine-Tuning',\n          year=2024,\n          citation='Yuchen Xia, Jiho Kim, Yuhan Chen, Haojie Ye, Souvik Kundu, Cong Hao, Nishil Talati. (2024). Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. arXiv. https://arxiv.org/pdf/2408.04693',\n          url='https://arxiv.org/pdf/2408.04693')\n\n\n\nleft_chunk, right_chunk = Chunks(all_chunks).get_neighbours(1850)\nfc.L(left_chunk, right_chunk).attrgot('chunk_id')\n\n(#2) [1849,1851]\n\n\n\nsome_dup_chunks = fc.L(Chunks(all_chunks).get_chunk(cid) for cid in [1850, 1851, 1852, 1853, 1850, 1851, 1852])\n\n\nChunks.unique(some_dup_chunks).attrgot('chunk_id')\n\n(#4) [1850,1851,1852,1853]\n\n\n\nans = Chunks(all_chunks).include_neighbours(some_dup_chunks)\nans.attrgot('chunk_id')\n\n(#6) [1849,1850,1851,1852,1853,1854]",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#lexical-search",
    "href": "retriever.html#lexical-search",
    "title": "Retriever",
    "section": "Lexical Search",
    "text": "Lexical Search\n\nWe will use BM25 here\n\n\nidx = np.random.randint(0, len(all_chunks))\nquery = all_chunks[idx].text\nall_chunks[idx]\n\nnamespace(text='easoning modes\\n8 Discussion and Policy Implications\\n8.1 The Critical Role of Infrastructure in AI Sustainability\\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\\nDeepSeek‚Äôs own servers exhibit water consumption and carbon emissions nearly six times higher than\\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\\ngains can stem as much from datacenter design as from model optimization. These observations\\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\\nrenewable energy sources, and infrastructure-aware deployment strategies.\\n8.2 Rebound Effects and the Jevons Paradox\\nAlthough large language models consume significantly less energy, water, and carbon per task than\\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\\nAs p',\n          chunk_id=869,\n          id='jegham2025',\n          type='paper',\n          title='How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference',\n          year=2025,\n          citation='Nidhal Jegham, Marwan Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi (2025). How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference. arXiv. https://arxiv.org/pdf/2505.09598',\n          url='https://arxiv.org/pdf/2505.09598')\n\n\n\ndef get_random_chunk(chunks): return all_chunks[np.random.randint(0, len(all_chunks))]\n\n\nget_random_chunk(chunks).text[:100]\n\n'eter, Compute and Data Trends in Machine Learning.https://epochai.org/data/epochdb/\\nvisualization, 2'\n\n\n\ntokenized_query = tokenize(query)\ntokenized_query[:10]\n\n['easoning',\n 'modes',\n '8',\n 'discussion',\n 'and',\n 'policy',\n 'implications',\n '8.1',\n 'the',\n 'critical']\n\n\n\nbm25 = bm25chunks(all_chunks)\nbm25.corpus_size\n\n1927\n\n\n\nbm25.get_top_n(tokenized_query, all_chunks, n=1)\n\n[namespace(text='easoning modes\\n8 Discussion and Policy Implications\\n8.1 The Critical Role of Infrastructure in AI Sustainability\\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\\nDeepSeek‚Äôs own servers exhibit water consumption and carbon emissions nearly six times higher than\\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\\ngains can stem as much from datacenter design as from model optimization. These observations\\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\\nrenewable energy sources, and infrastructure-aware deployment strategies.\\n8.2 Rebound Effects and the Jevons Paradox\\nAlthough large language models consume significantly less energy, water, and carbon per task than\\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\\nAs p',\n           chunk_id=869,\n           id='jegham2025',\n           type='paper',\n           title='How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference',\n           year=2025,\n           citation='Nidhal Jegham, Marwan Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi (2025). How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference. arXiv. https://arxiv.org/pdf/2505.09598',\n           url='https://arxiv.org/pdf/2505.09598')]\n\n\n\nls = LexicalSearch(all_chunks)\nlexical_res = ls.search(query, n=1)\nlexical_res\n\n(#1) [NS(text='easoning modes\\n8 Discussion and Policy Implications\\n8.1 The Critical Role of Infrastructure in AI Sustainability\\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\\nDeepSeek‚Äôs own servers exhibit water consumption and carbon emissions nearly six times higher than\\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\\ngains can stem as much from datacenter design as from model optimization. These observations\\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\\nrenewable energy sources, and infrastructure-aware deployment strategies.\\n8.2 Rebound Effects and the Jevons Paradox\\nAlthough large language models consume significantly less energy, water, and carbon per task than\\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\\nAs p', chunk_id=869, id='jegham2025', type='paper', title='How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference', year=2025, citation='Nidhal Jegham, Marwan Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi (2025). How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference. arXiv. https://arxiv.org/pdf/2505.09598', url='https://arxiv.org/pdf/2505.09598')]\n\n\n\nls = LexicalSearch(all_chunks, neighbour_chunks=True)\nls.search(query, n=1).attrgot('chunk_id')\n\n(#3) [868,869,870]",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#semantic-search",
    "href": "retriever.html#semantic-search",
    "title": "Retriever",
    "section": "Semantic Search",
    "text": "Semantic Search\n\nembed_model = 'nomic-ai/nomic-embed-text-v1.5'\n\n\nutils.fw().embed(embed_model, ['hi', 'anubhav']).shape\n\n(2, 768)\n\n\n\ntexts = all_chunks.attrgot('text')\nlen(texts), texts[0][:100]\n\n(1927,\n 'Amazon \\nSustainability \\nReport\\n2023 Contents\\nOverview\\n3 Introduction\\n4 A Letter from Our Chief \\nSust')\n\n\n\nembeddings = utils.fw().embed(embed_model, texts)\n\n\nembeddings.shape\n\n(1927, 768)\n\n\n\neda.data_path\n\nPath('../data')\n\n\n\nchunks_embeddings = embed_chunks(all_chunks)    \nlen(chunks_embeddings[0]), chunks_embeddings[-1].shape\n\n(1927, (1927, 768))\n\n\n\nrandom_chunk = get_random_chunk(all_chunks)\nrandom_chunk\n\nnamespace(text=' Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language\\nUnderstanding. arXiv:1810.04805 [cs.CL]\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\\n[11] Michael Gillenwater. 2008. Redefining RECs‚ÄîPart 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109‚Äì2119.\\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-\\ngoogle-cloud-region-with-the-lowest-co2\\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint\\narXiv:2006.06217 (2020).\\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\\nIEEE, 854‚Äì867.\\n[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dz',\n          chunk_id=530,\n          id='dodge2022',\n          type='paper',\n          title='Measuring the Carbon Intensity of AI in Cloud Instances',\n          year=2022,\n          citation=\"Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, Will Buchanan (2022). Measuring the Carbon Intensity of AI in Cloud Instances. FAccT '22. https://arxiv.org/pdf/2206.05229\",\n          url='https://arxiv.org/pdf/2206.05229')\n\n\n\nquery_embedding = utils.fw().embed(embed_model, random_chunk.text)\nquery_embedding.shape\n\n(1, 768)\n\n\n\nk = 10\nall_chunks, all_embeddings = chunks_embeddings\nscores = cosine_similarity(query_embedding, all_embeddings)\nbest_k_ind = np.argsort(scores)[0].tolist()[::-1][:k]\ntop_k_chunks = all_chunks[best_k_ind]\n\n\ntop_k_chunks[0]\n\nnamespace(text=' Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language\\nUnderstanding. arXiv:1810.04805 [cs.CL]\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\\n[11] Michael Gillenwater. 2008. Redefining RECs‚ÄîPart 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109‚Äì2119.\\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-\\ngoogle-cloud-region-with-the-lowest-co2\\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint\\narXiv:2006.06217 (2020).\\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\\nIEEE, 854‚Äì867.\\n[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dz',\n          chunk_id=530,\n          id='dodge2022',\n          type='paper',\n          title='Measuring the Carbon Intensity of AI in Cloud Instances',\n          year=2022,\n          citation=\"Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, Will Buchanan (2022). Measuring the Carbon Intensity of AI in Cloud Instances. FAccT '22. https://arxiv.org/pdf/2206.05229\",\n          url='https://arxiv.org/pdf/2206.05229')\n\n\n\nss = SemanticSearch(all_chunks)\nsemantic_res = ss.search(random_chunk.text, n=1)\nlen(semantic_res), semantic_res[0]\n\n(1,\n namespace(text=' Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language\\nUnderstanding. arXiv:1810.04805 [cs.CL]\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\\n[11] Michael Gillenwater. 2008. Redefining RECs‚ÄîPart 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109‚Äì2119.\\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-\\ngoogle-cloud-region-with-the-lowest-co2\\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint\\narXiv:2006.06217 (2020).\\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\\nIEEE, 854‚Äì867.\\n[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dz',\n           chunk_id=530,\n           id='dodge2022',\n           type='paper',\n           title='Measuring the Carbon Intensity of AI in Cloud Instances',\n           year=2022,\n           citation=\"Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, Will Buchanan (2022). Measuring the Carbon Intensity of AI in Cloud Instances. FAccT '22. https://arxiv.org/pdf/2206.05229\",\n           url='https://arxiv.org/pdf/2206.05229'))\n\n\n\nss = SemanticSearch(all_chunks, neighbour_chunks=True)\nss.search(random_chunk.text, n=1).attrgot('chunk_id')\n\n(#3) [529,530,531]",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "retriever.html#hybrid-rerank",
    "href": "retriever.html#hybrid-rerank",
    "title": "Retriever",
    "section": "Hybrid: Rerank",
    "text": "Hybrid: Rerank\nHere we will rerank the outputs from semantic search and lexical search using a reranker model.\nThere are other ways to mix the outputs from the above two searches like Reciprocal Rank Fusion (RRF), Linear Combination etc which you can try later\n\ncombined_res = combine_chunks(semantic_res, lexical_res)\nlen(combined_res)\n\n2\n\n\n\ncombined_res.attrgot('chunk_id')\n\n(#2) [530,6]\n\n\n\nranker = utils.Reranker()\n\n\nquery[:100]\n\n'on Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Let'\n\n\n\ncombined_res[-1].text[:100]\n\n'on Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Let'\n\n\n\nranker.rerank_chunks(combined_res[-1].text, combined_res)[0].text[:100]\n\n'on Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Let'\n\n\n\nhs = HybridSearch(ls, ss)\nchunks_res = hs.search(combined_res[-1].text)\nchunks_res[0].text[:100]\n\n'23 Amazon Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We Work'\n\n\n\nhs = HybridSearch(ls, ss, neighbour_chunks=True)\nhs.search(combined_res[-1].text, n=1).attrgot('chunk_id')\n\n(#3) [10,11,12]",
    "crumbs": [
      "Retriever"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "",
    "text": "5th place solution for the WattBot 2025 Kaggle Competition - a Retrieval-Augmented Generation (RAG) system for answering technical questions from PDF documents.",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Overview",
    "text": "Overview\nThis project implements a RAG pipeline that achieved 5th place on the private leaderboard (10th on public) in the WattBot 2025 competition. The system extracts information from technical PDFs, chunks and indexes the content, retrieves relevant passages, and generates answers using open-source LLMs.",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Key Features",
    "text": "Key Features\n\nPDF Extraction: Extraction preserving layout, tables, and images using Datalab\nHybrid Search: Combines lexical (BM25) and semantic search with reranking\nMultiple LLM Support: Integration with Fireworks AI models (DeepSeek v3.1, Kimi k2.5, GPT-OSS 20B)\nEvaluation Framework: Built-in evaluation using Braintrust platform with WattBot scoring metrics\nSliding Window Chunking: Overlapping chunks maintain contextual continuity\nNeighbor Chunk Inclusion: Retrieves adjacent chunks for better context",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#architecture",
    "href": "index.html#architecture",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Architecture",
    "text": "Architecture\nThe pipeline consists of five main stages:\n\nPDF Extraction ‚Üí Content with preserved structure\nChunking ‚Üí Sliding window approach with configurable overlap\nIndexing ‚Üí BM25 lexical index + semantic embeddings (Qwen 8B)\nRetrieval ‚Üí Hybrid search with Qwen 8B reranker\nGeneration ‚Üí JSON-formatted answers from open-source LLMs",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Installation",
    "text": "Installation\n\nClone the repository\ngit clone https://github.com/anubhavmaity/wattbot.git \ncd wattbot\n\n\nCreate and activate environment using uv\nuv venv\nsource .venv/bin/activate\n\n\nInstall dependencies using pyproject.toml\nuv pip install -e .\n\n\nInstall nbdev if not already installed\nuv pip install nbdev",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#configuration",
    "href": "index.html#configuration",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Configuration",
    "text": "Configuration\nKey parameters to tune:\n\nPDF Extraction: There are various tools to extract content from PDF out there like docling, pypdf, datalab etc\nChunk Size: Token/character count per chunk\nOverlap: Number of overlapping tokens between chunks\nRetrieval Count: Number of chunks to retrieve for lexical/semantic/hybrid\nReranker Model: Qwen 8B, Cohere etc\nLLM Selection: Closed vs Open source models\nRRF Parameters: Weights, window size, k constant",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Evaluation",
    "text": "Evaluation\nThe system uses the WattBot scoring metric:\nWattBot Score = 0.75 √ó answer_value + 0.15 √ó ref_id + 0.10 √ó is_NA\n\nanswer_value (75%): Numeric answers within ¬±0.1% tolerance; exact categorical matches\nref_id (15%): Jaccard overlap of reference IDs\nis_NA (10%): Correct identification of unanswerable questions\n\nRun evaluations and logged in Braintrust platform for detailed analysis of chunking strategies, model performance, and retrieval methods.",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Results",
    "text": "Results\n\nPrivate Leaderboard: 5th place\nPublic Leaderboard: 10th place\nKey Insights:\n\nLexical search (BM25) perfomed better than semantic search\nMarkdown extraction better than text-only extraction\nHybrid search with reranking provided best results\nNeighbor chunk increased accuracy\nProprietary models (OpenAI) showed improvements but exceeded budget\n\n\n\n\n\nLeaderboard Ranking",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#technical-highlights",
    "href": "index.html#technical-highlights",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Technical Highlights",
    "text": "Technical Highlights\n\nPDF Extraction\n\nUsed pypdf for text extraction from PDF\nUsed Datalab for markdown extraction. Preserves layout, tables, and images better than pypdf or Docling.\n\n\n\nSearch Strategy\nStarted with BM25 lexical search. Added Qwen 8B embeddings for semantic search. Combined both with reranking model for optimal retrieval.\n\n\nPrompting\n\nMarkdown-formatted prompts work best with open-source models\nExplicit instructions preferred over assumptions\nJSON output format for easy parsing\nTested zero-shot, few-shot, and chain-of-thought approaches\n\n\n\nChunking\n\nCharacter/Token based chunking with overlap\nMarkdown chunking with sliding window overlap maintained context across chunk boundaries.",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Project Structure",
    "text": "Project Structure\nBuilt with nbdev for literate programming approach.",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#lessons-learned",
    "href": "index.html#lessons-learned",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nStart with lexical search before jumping to vector embeddings\nPDF extraction quality matters more than retrieval sophistication\nEvaluation infrastructure is critical for iterative improvement\nPrompt engineering is artisanal and requires iteration\nOpen-source models can achieve competitive results with proper tuning",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "WattBot 2025 - RAG System for Technical Document Q&A",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nBuilt using Solveit for development workflow. Solveit dialogues: Exploration and SDK\nCompetition organized by Christopher Endemann, Dhruba Jyoti Paul, and Annie Zhao on Kaggle: WattBot 2025",
    "crumbs": [
      "WattBot 2025 - RAG System for Technical Document Q&A"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html",
    "href": "wattbot_sdk_solveit.html",
    "title": "Initial Development with Solveit",
    "section": "",
    "text": "import os\nimport openai\nimport pandas as pd\nimport fastcore.all as fc\nimport contextkit.read as rd\nimport fireworks as fw\nimport braintrust\nimport time\n\nimport ast\nfrom types import SimpleNamespace\nfrom qdrant_client import QdrantClient, models\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nfrom fastembed import SparseTextEmbedding\n\n/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nem = \"nomic-ai/nomic-embed-text-v1.5\" # embedding model\n# em = \"accounts/fireworks/models/qwen3-embedding-8b\"",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#defining-imports",
    "href": "wattbot_sdk_solveit.html#defining-imports",
    "title": "Initial Development with Solveit",
    "section": "",
    "text": "import os\nimport openai\nimport pandas as pd\nimport fastcore.all as fc\nimport contextkit.read as rd\nimport fireworks as fw\nimport braintrust\nimport time\n\nimport ast\nfrom types import SimpleNamespace\nfrom qdrant_client import QdrantClient, models\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nfrom fastembed import SparseTextEmbedding\n\n/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nem = \"nomic-ai/nomic-embed-text-v1.5\" # embedding model\n# em = \"accounts/fireworks/models/qwen3-embedding-8b\"",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#loading-data-files-related-to-competition",
    "href": "wattbot_sdk_solveit.html#loading-data-files-related-to-competition",
    "title": "Initial Development with Solveit",
    "section": "Loading data files (related to competition)",
    "text": "Loading data files (related to competition)\n\ndata_path = fc.Path('wattbot_data')\nmetadata = pd.read_csv(data_path/'metadata.csv', encoding='latin-1', dtype={'year': object})\nqa = pd.read_csv(data_path/'train_QA.csv')\ntst = pd.read_csv(data_path/'test_Q.csv')\n\n\nmetadata.head()\n\n\n\n\n\n\n\n\nid\ntype\ntitle\nyear\ncitation\nurl\n\n\n\n\n0\namazon2023\nreport\n2023 Amazon Sustainability Report\n2023\nAmazon Staff. (2023). Amazon Sustainability Re...\nhttps://sustainability.aboutamazon.com/2023-am...\n\n\n1\nchen2024\npaper\nEfficient Heterogeneous Large Language Model D...\n2024\nShaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...\nhttps://arxiv.org/pdf/2405.01814\n\n\n2\nchung2025\npaper\nThe ML.ENERGY Benchmark: Toward Automated Infe...\n2025\nJae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...\nhttps://arxiv.org/pdf/2505.06371\n\n\n3\ncottier2024\npaper\nThe Rising Costs of Training Frontier AI Models\n2024\nBen Cottier, Robi Rahman, Loredana Fattorini, ...\nhttps://arxiv.org/pdf/2405.21015\n\n\n4\ndodge2022\npaper\nMeasuring the Carbon Intensity of AI in Cloud ...\n2022\nJesse Dodge, Taylor Prewitt, Remi Tachet Des C...\nhttps://arxiv.org/pdf/2206.05229\n\n\n\n\n\n\n\n\nmetadata.dtypes\n\nid          object\ntype        object\ntitle       object\nyear        object\ncitation    object\nurl         object\ndtype: object\n\n\nHow to assign dtype object for column year when reading csv file?\n\nü§ñReplyü§ñ\nYou can specify the data type when reading the CSV file using the dtype parameter in pd.read_csv(). Here‚Äôs how:\nmetadata = pd.read_csv(data_path/'metadata.csv', encoding='latin-1', dtype={'year': object})\nThis tells pandas to read the ‚Äòyear‚Äô column as an object (string) type instead of trying to infer it as an integer.\nWould you like to reload your metadata with this change, or do you have a specific reason for wanting ‚Äòyear‚Äô as an object type?",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#document-processor",
    "href": "wattbot_sdk_solveit.html#document-processor",
    "title": "Initial Development with Solveit",
    "section": "Document Processor",
    "text": "Document Processor\n\nclass Chunk(fc.NS):\n    def __init__(self, chunk_id, doc_id, text, **metadata):\n        super().__init__(chunk_id=chunk_id, doc_id=doc_id, text=text, **metadata)\n    \n    def __repr__(self):\n        meta = \"\\n\".join([f\"\\033[1;35m{k}\\033[0m: {str(v)[:100]}\" for k, v in self.__dict__.items()])\n        return f\"{meta}\"\n\n\nChunk('WattBot', 0, 0, **{'title': 'RAG'})\n\n\nchunk_id: WattBot\ndoc_id: 0\ntext: 0\ntitle: RAG\n\n\n\nHow to make the keys bold?\n\nü§ñReplyü§ñ\nTo make the keys bold in the __repr__ output, you can wrap them in ANSI escape codes or markdown-style formatting. Here‚Äôs a simple approach using ANSI codes for terminal output:\ndef __repr__(self):\n    meta = \"\\n\".join([f\"\\033[1m{k}\\033[0m: {v}\" for k, v in self.__dict__.items() if k != '__stored_args__'])\n    return f\"{meta}\"\nThe \\033[1m starts bold text and \\033[0m resets it.\nWould you like to try this, or would you prefer a different formatting approach?\nCan it be colorize?\n\n\nü§ñReplyü§ñ\nYes! You can use different ANSI color codes. For example:\nf\"\\033[1;34m{k}\\033[0m: {v}\"  # Bold blue\nf\"\\033[1;32m{k}\\033[0m: {v}\"  # Bold green\nf\"\\033[1;35m{k}\\033[0m: {v}\"  # Bold magenta\nThe format is \\033[1;COLORm where common color codes are: - 31: red - 32: green - 33: yellow - 34: blue - 35: magenta - 36: cyan\nWhich color would you like to try?\n\nimport fastcore.all as fc\nimport contextkit.read as rd\n\nclass DocumentProcessor:\n    def __init__(self, data_path='wattbot_data', chunk_size=1500, step=1400):\n        fc.store_attr()\n    \n    def stream(self, doc_id, metadata=None):\n        content = rd.read_pdf(fc.Path(self.data_path)/f'{doc_id}.pdf')\n        for i in range(0, len(content), self.step): yield Chunk(text=content[i:i+self.chunk_size], chunk_id=i, doc_id=doc_id, **(metadata or {}))\n\n    def process(self, doc_id, metadata=None):\n        return fc.L(self.stream(doc_id, metadata=metadata))\n\n\ndoc_id, doc_type, doc_url, doc_title, doc_cite, doc_year = fc.getattrs(metadata.iloc[0], 'id', 'type', 'url', 'title', 'citation', 'year')\ndoc_proc = DocumentProcessor()\nextras = {'type': doc_type, 'url': doc_url, 'title': doc_title, 'citation': doc_cite, 'year': doc_year}\nchunks = fc.L(doc_proc.process(doc_id, metadata=extras))\nchunks[0]\n\n\nchunk_id: 0\ndoc_id: amazon2023\ntext: Amazon \nSustainability \nReport\n2023 Contents\nOverview\n3 Introduction\n4 A Letter from Our Chief \nSust\ntype: report\nurl: https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf\ntitle: 2023 Amazon Sustainability Report\ncitation: Amazon Staff. (2023). Amazon Sustainability Report. https://sustainability.aboutamazon.com/2023-amaz\nyear: 2023\n\n\n\n\ndef process_docs(data_path='wattbot_data'):\n    metadata = pd.read_csv(fc.Path(data_path)/'metadata.csv', encoding='latin-1')\n    doc_proc = DocumentProcessor()\n    all_chunks = metadata.apply(lambda r: doc_proc.process(r['id'], metadata=fc.attrdict(r, 'type', 'url', 'title', 'citation', 'year')), axis=1).tolist()\n    return fc.L(all_chunks).concat()\n\n\nall_chunks = process_docs()\nall_chunks[0]\n\n\nchunk_id: 0\ndoc_id: amazon2023\ntext: Amazon \nSustainability \nReport\n2023 Contents\nOverview\n3 Introduction\n4 A Letter from Our Chief \nSust\ntype: report\nurl: https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf\ntitle: 2023 Amazon Sustainability Report\ncitation: Amazon Staff. (2023). Amazon Sustainability Report. https://sustainability.aboutamazon.com/2023-amaz\nyear: 2023\n\n\n\nHow to save all the chunks so that I dont generate again?\n\n\nü§ñReplyü§ñ\nYou have several options for saving the chunks:\n\nPickle - Fast and preserves Python objects:\n\nfc.save_pickle('all_chunks.pkl', all_chunks)\n\nJSON - More portable and readable:\n\nimport json\nwith open('all_chunks.json', 'w') as f:\n    json.dump([chunk.__dict__ for chunk in all_chunks], f)\n\nParquet - Efficient for large datasets:\n\npd.DataFrame([chunk.__dict__ for chunk in all_chunks]).to_parquet('all_chunks.parquet')\nWhich format would you prefer, or would you like to know more about the tradeoffs?\nHow to load the chunks?\n\n\nü§ñReplyü§ñ\nFor each saving method, here‚Äôs how to load:\n\nPickle:\n\nall_chunks = fc.load_pickle('all_chunks.pkl')\n\nJSON:\n\nwith open('all_chunks.json', 'r') as f:\n    all_chunks = fc.L([Chunk(**d) for d in json.load(f)])\n\nParquet:\n\nall_chunks = fc.L([Chunk(**row) for row in pd.read_parquet('all_chunks.parquet').to_dict('records')])\nWhich format did you use to save them?",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#embedding",
    "href": "wattbot_sdk_solveit.html#embedding",
    "title": "Initial Development with Solveit",
    "section": "Embedding",
    "text": "Embedding\n\nsparse_model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\n\nFetching 18 files:   0%|          | 0/18 [00:00&lt;?, ?it/s]Fetching 18 files:   6%|‚ñå         | 1/18 [00:00&lt;00:07,  2.34it/s]Fetching 18 files:  11%|‚ñà         | 2/18 [00:00&lt;00:03,  4.22it/s]Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00&lt;00:00, 39.63it/s]Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00&lt;00:00, 26.24it/s]\n\n\n\nclass Embedder:\n    def __init__(self, dclient, dmodel, smodel=None, bs=50):\n        fc.store_attr()\n    \n    def create_dembs(self, texts):\n        data = fc.mapped(lambda o: self.dclient.embeddings.create(model=self.dmodel, input=o), fc.chunked(texts, chunk_sz=self.bs)).attrgot('data')\n        return data.concat().attrgot('embedding')\n\n    def create_sembs(self, texts):\n        if not self.smodel: raise ValueError(\"Sparse model not initialized\")\n        return list(self.smodel.embed(texts))\n\n\nopenai_cli = openai.OpenAI(base_url = \"https://api.fireworks.ai/inference/v1\", api_key=os.environ['FIREWORKS_API_KEY'])\nembedder = Embedder(openai_cli, em, smodel=sparse_model)\nembds = embedder.create_dembs(chunks.attrgot('text'))\nlen(embds), len(embds[0])\n\n(322, 768)\n\n\nI have an api key for gemini and want to use for embedding model with the openai spec https://ai.google.dev/gemini-api/docs/openai\n\nü§ñReplyü§ñ\nTo use Gemini‚Äôs embedding model with the OpenAI spec, you‚Äôll need to:\n\nChange the base URL to Gemini‚Äôs endpoint\nUse your Gemini API key\nSpecify a Gemini embedding model name\n\nHere‚Äôs what to modify:\nopenai_cli = openai.OpenAI(\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    api_key=os.environ['GEMINI_API_KEY']\n)\nFor the embedding model, Gemini supports models like text-embedding-004. You‚Äôd set:\nem = \"text-embedding-004\"\nHave you already set your GEMINI_API_KEY environment variable, or would you like help with that?\nI am getting info logs when calling to gemini 2025-11-29 15:22:21,210 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/embeddings ‚ÄúHTTP/1.1 200 OK‚Äù\n\n\nü§ñReplyü§ñ\nYou can suppress those INFO logs by adjusting the logging level for the HTTP client. Add this at the top of your notebook:\nimport logging\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\nThis will only show warnings and errors, not info messages. Would you like to try this?\n\nall_dense_embds = embedder.create_dembs(all_chunks.attrgot('text'))\nlen(all_dense_embds), len(all_dense_embds[0])\n\n(1899, 768)\n\n\n\nall_sparse_embds = embedder.create_sembs(all_chunks.attrgot('text'))\nlen(all_sparse_embds), len(all_sparse_embds[0].values)\n\n(1899, 137)",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#qdrant",
    "href": "wattbot_sdk_solveit.html#qdrant",
    "title": "Initial Development with Solveit",
    "section": "Qdrant",
    "text": "Qdrant\n\nQHOST = \"https://76eb75ff-5ca3-4a68-93d4-ae19a9c169a4.us-west-2-0.aws.cloud.qdrant.io:6333\"\napi_key = os.getenv(\"QDRANT_API_KEY\")\n\n\nclass QdrantStore:\n    def __init__(self, url, key, collection_name, use_hybrid=False):\n        fc.store_attr()\n        self.cli = QdrantClient(url=url, api_key=key)\n        self.points = []\n\n    def create_or_replace_collection(self, vs, replace=False, distance=Distance.COSINE):\n        if replace: self.cli.delete_collection(collection_name=self.collection_name)\n        sparse_vectors_config = None\n        if not self.cli.collection_exists(self.collection_name): \n            if self.use_hybrid: sparse_vectors_config={\"sparse\": models.SparseVectorParams()}\n            self.cli.create_collection(collection_name=self.collection_name, \n                vectors_config={\"dense\": VectorParams(size=vs, distance=distance)}, \n                sparse_vectors_config=sparse_vectors_config)\n        return self\n    \n\n    def _sv(self, semb):\n        return models.SparseVector(values=semb.values.tolist(), indices=semb.indices.tolist())\n\n    def create_hybrid_points(self, chunks, dembs, sembs, start_id=0):\n        return fc.L(chunks, dembs, sembs).zip().enumerate().map(lambda ele: PointStruct(id=start_id+ele[0], vector={\"dense\": ele[1][1], \"sparse\": self._sv(ele[1][2])}, payload=ele[1][0].__dict__))\n\n    def create_points(self, chunks, dembs, sembs=None, start_id=0):\n        if self.use_hybrid and sembs: \n            self.points = self.create_hybrid_points(chunks, dembs, sembs, start_id)\n        else: \n            self.points = fc.L(chunks, dembs).zip().enumerate().map(lambda ele: PointStruct(id=start_id+ele[0], vector={\"dense\": ele[1][1]}, payload=ele[1][0].__dict__))\n        return self\n    \n    def upsert(self, bs=100):\n        fc.mapped(lambda o: self.cli.upsert(collection_name=self.collection_name, points=list(o)), fc.chunked(self.points, chunk_sz=bs))\n        return self\n\n    def hybrid_search(self, query_demb, query_semb, limit=5):\n        return self.cli.query_points(collection_name=self.collection_name,\n            prefetch=[\n                models.Prefetch(query=query_demb, using=\"dense\", limit=limit*2),\n                models.Prefetch(query=self._sv(query_semb), using=\"sparse\", limit=limit*2)\n            ],\n            query=models.FusionQuery(fusion=models.Fusion.RRF),\n            limit=limit\n        )\n\n    def search(self, query_demb, query_semb=None, limit=5):\n        if self.use_hybrid and query_semb:\n            results = self.hybrid_search(query_demb, query_semb, limit)\n        else:\n            results = self.cli.query_points(collection_name=self.collection_name, query=query_vector, limit=limit)\n        return results.points\n\nDo we really require qdrant datastore for storing 1899 embeddings and searching? Will a brute force search work which does search across all the embedding\n\nü§ñReplyü§ñ\nGreat question! For 1,899 embeddings, brute force search would absolutely work and might even be simpler and faster than using Qdrant.\nWith modern hardware, computing cosine similarity across ~2,000 vectors is very quick (milliseconds). Vector databases like Qdrant are mainly beneficial when you have: - Hundreds of thousands or millions of vectors - Need for persistence across sessions - Advanced filtering on metadata - Multiple concurrent users\nFor your use case, you could use simple numpy/scipy operations:\nfrom sklearn.metrics.pairwise import cosine_similarity\nWould you like to try implementing a simple brute force search to compare? Or are you planning to scale up significantly and want to keep Qdrant for future growth?\nAnd what about the keyword search?\n\n\nü§ñReplyü§ñ\nGood point! For keyword search (like BM25), you have a few options without Qdrant:\n\nSimple approach: Use libraries like rank-bm25 which does in-memory BM25 scoring\nHybrid with numpy: Store sparse embeddings as scipy sparse matrices and compute similarities\nFull-text search: Use SQLite‚Äôs FTS5 for keyword search\n\nFor 1,899 documents, all of these would be fast enough. The sparse embeddings you‚Äôre generating with Qdrant/bm25 model could be stored as scipy sparse matrices and searched with simple dot products.\nAre you interested in implementing a lightweight alternative, or do you prefer keeping Qdrant for the convenience of having both dense and sparse search in one place?\n\ncname = 'wattbot_docs_nomic_768_sparse'\n# cname = 'wattbot_docs_qwen_8b'\nqs = QdrantStore(QHOST, api_key, cname, use_hybrid=True).create_or_replace_collection(768, replace=True)\nqs.create_points(all_chunks, all_dense_embds, all_sparse_embds).upsert()\n\n&lt;__main__.QdrantStore&gt;\n\n\n\nqs.cli.get_collections()\n\nCollectionsResponse(collections=[CollectionDescription(name='wattbot_docs_nomic_768'), CollectionDescription(name='wattbot_documents_768'), CollectionDescription(name='wattbot_docs_qwen_8b'), CollectionDescription(name='colbert_chunks'), CollectionDescription(name='wattbot_documents'), CollectionDescription(name='wattbot_docs_nomic_768_sparse')])\n\n\nInstead of the qdrant database I save the dense and sparse embeddings locally and do similarity search then combine the results\n\n\nü§ñReplyü§ñ\nThat‚Äôs a solid approach for this dataset size! You‚Äôd essentially be building a lightweight hybrid search system.\nThe basic steps would be: 1. Save dense embeddings (numpy array) 2. Save sparse embeddings (scipy sparse matrix or list) 3. For search: compute similarities separately for dense and sparse 4. Combine results using a fusion method (like RRF - Reciprocal Rank Fusion)\nWould you like me to help you implement this step by step, or would you prefer to try building it yourself with guidance on specific parts as needed?\nYes I would need to help with saving the sparse embedding and compute similarity for them\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs start with saving the sparse embeddings.\nSince your sparse embeddings have .values and .indices attributes (from the fastembed model), you have a couple of options:\n\nScipy sparse matrix - efficient for similarity computation\nList of dicts - simpler, keeps the structure as-is\n\nFor scipy, you‚Äôd convert to CSR format which is efficient for similarity searches. For the list approach, you‚Äôd just pickle the embeddings directly.\nWhich approach sounds better to you, or would you like to try the scipy sparse matrix route since it‚Äôs more efficient for similarity computation?\nWhat is CSR format\n\n\nü§ñReplyü§ñ\nCSR stands for Compressed Sparse Row format. It‚Äôs a way to efficiently store sparse matrices (matrices with mostly zeros) by only storing the non-zero values.\nInstead of storing a full matrix like:\n[0, 0, 3, 0, 5]\n[2, 0, 0, 0, 0]\n[0, 4, 0, 1, 0]\nCSR stores three arrays: - values: the non-zero values [3, 5, 2, 4, 1] - indices: which column each value is in - indptr: where each row starts\nThis saves memory and makes operations like dot products much faster when you have lots of zeros (which is typical for sparse embeddings like BM25).\nDoes this make sense? Would you like to proceed with using scipy‚Äôs CSR format for your sparse embeddings?\nOkay lets do that\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs start with converting your sparse embeddings to CSR format.\nFirst, you‚Äôll need scipy‚Äôs sparse module:\nfrom scipy.sparse import csr_matrix\nTo convert your all_sparse_embds (which are fastembed sparse vectors) to a CSR matrix, you‚Äôll need to extract the indices and values from each embedding and stack them into a matrix.\nTry creating a function that takes one sparse embedding and returns the indices and values. What do you get when you look at all_sparse_embds[0].indices and all_sparse_embds[0].values?\nI created the function def get_val_idx(s): return s.indices, s.values\n\n\nü§ñReplyü§ñ\nGood start! Now you have a way to extract the indices and values from each sparse embedding.\nThe next step is to build the CSR matrix from all your embeddings. You‚Äôll need to: 1. Determine the maximum index across all embeddings (this will be the number of columns) 2. Stack all the indices and values together 3. Create the CSR matrix\nTry finding the maximum index first. What do you get when you check the max index across all your sparse embeddings?\nTHe highest value of indices are 30151\n\n\nü§ñReplyü§ñ\nGreat! So your CSR matrix will have 30,152 columns (since indices are 0-based).\nNow you need to build the CSR matrix. Scipy‚Äôs csr_matrix can be constructed using:\ncsr_matrix((data, indices, indptr), shape=(n_rows, n_cols))\nWhere: - data = all values concatenated - indices = all column indices concatenated\n- indptr = array marking where each row starts in the data array\nTry creating the data and indices arrays first by concatenating all the values and indices from your sparse embeddings. What approach will you use to concatenate them?\nYes\n\n\nü§ñReplyü§ñ\nGreat! Now, back to building your CSR matrix - do you want to continue with concatenating the data and indices arrays, or would you like to explore something else about your RAG system first?\nlets do a simple lexical search instead of the model\n\n\nü§ñReplyü§ñ\nGood idea! For simple lexical search, you have a few options:\n\nBM25 library (rank-bm25) - classic keyword scoring\nTF-IDF with sklearn - term frequency approach\nSQLite FTS5 - built-in full-text search\n\nBM25 is generally considered best for search/retrieval tasks. Would you like to try the rank-bm25 library, or explore one of the other options?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! First, you‚Äôll need to install the library:\n!pip install rank-bm25\nThen import it:\nfrom rank_bm25 import BM25Okapi\nBM25 needs tokenized documents to build its index. You‚Äôll need to split your chunk texts into tokens (words). How would you like to tokenize - simple split() or something more sophisticated?\nWhen adding the package rank-bm25 I am getting the following error\nNo solution found when resolving dependencies for split (python_full_version == ‚Äô3.9.*‚Äô): ‚ï∞‚îÄ‚ñ∂ Because the requested Python version (&gt;=3.9) does not satisfy Python&gt;=3.10 and contextkit==0.0.10 depends on Python&gt;=3.10, we can conclude that contextkit==0.0.10 cannot be used. And because only contextkit&lt;=0.0.10 is available and your project depends on contextkit&gt;=0.0.10, we can conclude that your project‚Äôs requirements are unsatisfiable.\n  hint: The `requires-python` value (&gt;=3.9) includes Python versions that are not supported by your dependencies (e.g., contextkit==0.0.10 only supports &gt;=3.10). Consider using a more restrictive\n  `requires-python` value (like &gt;=3.10).\nhelp: If you want to add the package regardless of the failed resolution, provide the --frozen flag to skip locking and syncing. (wattbot) (base) MacBook-Pro:wattbot anubhavmaity$\n\n\nü§ñReplyü§ñ\nThis error is about your Python version and project dependencies, not specifically about rank-bm25.\nYour project requires contextkit&gt;=0.0.10 which needs Python 3.10+, but your project‚Äôs requires-python is set to &gt;=3.9.\nYou have two options:\n\nUpdate your project‚Äôs Python requirement to &gt;=3.10 in your pyproject.toml or similar config file\nUse the --frozen flag as suggested: uv pip install rank-bm25 --frozen (though this skips dependency resolution)\n\nWhich Python version are you actually running? You can check with python --version.\nokay I was able to install rank-bm25, now what is next\n\n\nü§ñReplyü§ñ\nGreat! Now you need to tokenize your chunk texts to build the BM25 index.\nTry creating a list of tokenized documents from your chunks. For each chunk‚Äôs text, split it into words. What method will you use - simple split() or something else?\nsimple split will do\n\n\nü§ñReplyü§ñ\nPerfect! Try creating the tokenized corpus:\ntokenized_corpus = [chunk.text.lower().split() for chunk in all_chunks]\nThe .lower() helps with case-insensitive matching. Once you have this, you can create the BM25 index. Give it a try and let me know what you get!\ngot the tokenized corpus what is next?\n\n\nü§ñReplyü§ñ\nExcellent! Now create the BM25 index:\nbm25 = BM25Okapi(tokenized_corpus)\nThis builds the index from your tokenized documents. Once you have this, you‚Äôll be able to search by tokenizing a query and using bm25.get_scores() or bm25.get_top_n().\nHave you created the index?\nWhat should I pass to get_top_n\n\n\nü§ñReplyü§ñ\nget_top_n() takes three main arguments:\n\ntokenized_query - your search query split into tokens (like you did for the corpus)\ndocuments - the original documents/chunks you want returned\nn - how many top results you want\n\nSo it would look something like:\nbm25.get_top_n(tokenized_query, all_chunks, n=5)\nWhat query would you like to test with?\nHow should I create a new class to have bm25 and do the cosine similarity across all vectos?\n\n\nü§ñReplyü§ñ\nGood thinking to organize this into a class! Here‚Äôs a structure to consider:\nclass HybridSearch:\n    def __init__(self, chunks, dense_embds, tokenized_corpus):\n        # Store your data\n        # Initialize BM25\n        # Store dense embeddings as numpy array\nYou‚Äôll need methods like: - lexical_search(query, top_k) - for BM25 - dense_search(query_emb, top_k) - for cosine similarity - hybrid_search(query, query_emb, top_k) - combines both\nFor cosine similarity, you can use sklearn.metrics.pairwise.cosine_similarity or just normalize vectors and use dot product.\nWould you like to start building this class yourself, or would you like me to write it out for you?\nCan you please show me how to do the cosine similarity?\n\n\nü§ñReplyü§ñ\nSure! Here are two approaches for cosine similarity:\nOption 1 - Using sklearn:\nfrom sklearn.metrics.pairwise import cosine_similarity\nscores = cosine_similarity([query_emb], dense_embds)[0]\nOption 2 - Using numpy (faster for normalized vectors):\nimport numpy as np\n# Normalize vectors first\ndense_embds_norm = dense_embds / np.linalg.norm(dense_embds, axis=1, keepdims=True)\nquery_norm = query_emb / np.linalg.norm(query_emb)\nscores = dense_embds_norm @ query_norm\nBoth give you similarity scores for each document. To get top-k results, you‚Äôd use np.argsort(scores)[-k:][::-1] to get the indices.\nWhich approach would you like to use in your class?",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#retriever",
    "href": "wattbot_sdk_solveit.html#retriever",
    "title": "Initial Development with Solveit",
    "section": "Retriever",
    "text": "Retriever\n\nclass Retriever:\n    def __init__(self, embedder, vector_store):\n        fc.store_attr()\n    \n    def retrieve(self, query, limit=5):\n        query_demb = self.embedder.create_dembs(texts=[query])[0]\n        query_semb = self.embedder.create_sembs(texts=[query])[0] if self.embedder.smodel else None\n        return self.vector_store.search(query_demb, query_semb, limit=limit)\n\n\nquery_text = \"What are Amazon's carbon emissions goals?\"\nretriever = Retriever(embedder, qs)\nretriever.retrieve(query_text, limit=1)[0].payload['text'][:200]\n\n'to build Climate Pledge Arena, the first International \\nLiving Future Institute zero-carbon certified sports and \\nentertainment venue in the world.\\nCarbon \\nThe science is clear. Significant carbon emi'\n\n\nTo support creating the embeddings for both sparse and dense.\n\nü§ñReplyü§ñ\nYou need to update the Retriever class to handle both dense and sparse embeddings. The error shows it‚Äôs calling create_embeddings which doesn‚Äôt exist - your Embedder has create_dembs and create_sembs instead.\nHere‚Äôs how you can modify the retrieve method to work with your hybrid setup:\ndef retrieve(self, query, limit=5):\n    query_demb = self.embedder.create_dembs(texts=[query])[0]\n    query_semb = self.embedder.create_sembs(texts=[query])[0] if self.embedder.smodel else None\n    # Now you need to decide how to combine them for search\nHowever, I notice your QdrantStore.search method currently only takes a single query_vector. For hybrid search with both dense and sparse vectors, you‚Äôll need to update that method too.\nWould you like help modifying the search method to support hybrid queries, or do you want to work on that yourself?\nYes\n\n\nü§ñReplyü§ñ\nGreat! For hybrid search in Qdrant, you need to use the query_points method with a prefetch parameter to combine dense and sparse searches.\nHere‚Äôs the key structure:\ndef search(self, query_demb, query_semb=None, limit=5):\n    if self.use_hybrid and query_semb:\n        # Use prefetch for hybrid search\n        results = self.cli.query_points(\n            collection_name=self.collection_name,\n            prefetch=[\n                models.Prefetch(query=query_demb, using=\"dense\", limit=limit*2),\n                models.Prefetch(query=self._sv(query_semb), using=\"sparse\", limit=limit*2)\n            ],\n            query=models.FusionQuery(fusion=models.Fusion.RRF),\n            limit=limit\n        )\nThe prefetch retrieves candidates from both dense and sparse vectors, then FusionQuery with RRF (Reciprocal Rank Fusion) combines the results.\nWould you like to complete the rest of the method to handle the non-hybrid case?\n\nquery_text = \"What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\"\nretriever.retrieve(query_text, limit=3)[2]\n\nScoredPoint(id=1864, version=18, score=0.33333334, payload={'chunk_id': 8400, 'doc_id': 'zschache2025', 'text': ' on a single\\nbenchmark, we extend these efforts to a greater number of partially very large models\\ndeployed to a practical inference scenario.\\nThis makes our study very similar to the one by Alizadeh et al. (2025), who inves-\\ntigated the trade-offs between accuracy and energy consumption when deploying large\\nlanguage models (LLMs) for software development tasks. Besides the finding that\\nlarger LLMs with higher energy consumption do not always yield significantly bet-\\nter accuracy, the authors demonstrated that architectural factors, such as feedforward\\nlayer size and transformer block count, directly correlate with energy usage.\\n3 Finally, Luccioni et al. (2024) provide one of the most comprehensive analyses\\nof energy consumption during ML model inference. Their study systematically com-\\npared the energy costs of 88 models across 10 tasks and 30 datasets, including both\\nsmaller task-specific and larger multi-purpose models. They found that the larger\\nmodels are orders of magnitude more energy-intensive than smaller task-specific ones,\\nespecially for tasks involving text and image generation. Furthermore, their research\\nunderscores the variability in energy consumption across tasks and model architec-\\ntures. The authors advocate for increased transparency and sustainable deployment\\npractices, emphasizing that the environmental costs of deploying large, multi-purpose\\nAI systems must be carefully weighed against their utility.\\n3 Data and methods\\nOur experiments are inspired by a', 'type': 'paper', 'url': 'https://arxiv.org/pdf/2508.14170 ', 'title': 'Comparing energy consumption and accuracy in text classification inference', 'citation': 'Johannes Zschache, & Tilman Hartwig (2025). Comparing energy consumption and accuracy in text classification inference arXiv. https://arxiv.org/pdf/2508.14170', 'year': 2025}, vector=None, shard_key=None, order_value=None)\n\n\nHow to remove the stored_args from the ScoredPoint above?\n\n\nü§ñReplyü§ñ\nThe __stored_args__ is being added to the payload because it‚Äôs part of the Chunk object‚Äôs __dict__. You can filter it out when creating the points.\nIn your QdrantStore.create_points method, modify the payload creation to exclude it:\npayload={k: v for k, v in ele[1][0].__dict__.items() if k != '__stored_args__'}\nThis filters out __stored_args__ before storing the payload. Would you like to update both the create_points and create_hybrid_points methods with this change?\nCant we modify the Chunk class to not store the stored_args?\n\n\nü§ñReplyü§ñ\nYes, absolutely! That‚Äôs a cleaner solution. The __stored_args__ comes from fastcore‚Äôs store_attr() function. You can either:\n\nOverride __dict__ to exclude it\nDelete it after calling store_attr()\nUse a property/method to return a clean dict\n\nThe simplest approach would be to add this line in your Chunk.__init__:\ndel self.__stored_args__\nWould you like to try that, or would you prefer a different approach?\nI want to search for the text ‚Äú5,842 labeled entries‚Äù in all_chunks\n\n\nü§ñReplyü§ñ\nYou can search for that text using a list comprehension or the filter method. Here‚Äôs a simple approach:\nmatching_chunks = [chunk for chunk in all_chunks if '5,842 labeled entries' in chunk.text]\nThis will give you all chunks that contain that exact phrase. Would you like to try this and see what you find?\n\nkhan2025_chunks = DocumentProcessor().process('khan2025')\n[chunk for chunk in khan2025_chunks if '5,842 entries' in chunk.text]\n\n\n[chunk_id: 18200\n doc_id: khan2025\n text: sed language model\n tuned for general-purpose tasks.\n ‚Ä¢ Mistral-7B-Instruct-v0.3: An instruction-tuned]\n\n\n\nIt does not return the result with 5,482 labeled entries. How to make sure it returns that?\n\n\nü§ñReplyü§ñ\nThe issue is that you‚Äôre searching for the exact phrase ‚Äò5,842 entries‚Äô but the actual text might have different formatting - perhaps it says ‚Äò5,842 labeled entries‚Äô or has extra spaces or punctuation.\nTry searching with just the number:\n[chunk for chunk in khan2025_chunks if '5,842' in chunk.text]\nThis will find any chunk containing that number, regardless of the surrounding words. Does that return the result you‚Äôre looking for?\n\n[chunk for chunk in khan2025_chunks if '5,842' in chunk.text]\n\n\n[chunk_id: 18200\n doc_id: khan2025\n text: sed language model\n tuned for general-purpose tasks.\n ‚Ä¢ Mistral-7B-Instruct-v0.3: An instruction-tuned]\n\n\n\n\n# [chunk for chunk in all_chunks if 'JetMoE-8B' in chunk.text]\n\nWhy the retrieve function is returning the result as 2nd index instead of first?\n\n\nü§ñReplyü§ñ\nGood question! There are a few possible reasons why the most relevant chunk isn‚Äôt ranking first:\n\nEmbedding similarity - The query embedding might be more similar to other chunks based on semantic meaning, not just keyword matching\nHybrid search weighting - The RRF (Reciprocal Rank Fusion) is combining dense and sparse results, and the sparse (BM25) scoring might be favoring other chunks\nContext around the keywords - Other chunks might have more context that semantically matches your query\n\nTo debug this, you could: - Check the scores of the top 3 results to see how close they are - Try adjusting the fusion weights or prefetch limits in your hybrid search - Compare dense-only vs sparse-only vs hybrid results\nWould you like to investigate any of these possibilities?",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#generator",
    "href": "wattbot_sdk_solveit.html#generator",
    "title": "Initial Development with Solveit",
    "section": "Generator",
    "text": "Generator\n\nclass Generator:\n    def __init__(self, llm_client, prompt_template):\n        fc.store_attr()\n    \n    def create_context(self, retrieved_chunks):\n        return \"\\n\\n\".join([f\"Doc ID: {chunk.payload['doc_id']}\\nURL: {chunk.payload['url']}\\nTitle: {chunk.payload['title']}\\nText: {chunk.payload['text']}\" for chunk in retrieved_chunks])\n    \n    def format_prompt(self, query, context):\n        return self.prompt_template.build(context=context, question=query)\n    \n    def generate(self, query, retrieved_chunks):\n        context = self.create_context(retrieved_chunks)\n        formatted_prompt = self.format_prompt(query, context)\n        response = self.llm_client.chat.completions.create(formatted_prompt['messages'], response_format={\"type\": \"json_object\"})\n        time.sleep(1)\n        return fc.loads(response.choices[0].message.content)\n\n\n# llm = fw.LLM(model=\"accounts/fireworks/models/deepseek-v3p1\",  deployment_type=\"auto\")\n# llm = fw.LLM(model=\"accounts/fireworks/models/deepseek-r1-0528\",  deployment_type=\"auto\")\n# model = 'accounts/fireworks/models/kimi-k2-instruct-0905'\n# model = 'accounts/fireworks/models/llama-v3p1-70b-instruct'\nmodel = 'accounts/fireworks/models/deepseek-v3p1'\n\n# model = 'accounts/fireworks/models/kimi-k2-thinking'\n\nllm = fw.LLM(model=model,  deployment_type=\"auto\")\n\nI want to use a gemini model with openai spec\n\nü§ñReplyü§ñ\nYou can use Gemini models with the OpenAI spec by creating an OpenAI client with Gemini‚Äôs base URL, similar to what you did for embeddings earlier.\nFor the Generator class, you‚Äôd pass in an OpenAI client configured for Gemini:\ngemini_client = openai.OpenAI(\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    api_key=os.environ['GEMINI_API_KEY']\n)\nThen for the model name, you‚Äôd use something like \"gemini-1.5-pro\" or \"gemini-1.5-flash\".\nHave you already set up your Gemini API key, or would you like help with that part?\nHow to pass the model name?\n\n\nü§ñReplyü§ñ\nWhen you create the chat completion in your Generator.generate method, you pass the model name to the create call:\nresponse = self.llm_client.chat.completions.create(\n    model=\"gemini-1.5-pro\",  # or \"gemini-1.5-flash\"\n    messages=formatted_prompt['messages'],\n    response_format={\"type\": \"json_object\"}\n)\nNote: Check if Gemini supports the response_format parameter - not all models/providers support structured JSON output this way.\nWould you like to test this with your Gemini client?\nHow to enable the thinking model for kimi-k2-thinking model? And is there a prefill the model with {\n\n\nü§ñReplyü§ñ\nFor thinking models like kimi-k2-thinking, you typically need to:\n\nEnable thinking mode - Check if there‚Äôs a parameter like enable_thinking=True or similar in the Fireworks API\nPrefill with { - Yes, some models support prefilling the assistant‚Äôs response to force JSON output\n\nFor prefilling, you‚Äôd add an assistant message with just { to force JSON formatting:\nmessages = [\n    {\"role\": \"user\", \"content\": \"your prompt\"},\n    {\"role\": \"assistant\", \"content\": \"{\", \"prefix\": True}\n]\nThe exact parameter name might vary. Have you checked the Fireworks documentation for kimi-k2-thinking specific parameters? Would you like me to help you look that up?\nYes please\n\n\nü§ñReplyü§ñ\n\n{\n  \"id\": \"toolu_01QueTq9m7VMedbMSWThKkaY\",\n  \"call\": {\n    \"function\": \"read_url\",\n    \"arguments\": {\n      \"url\": \"https://docs.fireworks.ai/guides/querying-text-models\"\n    }\n  },\n  \"result\": \"'Skip to main content\\\\n\\\\n[Fireworks AI Docs home page![light logo](https://mintcdn.com/fireworksai/sTHhFfY93wc80BaS/logo/logo.svg?fit=max&auto=format&n=sTHhFfY93wc80BaS&q=85&s=33df07dcf4adc8ba74a3d9c6e5153b3a)![dark logo](https://mintcdn.com/fireworksai/sTHhFfY93wc80BaS/logo/logo.svg?fit=max&auto=format&n=sTHhFfY93wc80BaS&q=85&s=33df07dcf4adc8ba74a3d9c6e5153b3a)](/)\\\\n\\\\n[Documentation](/getting-started/introduction)[API & SDK Reference](/api-reference/introduction)[CLI Reference](/tools-sdks/firectl/firectl)[Demos](https://demos.fireworks.ai)[Resources](/getting-started/concepts)\\\\n\\\\n  * [Community](https://discord.gg/fireworks-ai)\\\\n  * [Status](https://status.fireworks.ai/)\\\\n  * [Dashboard](https://app.fireworks.ai/login)\\\\n  * [Dashboard](https://app.fireworks.ai/login)\\\\n\\\\nSearch...\\\\n\\\\nNavigation\\\\n\\\\nModels & Inference\\\\n\\\\nText Models\\\\n\\\\nSearch...\\\\n\\\\n\\u2318K\\\\n\\\\n##### Get Started\\\\n\\\\n  * [Overview](/getting-started/introduction)\\\\n  * [Serverless Quickstart](/getting-started/quickstart)\\\\n  * [Deployments Quickstart](/getting-started/ondemand-quickstart)\\\\n  * [Which model should I use?](/guides/recommended-models)\\\\n\\\\n##### Deployments\\\\n\\\\n  * [Deployments Overview](/guides/ondemand-deployments)\\\\n  * [Autoscaling](/deployments/autoscaling)\\\\n  * [Custom Models](/models/uploading-custom-models)\\\\n  * [Quantization](/models/quantization)\\\\n  * Advanced\\\\n\\\\n##### Models & Inference\\\\n\\\\n  * [Text Models](/guides/querying-text-models)\\\\n  * [Vision Models](/guides/querying-vision-language-models)\\\\n  * [Speech to Text Models](/guides/querying-asr-models)\\\\n  * [Embeddings & Reranking](/guides/querying-embeddings-models)\\\\n  * [Tool Calling](/guides/function-calling)\\\\n  * [Batch API](/guides/batch-inference)\\\\n  * Advanced\\\\n\\\\n##### Fine Tuning\\\\n\\\\n  * [Fine Tuning Overview](/fine-tuning/finetuning-intro)\\\\n  * Supervised Fine Tuning\\\\n\\\\n  * Reinforcement Fine Tuning\\\\n\\\\n  * [Deploying Fine Tuned Models](/fine-tuning/deploying-loras)\\\\n\\\\n##### Administration\\\\n\\\\n  * [Managing users](/accounts/users)\\\\n  * [Custom SSO](/accounts/sso)\\\\n  * [Service Accounts](/accounts/service-accounts)\\\\n  * [Rate Limits & Quotas](/guides/quotas_usage/rate-limits)\\\\n  * [Exporting Billing Metrics](/accounts/exporting-billing-metrics)\\\\n\\\\n##### Security & Compliance\\\\n\\\\n  * [Data Security & Privacy](/guides/security_compliance/data_security)\\\\n  * [Zero Data Retention](/guides/security_compliance/data_handling)\\\\n  * [Audit & Access Logs](/guides/security_compliance/audit_logs)\\\\n\\\\n##### Integrations\\\\n\\\\n  * [Cloud Integrations](/ecosystem/integrations)\\\\n  * [Agent Frameworks](/ecosystem/integrations/agent-frameworks)\\\\n  * [MLOps & Observability](/ecosystem/integrations/mlops-observability)\\\\n\\\\nOn this page\\\\n\\\\n  * Chat Completions API\\\\n  * Alternative query methods\\\\n  * Querying dedicated deployments\\\\n  * Common patterns\\\\n  * Multi-turn conversations\\\\n  * System prompts\\\\n  * Streaming responses\\\\n  * Async requests\\\\n  * Usage & performance tracking\\\\n  * Advanced capabilities\\\\n  * Configuration & debugging\\\\n  * Understanding tokens\\\\n  * OpenAI SDK Migration\\\\n  * Next steps\\\\n\\\\nModels & Inference\\\\n\\\\n# Text Models\\\\n\\\\nCopy page\\\\n\\\\nQuery, track and manage inference for text models\\\\n\\\\nCopy page\\\\n\\\\nNew to Fireworks? Start with the [Serverless Quickstart](/getting-started/quickstart) for a step-by-step guide to making your first API call.\\\\n\\\\nFireworks provides fast, cost-effective access to leading open-source text models through OpenAI-compatible APIs. Query models via serverless inference or dedicated deployments using the chat completions API (recommended), completions API, or responses API. [Browse 100+ available models \\u2192](https://fireworks.ai/models)\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nChat Completions API\\\\n\\\\n  * Python\\\\n\\\\n  * JavaScript\\\\n\\\\n  * curl\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nimport os\\\\nfrom openai import OpenAI\\\\n\\\\nclient = OpenAI(\\\\n    api_key=os.environ.get(\\\"FIREWORKS_API_KEY\\\"),\\\\n    base_url=\\\"https://api.fireworks.ai/inference/v1\\\"\\\\n)\\\\n\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\\n        \\\"role\\\": \\\"user\\\",\\\\n        \\\"content\\\": \\\"Explain quantum computing in simple terms\\\"\\\\n    }]\\\\n)\\\\n\\\\nprint(response.choices[0].message.content)\\\\n\\\\n```\\\\n\\\\nMost models automatically format your messages with the correct template. To verify the exact prompt used, enable the `echo` parameter.\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nAlternative query methods\\\\n\\\\nFireworks also supports [Completions API](/guides/completions-api) and [Responses API](/guides/response-api).\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nQuerying dedicated deployments\\\\n\\\\nFor consistent performance, guaranteed capacity, or higher throughput, you can query [on-demand deployments](/guides/ondemand-deployments) instead of serverless models. Deployments use the same APIs with a deployment-specific model identifier:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\n&lt;MODEL_NAME&gt;#&lt;DEPLOYMENT_NAME&gt;\\\\n\\\\n```\\\\n\\\\nFor example:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1#accounts/&lt;ACCOUNT_ID&gt;/deployments/&lt;DEPLOYMENT_ID&gt;\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}]\\\\n)\\\\n\\\\n```\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nCommon patterns\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nMulti-turn conversations\\\\n\\\\nMaintain conversation history by including all previous messages:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nmessages = [\\\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\\\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What\\\\'s the capital of France?\\\"},\\\\n    {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"The capital of France is Paris.\\\"},\\\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"What\\\\'s its population?\\\"}\\\\n]\\\\n\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=messages\\\\n)\\\\n\\\\nprint(response.choices[0].message.content)\\\\n\\\\n```\\\\n\\\\nThe model uses the full conversation history to provide contextually relevant responses.\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nSystem prompts\\\\n\\\\nOverride the default system prompt by setting the first message with `role: \\\"system\\\"`:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nmessages = [\\\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful Python expert who provides concise code examples.\\\"},\\\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"How do I read a CSV file?\\\"}\\\\n]\\\\n\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=messages\\\\n)\\\\n\\\\n```\\\\n\\\\nTo completely omit the system prompt, set the first message\\u2019s `content` to an empty string.\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nStreaming responses\\\\n\\\\nStream tokens as they\\u2019re generated for real time, interactive UX. Covered in detail in the [Serverless Quickstart](/getting-started/quickstart#streaming-responses).\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nstream = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Tell me a story\\\"}],\\\\n    stream=True\\\\n)\\\\n\\\\nfor chunk in stream:\\\\n    if chunk.choices[0].delta.content:\\\\n        print(chunk.choices[0].delta.content, end=\\\"\\\")\\\\n\\\\n```\\\\n\\\\n**Aborting streams:** Close the connection to stop generation and avoid billing for ungenerated tokens:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nfor chunk in stream:\\\\n    print(chunk.choices[0].delta.content, end=\\\"\\\")\\\\n    if some_condition:\\\\n        stream.close()\\\\n        break\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nAsync requests\\\\n\\\\nUse async clients to make multiple concurrent requests for better throughput:\\\\n\\\\n  * Python\\\\n\\\\n  * JavaScript\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nimport asyncio\\\\nfrom openai import AsyncOpenAI\\\\n\\\\nclient = AsyncOpenAI(\\\\n    api_key=os.environ.get(\\\"FIREWORKS_API_KEY\\\"),\\\\n    base_url=\\\"https://api.fireworks.ai/inference/v1\\\"\\\\n)\\\\n\\\\nasync def main():\\\\n    response = await client.chat.completions.create(\\\\n        model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n        messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}]\\\\n    )\\\\n    print(response.choices[0].message.content)\\\\n\\\\nasyncio.run(main())\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nUsage & performance tracking\\\\n\\\\nEvery response includes token usage information and performance metrics for debugging and observability. For aggregate metrics over time, see the [usage dashboard](https://app.fireworks.ai/account/usage). **Token usage** (prompt, completion, total tokens) is included in the response body for all requests. **Performance metrics** (latency, time-to-first-token, etc.) are included in response headers for non-streaming requests. For streaming requests, use the `perf_metrics_in_response` parameter to include all metrics in the response body.\\\\n\\\\n  * Non-streaming\\\\n\\\\n  * Streaming (usage only)\\\\n\\\\n  * Streaming (with performance metrics)\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}]\\\\n)\\\\n\\\\n# Token usage (always included)\\\\nprint(response.usage.prompt_tokens)      # Tokens in your prompt\\\\nprint(response.usage.completion_tokens)  # Tokens generated\\\\nprint(response.usage.total_tokens)       # Total tokens billed\\\\n\\\\n# Performance metrics are in response headers:\\\\n# fireworks-prompt-tokens, fireworks-server-time-to-first-token, etc.\\\\n\\\\n```\\\\n\\\\nUsage information is automatically included in the final chunk for streaming responses (the chunk with `finish_reason` set). This is a Fireworks extension - OpenAI SDK doesn\\u2019t return usage for streaming by default.\\\\n\\\\nFor all available metrics and details, see the [API reference documentation](/api-reference/post-chatcompletions#response-perf_metrics).\\\\n\\\\nIf you encounter errors during inference, see [Inference Error Codes](/guides/inference-error-codes) for common issues and resolutions.\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nAdvanced capabilities\\\\n\\\\nExtend text models with additional features for structured outputs, tool integration, and performance optimization:\\\\n\\\\n## [Tool callingConnect models to external tools and APIs with type-safe parameters](/guides/function-calling)## [Structured outputsEnforce JSON schemas for reliable data extraction](/structured-responses/structured-response-formatting)## [Responses APIMulti-step reasoning for complex problem-solving](/guides/response-api)## [Predicted outputsSpeed up edits by predicting unchanged sections](/guides/predicted-outputs)## [Prompt cachingCache common prompts to reduce latency and cost](/guides/prompt-caching)## [Batch inferenceProcess large volumes of requests asynchronously](/guides/batch-inference)\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nConfiguration & debugging\\\\n\\\\nSampling parameters\\\\n\\\\nControl how the model generates text. Fireworks automatically uses recommended sampling parameters from each model\\u2019s HuggingFace `generation_config.json` when you don\\u2019t specify them explicitly, ensuring optimal performance out-of-the-box.We pull `temperature`, `top_k`, `top_p`, `min_p`, and `typical_p` from the model\\u2019s configuration when not explicitly provided.\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nTemperature\\\\n\\\\nAdjust randomness (0 = deterministic, higher = more creative):\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Write a poem\\\"}],\\\\n    temperature=0.7  # Override model default\\\\n)\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nMax tokens\\\\n\\\\nControl the maximum number of tokens in the generated completion:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nmax_tokens=100  # Generate at most 100 tokens\\\\n\\\\n```\\\\n\\\\n**Important notes:**\\\\n\\\\n  * Default value is 2048 tokens if not specified\\\\n  * Most models support up to their full context window (e.g., 128K for DeepSeek R1)\\\\n  * When the limit is reached, you\\u2019ll see `\\\"finish_reason\\\": \\\"length\\\"` in the response\\\\n\\\\nSet `max_tokens` appropriately for your use case to avoid truncated responses. Check the model\\u2019s context window in the [Model Library](https://fireworks.ai/models).\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nTop-p (nucleus sampling)\\\\n\\\\nConsider only the most probable tokens summing to `top_p` probability mass:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\ntop_p=0.9  # Consider top 90% probability mass\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nTop-k\\\\n\\\\nConsider only the k most probable tokens:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\ntop_k=50  # Consider top 50 tokens\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nMin-p\\\\n\\\\nExclude tokens below a probability threshold:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nmin_p=0.05  # Exclude tokens with &lt;5% probability\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nTypical-p\\\\n\\\\nUse typical sampling to select tokens with probability close to the entropy of the distribution:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\ntypical_p=0.95  # Consider tokens with typical probability\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nRepetition penalties\\\\n\\\\nReduce repetitive text with `frequency_penalty`, `presence_penalty`, or `repetition_penalty`:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nfrequency_penalty=0.5,   # Penalize frequent tokens (OpenAI compatible)\\\\npresence_penalty=0.5,    # Penalize any repeated token (OpenAI compatible)\\\\nrepetition_penalty=1.1   # Exponential penalty from prompt + output\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nSampling options header\\\\n\\\\nThe `fireworks-sampling-options` header contains the actual default sampling parameters used for the model, including values from the model\\u2019s HuggingFace `generation_config.json`:\\\\n\\\\n  * Python\\\\n\\\\n  * JavaScript\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nimport os\\\\nfrom openai import OpenAI\\\\n\\\\nclient = OpenAI(\\\\n    api_key=os.environ.get(\\\"FIREWORKS_API_KEY\\\"),\\\\n    base_url=\\\"https://api.fireworks.ai/inference/v1\\\"\\\\n)\\\\n\\\\nresponse = client.chat.completions.with_raw_response.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}]\\\\n)\\\\n\\\\n# Access headers from the raw response\\\\nsampling_options = response.headers.get(\\\\'fireworks-sampling-options\\\\')\\\\nprint(sampling_options)  # e.g., \\\\'{\\\"temperature\\\": 0.7, \\\"top_p\\\": 0.9}\\\\'\\\\n\\\\ncompletion = response.parse()  # get the parsed response object\\\\nprint(completion.choices[0].message.content)\\\\n\\\\n```\\\\n\\\\nSee the [API reference](/api-reference/post-chatcompletions) for detailed parameter descriptions.\\\\n\\\\nMultiple generations\\\\n\\\\nGenerate multiple completions in one request:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Tell me a joke\\\"}],\\\\n    n=3  # Generate 3 different jokes\\\\n)\\\\n\\\\nfor choice in response.choices:\\\\n    print(choice.message.content)\\\\n\\\\n```\\\\n\\\\nToken probabilities (logprobs)\\\\n\\\\nInspect token probabilities for debugging or analysis:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}],\\\\n    logprobs=True,\\\\n    top_logprobs=5  # Show top 5 alternatives per token\\\\n)\\\\n\\\\nfor content in response.choices[0].logprobs.content:\\\\n    print(f\\\"Token: {content.token}, Logprob: {content.logprob}\\\")\\\\n\\\\n```\\\\n\\\\nPrompt inspection (echo & raw_output)\\\\n\\\\nVerify how your prompt was formatted:**Echo:** Return the prompt along with the generation:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}],\\\\n    echo=True\\\\n)\\\\n\\\\n```\\\\n\\\\n**Raw output:** See raw token IDs and prompt fragments:\\\\n\\\\nExperimental API - may change without notice.\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}],\\\\n    raw_output=True\\\\n)\\\\n\\\\nprint(response.raw_output.prompt_token_ids)  # Token IDs\\\\nprint(response.raw_output.completion)        # Raw completion\\\\n\\\\n```\\\\n\\\\nIgnore EOS token\\\\n\\\\nForce generation to continue past the end-of-sequence token (useful for benchmarking):\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}],\\\\n    ignore_eos=True,\\\\n    max_tokens=100  # Will always generate exactly 100 tokens\\\\n)\\\\n\\\\n```\\\\n\\\\nOutput quality may degrade when ignoring EOS. This API is experimental and should not be relied upon for production use cases.\\\\n\\\\nLogit bias\\\\n\\\\nModify token probabilities to encourage or discourage specific tokens:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}],\\\\n    logit_bias={\\\\n        123: 10.0,   # Strongly encourage token ID 123\\\\n        456: -50.0   # Strongly discourage token ID 456\\\\n    }\\\\n)\\\\n\\\\n```\\\\n\\\\nMirostat sampling\\\\n\\\\nControl perplexity dynamically using the [Mirostat algorithm](https://arxiv.org/abs/2007.14966):\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nresponse = client.chat.completions.create(\\\\n    model=\\\"accounts/fireworks/models/deepseek-v3p1\\\",\\\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello\\\"}],\\\\n    mirostat_target=5.0,  # Target perplexity\\\\n    mirostat_lr=0.1       # Learning rate for adjustments\\\\n)\\\\n\\\\n```\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nUnderstanding tokens\\\\n\\\\nLanguage models process text in chunks called **tokens**. In English, a token can be as short as one character or as long as one word. Different model families use different **tokenizers** , so the same text may translate to different token counts depending on the model. **Why tokens matter:**\\\\n\\\\n  * Models have maximum context lengths measured in tokens\\\\n  * Pricing is based on token usage (prompt + completion)\\\\n  * Token count affects response time\\\\n\\\\nFor Llama models, use [this tokenizer tool](https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/) to estimate token counts. Actual usage is returned in the `usage` field of every API response.\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nOpenAI SDK Migration\\\\n\\\\nOpenAI SDK compatibility notes\\\\n\\\\nFireworks provides an OpenAI-compatible API, making migration straightforward. However, there are some minor differences to be aware of:\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nBehavioral differences\\\\n\\\\n**`stop` parameter:**\\\\n\\\\n  * **Fireworks** : Returns text including the stop word\\\\n  * **OpenAI** : Omits the stop word\\\\n  * _You can easily truncate it client-side if needed_\\\\n\\\\n**`max_tokens` with context limits:**\\\\n\\\\n  * **Fireworks** : Automatically adjusts `max_tokens` lower if `prompt + max_tokens` exceeds the model\\u2019s context window\\\\n  * **OpenAI** : Returns an invalid request error\\\\n  * _Control this behavior with the`context_length_exceeded_behavior` parameter_\\\\n\\\\n**Streaming usage stats:**\\\\n\\\\n  * **Fireworks** : Returns `usage` field in the final chunk (where `finish_reason` is set) for both streaming and non-streaming\\\\n  * **OpenAI** : Only returns usage for non-streaming responses\\\\n\\\\nExample accessing streaming usage:\\\\n\\\\nCopy\\\\n\\\\nAsk AI\\\\n\\\\n```\\\\nfor chunk in client.chat.completions.create(stream=True, ...):\\\\n    if chunk.usage:  # Available in final chunk\\\\n        print(f\\\"Tokens: {chunk.usage.total_tokens}\\\")\\\\n\\\\n```\\\\n\\\\n###\\\\n\\\\n\\\\u200b\\\\n\\\\nUnsupported parameters\\\\n\\\\nThe following OpenAI parameters are not yet supported:\\\\n\\\\n  * `presence_penalty`\\\\n  * `frequency_penalty`\\\\n  * `best_of` (use `n` instead)\\\\n  * `logit_bias`\\\\n  * `functions` (deprecated - use [Tool Calling](/guides/function-calling) with the `tools` parameter instead)\\\\n\\\\nHave a use case requiring one of these? [Join our Discord](https://discord.gg/fireworks-ai) to discuss.\\\\n\\\\n##\\\\n\\\\n\\\\u200b\\\\n\\\\nNext steps\\\\n\\\\n## [Vision modelsProcess images alongside text](/guides/querying-vision-language-models)## [Audio modelsTranscribe and translate audio](/guides/querying-asr-models)## [EmbeddingsGenerate vector representations for search](/guides/querying-embeddings-models)## [On-demand deploymentsDeploy models on dedicated GPUs](/guides/ondemand-deployments)## [Fine-tuningCustomize models for your use case](/fine-tuning/finetuning-intro)## [Error codesTroubleshoot common inference errors](/guides/inference-error-codes)## [API ReferenceComplete API documentation](/api-reference/post-chatcompletions)\\\\n\\\\nWas this page helpful?\\\\n\\\\nYesNo\\\\n\\\\n[Exporting MetricsPrevious](/deployments/exporting-metrics)[Vision ModelsNext](/guides/querying-vision-language-models)\\\\n\\\\n\\u2318I\\\\n\\\\n[Fireworks AI Docs home page![light logo](https://mintcdn.com/firew&lt;TRUNCATED&gt;\"\n}\n\nI don‚Äôt see specific documentation for kimi-k2-thinking model parameters in that page. Let me search for more specific information about thinking models and prefilling:Let me search more specifically for prefilling and JSON formatting with thinking models:Based on my search, I don‚Äôt see specific documentation about prefilling or thinking mode parameters for the kimi-k2-thinking model on Fireworks. However, here‚Äôs what I found:\n* * For JSON formatting, Fireworks supports response_format with {\"type\": \"json_object\"} to guarantee valid JSON output. * You should also instruct the model to produce JSON via a system or user message, otherwise it may generate whitespace.\nFor prefilling with {, this is typically an Anthropic-specific feature that may not be available in the Fireworks API. The standard approach would be:\nresponse_format={\"type\": \"json_object\"}\nAs for ‚Äúthinking mode‚Äù - * Kimi K2 Thinking is built as a thinking agent that reasons step-by-step, so the thinking behavior should be built-in. You likely don‚Äôt need a special parameter to enable it.\nWould you like to try using response_format for JSON output instead of prefilling?\nYes how to change the generator code?\n\n\nü§ñReplyü§ñ\nYou‚Äôll need to modify the generate method in your Generator class to add the response_format parameter. Here‚Äôs the key change:\nresponse = self.llm_client.chat.completions.create(\n    **formatted_prompt['messages'],\n    response_format={\"type\": \"json_object\"}\n)\nHowever, I notice your current code is calling formatted_prompt['messages'] directly - can you show me what your prompt_template.build() method returns? That will help me give you the exact syntax you need.\n\nloaded_prompt = braintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt-v1\")\n\n\nqa.iloc[0]\n\nid                                                                   q003\nquestion                What is the name of the benchmark suite presen...\nanswer                                            The ML.ENERGY Benchmark\nanswer_value                                          ML.ENERGY Benchmark\nanswer_unit                                                      is_blank\nref_id                                                      ['chung2025']\nref_url                              ['https://arxiv.org/pdf/2505.06371']\nsupporting_materials    We present the ML.ENERGY Benchmark, a benchmar...\nexplanation                                                         Quote\nName: 0, dtype: object\n\n\n\nquery_text = 'What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?'\nretrieved_chunks = Retriever(embedder, qs).retrieve(query_text, limit=10)\ngenerator = Generator(llm, loaded_prompt)\ngenerator.generate(query_text, retrieved_chunks)\n\n{'answer': 'ML.ENERGY Benchmark',\n 'answer_value': 'ML.ENERGY Benchmark',\n 'answer_unit': 'is_blank',\n 'ref_id': ['chung2025'],\n 'ref_url': ['https://arxiv.org/pdf/2505.06371'],\n 'supporting_materials': 'We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments',\n 'explanation': 'Quote'}",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#rag",
    "href": "wattbot_sdk_solveit.html#rag",
    "title": "Initial Development with Solveit",
    "section": "RAG",
    "text": "RAG\n\nclass RAG:\n    def __init__(self, retriever, generator):\n        fc.store_attr()\n    \n    def answer_question(self, query, limit=5):\n        retrieved_chunks = self.retriever.retrieve(query, limit=limit)\n        return self.generator.generate(query, retrieved_chunks), retrieved_chunks\n    \n    def predict_dataset(self, df, limit=5, n_workers=4):\n        def process_row(idx_r):\n            idx, r = idx_r\n            answer, retrieved_chunks = self.answer_question(r['question'], limit=limit)\n            if not isinstance(answer, dict): print(idx)\n            return {\n                'id': r['id'],\n                'question': r['question'],\n                'answer': answer['answer'],\n                'answer_value': answer['answer_value'],\n                'answer_unit': answer['answer_unit'],\n                'ref_id': answer['ref_id'],\n                'ref_url': answer['ref_url'],\n                'supporting_materials': answer['supporting_materials'],\n                'explanation': answer['explanation']\n            }\n    \n        results = fc.parallel(process_row, df.iterrows(), threadpool=True, n_workers=n_workers)\n        return pd.DataFrame(list(results))\n\n\nrag = RAG(retriever, generator)\npred_answer, retrieved_chunks = rag.answer_question(query_text, limit=10); pred_answer\n\n{'answer': 'ML.ENERGY Benchmark',\n 'answer_value': 'ML.ENERGY Benchmark',\n 'answer_unit': 'is_blank',\n 'ref_id': ['chung2025'],\n 'ref_url': ['https://arxiv.org/pdf/2505.06371'],\n 'supporting_materials': 'The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization',\n 'explanation': 'Quote'}",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "wattbot_sdk_solveit.html#evaluation",
    "href": "wattbot_sdk_solveit.html#evaluation",
    "title": "Initial Development with Solveit",
    "section": "Evaluation",
    "text": "Evaluation\n\ndef is_numeric(value):\n    try:\n        float(value)\n        return True\n    except (ValueError, TypeError):\n        return False\n\n\ndef score_answer_value(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    if isinstance(expected, str) and expected.startswith('['): expected = ast.literal_eval(expected)\n    \n    if is_numeric(expected) and is_numeric(predicted):\n        pred_num, exp_num = map(float, (predicted, expected))\n        return 1.0 if abs(pred_num - exp_num) &lt;= abs(exp_num * 0.001) else 0.0\n    else:\n        return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0\n\n\nscore_answer_value([80,90], \"[80,90]\")\n\n1.0\n\n\nI want to change the answer_value to evaluate the ranges properly\n\nü§ñReplyü§ñ\nTo handle range evaluation properly, you need to detect when the expected value is a range (like [80,90]) and check if the predicted value falls within that range.\nHere are a few approaches:\n\nParse ranges and check if predicted value is within bounds\nHandle both list formats [80,90] and string formats \"[80,90]\"\nConsider whether predicted should be a single value in the range or also a range\n\nWhat format is your predicted value in - is it a single number, a list, or a string? And should it match the exact range or just fall within it?\nThe answer value will be a list but the dataframe can have string of list\n\n\nü§ñReplyü§ñ\nGot it! So you need to:\n\nParse the string representation of a list (like \"[80,90]\") into an actual list\nCompare two lists for equality\n\nYou can use ast.literal_eval() to safely parse the string into a list:\nimport ast\nparsed = ast.literal_eval(\"[80,90]\")  # Returns [80, 90]\nThen compare the predicted list with the parsed expected list. Would you like to update the score_answer_value function to handle this, or would you prefer to try it yourself first?\nPlease update it\n\n\nü§ñReplyü§ñ\nHere‚Äôs the updated function to handle list comparisons:\ndef score_answer_value(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n    \n    # Handle list/range comparisons\n    if isinstance(expected, str) and expected.startswith('['):\n        try:\n            expected = ast.literal_eval(expected)\n        except (ValueError, SyntaxError):\n            pass\n    \n    if isinstance(expected, list) and isinstance(predicted, list):\n        return 1.0 if predicted == expected else 0.0\n        \n    if is_numeric(expected) and is_numeric(predicted):\n        pred_num, exp_num = map(float, (predicted, expected))\n        return 1.0 if abs(pred_num - exp_num) &lt;= abs(exp_num * 0.001) else 0.0\n    else:\n        return 1.0 if str(predicted).strip().lower() == str(expected).strip().lower() else 0.0\nThis adds list handling before the numeric check. Try it with your test case and let me know if it works!\n\nscore_answer_value(pred_answer['answer_value'], qa.iloc[0]['answer_value'])\n\n1.0\n\n\n\ndef score_ref_id(predicted, expected):\n    if expected == \"is_blank\":\n        return 1.0 if predicted == \"is_blank\" else 0.0\n        \n    if isinstance(expected, str) and expected.startswith('['): expected = ast.literal_eval(expected)\n        \n    pred_set = set(predicted) if isinstance(predicted, list) else set([predicted])\n    exp_set = set(expected) if isinstance(expected, list) else set([expected])\n        \n    intersection = len(pred_set.intersection(exp_set))\n    union = len(pred_set.union(exp_set))\n        \n    return intersection / union if union &gt; 0 else 0.0\n\n\ndef score_is_na(predicted_answer, expected_answer):\n    expected_is_na = expected_answer['answer_value'] == 'is_blank'\n    predicted_is_na = predicted_answer['answer_value'] == 'is_blank'\n    return 1.0 if expected_is_na == predicted_is_na else 0.0\n\n\ndef calculate_wattbot_score(predicted_answer, expected_row):\n    if 'answer_value' not in predicted_answer: print(expected_row['id'], predicted_answer)\n    answer_score = score_answer_value(predicted_answer['answer_value'], expected_row['answer_value'])\n    ref_score = score_ref_id(predicted_answer['ref_id'], expected_row['ref_id'])\n    na_score = score_is_na(predicted_answer, expected_row)\n    score = 0.75 * answer_score + 0.15 * ref_score + 0.10 * na_score\n    return fc.NS(score=score, answer_score=answer_score, ref_score=ref_score, na_score=na_score)\n\n\ncalculate_wattbot_score(pred_answer, qa.iloc[0])\n\nnamespace(score=1.0, answer_score=1.0, ref_score=1.0, na_score=1.0)\n\n\n\nclass Evaluator:\n    def __init__(self, rag):\n        fc.store_attr()\n        self.results = []\n    \n    def evaluate_dataset(self, df, limit=5):\n        experiment = braintrust.init(project=\"wattbot\", experiment=\"evaluation\")\n        def evaluate_row(idx_r):\n            idx, r = idx_r\n            predicted_answer, retrieved_chunks = self.rag.answer_question(r['question'], limit=limit)\n            wattbot_score = calculate_wattbot_score(predicted_answer, r)\n            context = self.rag.generator.create_context(retrieved_chunks)\n            prompt = self.rag.generator.format_prompt(r['question'], context)\n            experiment.log(input=r['question'], output=predicted_answer, expected=r, scores=wattbot_score.__dict__, metadata = {'context': context, 'prompt': prompt})\n            return fc.NS(qid=r['id'], q=r['question'], predicted=predicted_answer, expected=r, **wattbot_score.__dict__, context=context)\n        \n        self.results = fc.parallel(evaluate_row, df.iterrows(), threadpool=True)\n        return sum(r.score for r in self.results) / len(self.results)\n    \n    def get_worst_predictions(self, n=5):\n        sorted_results = sorted(self.results, key=lambda r: r.score)\n        return sorted_results[:n]\n\nHow can I have the context and prompt logged into braintrust respecting the newlines?\n\n\nü§ñReplyü§ñ\nThe newlines in your context and prompt strings might not be preserved when logged to Braintrust. To ensure they‚Äôre displayed properly, you can try:\n\nStore as a list instead of a single string - Break the context into separate chunks\nUse explicit newline encoding - Though this usually isn‚Äôt necessary\nCheck Braintrust‚Äôs UI settings - Some UIs have options to preserve formatting\n\nThe simplest approach is to log the context as a structured object:\nmetadata = {\n    'context_chunks': [chunk.payload for chunk in retrieved_chunks],\n    'prompt': prompt\n}\nWould you like to try this approach, or would you prefer to keep it as a single formatted string and investigate Braintrust‚Äôs display options?\nIn braintrust for some examples it is respecting the new line and for some it does not. Is there an issue on our code?\n\n\nü§ñReplyü§ñ\nThat‚Äôs interesting - inconsistent newline handling suggests the issue might be with how the data is being formatted before logging.\nA few things to check:\n\nAre the newlines actually in the strings (\\n) or are they literal line breaks?\nIs the context or prompt being processed differently in some cases (e.g., through JSON serialization)?\nAre you logging strings vs dict values differently?\n\nCould you show me an example of one that works and one that doesn‚Äôt? Or check if the create_context method is consistently returning strings with \\n characters?\nYes please\n\n\nü§ñReplyü§ñ\nThe core issue is that self.rag.retrieved_chunks is shared state that gets overwritten by concurrent threads. Here are two solutions:\nOption 1: Make RAG return chunks along with the answer\nModify your RAG.answer_question method to return both the answer and the chunks:\ndef answer_question(self, query, limit=5):\n    retrieved_chunks = self.retriever.retrieve(query, limit=limit)\n    answer = self.generator.generate(query, retrieved_chunks)\n    return answer, retrieved_chunks\nThen update your evaluator to use the returned chunks instead of the shared state.\nOption 2: Remove threading\nChange threadpool=True to threadpool=False in your fc.parallel call. This makes execution sequential, avoiding the race condition entirely.\nWhich approach would you prefer? Option 1 is better if you want to keep parallel execution for speed, while Option 2 is simpler if speed isn‚Äôt critical.\nDo you see any other race condition problem with the code?\n\n\nü§ñReplyü§ñ\nYes! Looking at your code more carefully, there‚Äôs another race condition in the Generator class and the braintrust.init call:\n\nBraintrust experiment object: You‚Äôre creating experiment = braintrust.init(...) once, but then multiple threads are calling experiment.log() simultaneously. This could cause issues depending on whether Braintrust‚Äôs logging is thread-safe.\nLLM client state: Your Generator uses self.llm_client, and if multiple threads call generate() at the same time, they‚Äôre all sharing the same client connection. This might work depending on the client implementation, but could cause problems.\n\nDo you want to explore solutions for these, or would you like to first check if there are other issues you‚Äôve noticed?\n\nevals = Evaluator(rag)\nevals.evaluate_dataset(df=qa, limit=10)\n\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\n\n\n0.7838414634146342\n\n\nWhy some of the outputs are empty? This is for the kimi k2 thinking model whereas the deepseek model gives proper output\n\n\nü§ñReplyü§ñ\nThe empty outputs ({}) from the Kimi K2 Thinking model suggest it‚Äôs not returning valid JSON in some cases. This could be due to:\n\nThinking tokens interfering with JSON - The model might be outputting reasoning/thinking text before the JSON, breaking the json.loads() parsing\nResponse format not properly enforced - The response_format={\"type\": \"json_object\"} might not be working as expected with this model\nModel-specific behavior - Thinking models sometimes structure their output differently\n\nTo debug this, you could:\n\nPrint the raw response content before parsing to see what the model is actually returning\nTry adding error handling to catch and log failed parses\nCheck if the prompt needs to be more explicit about JSON-only output for thinking models\n\nWould you like to add some debugging to see what the raw responses look like?\n\ntst_ans = rag.predict_dataset(tst)\n\n\ntst_ans.isnull().sum()\n\nid                      0\nquestion                0\nanswer                  0\nanswer_value            0\nanswer_unit             0\nref_id                  0\nref_url                 0\nsupporting_materials    0\nexplanation             0\ndtype: int64\n\n\n\ntst_ans.to_csv(data_path/'tst_ans_v5.csv', index=False)\n\nWhat are other ways to improve this model?\n\n\nü§ñReplyü§ñ\nGreat question! Here are several ways you could improve your RAG model‚Äôs performance:\nRetrieval improvements: - Experiment with different chunk sizes and overlap (your current chunk_size=1500, step=1400) - Try different embedding models or ensemble multiple embeddings - Adjust the hybrid search fusion weights between dense and sparse - Increase the number of retrieved chunks and use reranking\nGeneration improvements: - Experiment with different LLMs or prompt engineering - Add few-shot examples to your prompt template - Implement answer verification or self-consistency checks\nEvaluation-driven improvements: - Analyze your worst predictions (you already have get_worst_predictions) - Look for patterns in what types of questions fail\nWhich area interests you most, or would you like to start by examining your worst predictions to see where the model struggles?\n\nevals.get_worst_predictions()\n\n[namespace(qid='q280',\n           q='Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.',\n           predicted={'answer': 'Unanswerable from the provided context',\n                      'answer_value': 'is_blank',\n                      'answer_unit': 'is_blank',\n                      'ref_id': ['is_blank'],\n                      'ref_url': ['is_blank'],\n                      'supporting_materials': 'is_blank',\n                      'explanation': 'is_blank'},\n           expected=id                                                                   q280\n                    question                Given the total pre-training GPU hours and the...\n                    answer                                                           ~13 days\n                    answer_value                                                           13\n                    answer_unit                                                          days\n                    ref_id                                                       ['shen2024']\n                    ref_url                              ['https://arxiv.org/pdf/2404.07413']\n                    supporting_materials    \"‚Ä¶30,000 H100 GPU hours‚Ä¶ We conduct training o...\n                    explanation             Math: wall_clock_hours ‚âà 30,000 GPUh √∑ 96 GPUs...\n                    Name: 34, dtype: object,\n           score=0.0,\n           answer_score=0.0,\n           ref_score=0.0,\n           na_score=0.0,\n           context='Doc ID: strubell2019\\nURL: https://arxiv.org/pdf/1906.02243\\nTitle: Energy and Policy Considerations for Deep Learning in NLP\\nText: day . W e train all models on\\na single NVIDIA Titan X GPU, with the excep-\\ntion of ELMo which was trained on 3 NVIDIA\\nGTX 1080 Ti GPUs. While training, we repeat-\\nedly query the NVIDIA System Management In-\\nterface\\n2 to sample the GPU power consumption\\nand report the average over all samples. T o sample\\nCPU power consumption, we use Intel‚Äôs Running\\nA verage Power Limit interface.\\n3\\n2 nvidia-smi: https://bit.ly/30sGEbi\\n3 RAPL power meter: https://bit.ly/2LObQhV\\nConsumer Renew . Gas Coal Nuc.\\nChina 22% 3% 65% 4%\\nGermany 40% 7% 38% 13%\\nUnited States 17% 35% 27% 19%\\nAmazon-A WS 17% 24% 30% 26%\\nGoogle 56% 14% 15% 10%\\nMicrosoft 32% 23% 31% 10%\\nT able 2: Percent energy sourced from: Renewable (e.g.\\nhydro, solar, wind), natural gas, coal and nuclear for\\nthe top 3 cloud compute providers (\\nCook et al. , 2017),\\ncompared to the United States, 4 China5 and Germany\\n(Burger, 2019).\\nW e estimate the total time expected for mod-\\nels to train to completion using training times and\\nhardware reported in the original papers. W e then\\ncalculate the power consumption in kilowatt-hours\\n(kWh) as follows. Let pc be the average power\\ndraw (in watts) from all CPU sockets during train-\\ning, let pr be the average power draw from all\\nDRAM (main memory) sockets, let pg be the aver-\\nage power draw of a GPU during training, and let\\ng be the number of GPUs used to train. W e esti-\\nmate total power consumption as combined GPU,\\nCPU and DRAM consumption, then multiply this\\nby Power Usage Effectiveness (PUE), whi\\n\\nDoc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText: 2, June 21‚Äì24, 2022, Seoul, Republic of Korea\\nModel BERT BERT 6B Dense Dense Dense ViT ViT ViT ViT ViT\\nfinetune pretrain Transf. 121 169 201 Tiny Small Base Large Huge\\nGPU 4¬∑V100 8¬∑V100 256¬∑A100 1¬∑P40 1¬∑P40 1¬∑P40 1¬∑V100 1¬∑V100 1¬∑V100 4¬∑V100 4¬∑V100\\nHours 6 36 192 0.3 0.3 0.4 19 19 21 90 216\\nkWh 3.1 37.3 13,812.4 0.02 0.03 0.04 1.7 2.2 4.7 93.3 237.6\\nTable 2. For the 11 models in our analysis: the type of GPU, the number of GPUs of that type, the number of hours, and the energy\\nused in kWh. For example, our BERT language modeling (BERT LM) experiment used 8 V100 GPUs for 36 hours and used a total of\\n37.3 kWh. We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would\\ntake to train to completion, we estimate a full training run would consume approximately 103,593 kWh.\\n4.1 NLP\\nBERT Training. We monitored the energy consumption while training a BERT-small model [8] for approximately 36\\nhours on 8 NVIDIA V100 GPUs. That training run consumed over 37 kWh of electricity.\\nBERT Finetuning. We tracked the energy consumption while finetuning the BERT-small model on a standard natural\\nlanguage inference task [48, MNLI] for approximately 6 hours on 4 NVIDIA V100 GPUs. Our finetuning run consumed\\naround 3.2 kWh of electricity, i.e., less than one tenth that due to BERT-small pre-training.\\n6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising\\nover 6.1 billion parameters dur\\n\\nDoc ID: strubell2019\\nURL: https://arxiv.org/pdf/1906.02243\\nTitle: Energy and Policy Considerations for Deep Learning in NLP\\nText: n, an increase of just 0.1\\nBLEU at the cost of at least $150k in on-demand\\ncompute time and non-trivial carbon emissions.\\n4.2 Cost of development: Case study\\nT o quantify the computational requirements of\\nR&D for a new model we study the logs of\\nall training required to develop Linguistically-\\nInformed Self-Attention (\\nStrubell et al. , 2018), a\\nmulti-task model that performs part-of-speech tag-\\nging, labeled dependency parsing, predicate detec-\\ntion and semantic role labeling. This model makes\\nfor an interesting case study as a representative\\nNLP pipeline and as a Best Long Paper at EMNLP .\\nModel training associated with the project\\nspanned a period of 172 days (approx. 6 months).\\nDuring that time 123 small hyperparameter grid\\nsearches were performed, resulting in 4789 jobs\\nin total. Jobs varied in length ranging from a min-\\nimum of 3 minutes, indicating a crash, to a maxi-\\nmum of 9 days, with an average job length of 52\\nhours. All training was done on a combination of\\nNVIDIA Titan X (72%) and M40 (28%) GPUs.\\n8\\nThe sum GPU time required for the project\\ntotaled 9998 days (27 years). This averages to\\n8 W e approximate cloud compute cost using P100 pricing.\\nEstimated cost (USD)\\nModels Hours Cloud compute Electricity\\n1 120 $52‚Äì$175 $5\\n24 2880 $1238‚Äì$4205 $118\\n4789 239,942 $103k‚Äì$350k $9870\\nT able 4: Estimated cost in terms of cloud compute and\\nelectricity for training: (1) a single model (2) a single\\ntune and (3) all models trained during R&D.\\nabout 60 GPUs running constantly th\\n\\nDoc ID: luccioni2023\\nURL: https://arxiv.org/pdf/2302.08476\\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\\nText: e to ML model are rising.\\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\\nwhereas simpler and shallower models such as convolutional neural networks te\\n\\nDoc ID: cottier2024\\nURL: https://arxiv.org/pdf/2405.21015\\nTitle: The Rising Costs of Training Frontier AI Models\\nText: of chips √ó\\n\\x10\\n1 ‚àí exp\\n\\x02\\n‚àí Training time √ó r ln 10\\n\\x03\\x11\\nwhere training time is in years. However, we could estimate chip-hours more often and more reliably than the training\\ntime or the number of chips separately. This is because chip-hours can also be estimated from training compute in\\nFLOP divided by the FLOP/s achieved during training. We used a linear approximation to take advantage of these\\nchip-hour estimates:\\nAmortized training cost = Start value per chip √ó Training chip-hours\\n(365 √ó 24) hours/year √ó r ln 10\\nThis approximation is valid if(Training time)√ór ln 10is small, and this is the case for the training times in our data and\\nour choice of r = 0.14. In an extreme case, a training time of 1 year results in 1 √ó 0.14 ln(10)‚àº= 32%deprecation\\ncompared to 1 ‚àí exp(‚àí1 √ó 0.14 ln(10))‚àº= 28%depreciation. This is not a large difference relative to other sources\\nof uncertainty.\\nDue to NVIDIA covering defects and component failures under warranty, we concluded that hardware failures are not a\\nsignificant source of depreciation relative to hardware progress. As one data point, an average of 1 to 2 failures per\\nweek occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [25]. Even if these were all\\ncatastrophic failures, the expected hardware lifetime would be 3.7 years. We expect that NVIDIA replaces or repairs\\ndefective GPUs on a faster timescale, which makes the cost of failure small compared to hardware price depreciation.\\nA.4 Energy cost estimation\\nTo model th\\n\\nDoc ID: luccioni2023\\nURL: https://arxiv.org/pdf/2302.08476\\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\\nText:  [38] and it remains a fair approximation of the\\nactual energy consumption of many hardware models. We provide more information about TDP and the hardware used\\nfor training the models in our sample in Section A.2 of the Appendix.\\nTraining Time. Training time was computed as the total number of hardware hours, which is different from the\\n\"wall time\" of ML model training, since most models were trained on multiple units at once. For instance, if training a\\n1For instance, methane is 28 times more potent than CO 2 based on its 100-year global warming potential, so energy generation emitting 1 gram of\\nmethane per kWh will emit 28 grams of CO2eq per kWh.\\nManuscript pending review Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 5\\nmodel used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours ; a model using 8 GPUs for 48 hours will\\ntherefore have an equivalent training time.\\n4 DATA ANALYSIS\\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the\\nmain sources of energy used for training (¬ß 4.1), the order of magnitude of CO2 emissions produced (¬ß 4.2), the evolution\\nof these emissions over time (¬ß 4.3) and the relationship between carbon emissions and model performance (¬ß 4.4) 2.\\n4.1 What are the main sources of energy used for training ML models?\\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity\\nof that \\n\\nDoc ID: cottier2024\\nURL: https://arxiv.org/pdf/2405.21015\\nTitle: The Rising Costs of Training Frontier AI Models\\nText:  of about $5,000.\\n14 A.3 Amortization model\\nAs explained in section 2.2, we estimated the value of the training hardware at the beginning of training as:\\nStart value per chip = Acquisition cost per chip\\nexp\\n\\x10\\x02\\nTraining start date ‚àí Hardware availability date\\n\\x03\\n¬∑ r ln 10\\n\\x11\\nwhere r is a depreciation rate in orders of magnitude per year, and the difference in dates is in years. The hardware\\navailability date depended on the type of hardware. If the hardware was a Google TPU, we used the hardware\\nannouncement date. For GPUs, we used a 90-day buffer between the GPU first going on the market and the GPU\\nactually being shipped to the buyer. Our results are robust to variations in this buffer time‚Äîsee Appendix B.4.\\nFor the training start date, there were a few known cases‚Äîfor example, GPT-4 finished training in August 2022 [12].\\nOtherwise, we subtracted the training time from the publication date, and then subtracted a further 60 days to account\\nfor time spent evaluating the model and writing the paper. Again, our results are robust to variations in this buffer. If the\\ntraining time was unknown, we used the median of known values in our dataset, which was approximately 33 days.\\nThe precise way to amortize the training cost through exponential depreciation is:\\nAmortized training cost = Start value per chip √ó Number of chips √ó Depreciation during training\\n= Start value per chip √ó Number of chips √ó\\n\\x10\\n1 ‚àí exp\\n\\x02\\n‚àí Training time √ó r ln 10\\n\\x03\\x11\\nwhere training time is in years. However, we cou\\n\\nDoc ID: wu2021a\\nURL: https://arxiv.org/pdf/2111.00364\\nTitle: Sustainable AI: Environmental Implications, Challenges and Opportunities\\nText:  large collection of diverse ML\\nideas are explored simultaneously at-scale. Thus, during this\\nphase, we observe unique system resource requirements from\\nthe large pool of training experiments. Within Facebook‚Äôs ML\\nresearch cluster, 50% (p50) of ML training experiments take up\\nto 1.5 GPU days while 99% (p99) of the experiments complete\\nwithin 24 GPU days. There are a number of large-scale, trillion\\nparameter models which require over 500 GPUs days.\\nOnce a ML solution is determined as promising, it moves into\\nTraining where the ML solution is evaluated using extensive\\nproduction data ‚Äî data that is more recent, is larger in quantity,\\nand contains richer features . The process often requires\\nadditional hyper-parameter tuning. Depending on the ML task\\nrequirement, the models can be trained/re-trained at different\\nfrequencies. For example, models supporting Facebook‚ÄôsSearch\\nservice were trained at an hourly cadence whereas the Language\\nTranslation models were trained weekly [24]. A p50 production\\nmodel training workÔ¨Çow takes 2.96 GPU days while a training\\nworkÔ¨Çow at p99 can take up to 125 GPU days.\\nFinally, for Inference, the best-performing model is de-\\nployed, producing trillions of daily predictions to serve billions\\nof users worldwide. The total compute cycles for inference\\npredictions are expected to exceed the corresponding training\\ncycles for the deployed model.\\nB. Machine Learning System Life Cycle\\nLife Cycle Analysis (LCA) is a common methodology to\\nassess the carbon emis\\n\\nDoc ID: patterson2021\\nURL: https://arxiv.org/pdf/2104.10350\\nTitle: Carbon Emissions and Large Neural Network Training\\nText: takes   ~14.8   days   for   10,000   \\nGPUs   at   24.6   TeraFLOPS/sec   to   compute   3.14E+23   FLOPS.   For   the   CO 2 e   calculation,   it   doesn‚Äôt   \\nactually   matter   whether   it   takes   2   weeks   on   10,000   GPUs   or   20   weeks   on   1,000   GPUs,   but   we   need   \\none   number   for   Table   4,   so   we   used   NVIDIA‚Äôs   suggestion   of   10,000   GPUs.   \\n‚óè Total   Computation   (Table   1,   row   13;   Table   4,   row   16):    We   calculate   from   measured   performance,   \\nnumber   of   chips,   and   days   to   train   (except   for   GPT-3,   as   OpenAI   published   the   total   FLOPS).   \\n‚óè %   of   Google   2019   Energy   Consumption.   (Table   4,   row   17):    For   all   models   (even   those   not   actually   run   \\nin   Google   datacenters   or   not   run   in   2019),   we   calculate   the   percentage   of   Google‚Äôs   total   energy   \\nconsumption   of   12.2   Terawatt-hours   in   2019   [Goo20].     \\n‚óè Ratio   of   round   trips   (Table   4,   row   22) .   To   give   perspective   on   the   CO 2 e   cost   of   training   a   model   is   \\ncompared   to   other   activities,   we   show   the   CO 2 e   of   passenger   jets.    Google   Flights    calculated   the   \\naverage   CO 2    emission   for   all   the   direct   flights   between   San   Francisco   (SFO)   and   New   York   (JFK)   in   its   \\ndatabase   as   90.2t,   so   the   average   round   trip   is   180.4t.   (This   is   for   the\\n\\nDoc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText: the energy consumption of training a large language model comprising\\nover 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.\\nThis model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate\\nthe total energy consumption to train this model to completion would be approximately (60/8)‚àó13.8 = 103.5 MWh, or\\n103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!\\n4.2 Computer Vision\\nDenseNets. We trained three sizes of DenseNets [19] on MNIST [25]. The jobs lasted between 20 and 25 minutes and\\nconsumed between 20 and 38Wh (or 0.02 to 0.04 kWh) of electricity, which is negligible compared to the other models.\\nVision Transformers. We evaluated the energy consumption during the training of five sizes of Vision Transformers [9]\\non ImageNet [7]. For the smallest ViT experiment (ViT tiny), training lasted around 19 hours on a single V100 and\\nconsumed approximately 1.7 kWh. For the largest one (ViT huge), training lasted more than 9 days on a 4 V100s and\\nconsumed approximately 237 kWh. The full list of models can be found in Table 2.\\n5 EMISSIONS BY REGION AND TIME OF DAY\\nUsing the methodology presented above, we provide some of the first measurements of the differences of actual\\ndatacenters from a major cloud provider. Importantly, what we have is a time series of marginal emissions: for example,\\nif a job were to run from 1 pm to 5 pm in the US West region wit'),\n namespace(qid='q170',\n           q='How many days of CO‚ÇÇ emissions from an average American life are equivalent to training BERT base?',\n           predicted={'answer': '6 years, 1 month',\n                      'answer_value': '6.083',\n                      'answer_unit': 'years',\n                      'ref_id': ['morrison2025'],\n                      'ref_url': ['https://arxiv.org/pdf/2503.05804'],\n                      'supporting_materials': 'Table 2: We list the estimated power usage, carbon emissions, and water consumption from training our dense transformers... Llama 2 7B 81 31 6 yrs, 1 mo - -',\n                      'explanation': 'Table'},\n           expected=id                                                                   q170\n                    question                How many days of CO‚ÇÇ emissions from an average...\n                    answer                                                               14.4\n                    answer_value                                                         14.4\n                    answer_unit                                                          days\n                    ref_id                                                   ['strubell2019']\n                    ref_url                              ['https://arxiv.org/pdf/1906.02243']\n                    supporting_materials                                  Table 1 and Table 3\n                    explanation                               1438 lbs / 99.8 lbs/day = 14.4 \n                    Name: 18, dtype: object,\n           score=0.1,\n           answer_score=0.0,\n           ref_score=0.0,\n           na_score=1.0,\n           context='Doc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText:  average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating\\nelectricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.\\n8 Measuring the Carbon Intensity of AI in Cloud Instances FAccT ‚Äô22, June 21‚Äì24, 2022, Seoul, Republic of Korea\\nFig. 2. Emissions for our 11 experiments described in ¬ß4. For each model we show a vertical blue bar, where the top of the bar is\\nthe max, the bottom is the min, and the black line represents the average emissions (across regions and time of year). First and\\nfourth quartiles are represented by the light blue at the top and bottom of each vertical blue bar. The largest training runs (e.g., 6\\nbillion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only\\ntrained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest\\nexperiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the\\nUnited States Environmental Protection Agency [46].\\nThe largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as\\ndescribed in ¬ß4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of\\nthis size can emit more CO2 than all emissions from the average\\n\\nDoc ID: luccioni2024\\nURL: https://arxiv.org/pdf/2311.16863\\nTitle: Power Hungry Processing: Watts Driving the Cost of AI Deployment?\\nText: s, architecture\\nand carbon emissions of their products, we can make a comparison based on the experiments carried out in the\\npresent study. For instance, the average emissions of a BERT-based model fine-tuned for extractive question answering\\n(bert-large-uncased-whole-word-masking-finetuned-squad), a task akin to extractive web search, is 0.70gùê∂ùëÇ2ùëíùëû\\nper 1,000 queries, which is less than 3 times that of the multi-purpose models (2.36g for Flan-T5 base and 2.34g for\\nBLOOMz-560M). The difference is much more drastic if comparing BERT-based models for tasks such as text classification\\nwith the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of\\nùê∂ùëÇ2ùëíùëûper 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B. For comparison, the first PaLM\\nmodel, released in 2022, has 540 billion parameters [7], whereas GPT-3 has 175 billion parameters [5] 8. While we see\\nthe benefit of deploying generative zero-shot models given their ability to carry out multiple tasks, we do not see\\nconvincing evidence for the necessity of their deployment in contexts where tasks are well-defined, for instance web\\nsearch and navigation, given these models‚Äô energy requirements.\\nFinally, the intent of our study is to set the stage for better understanding of the energy requirements and carbon\\nemissions of the final, often overlooked, step in the ML model life cycle: model deployment. The comparison between\\ntraining, finetuning and inference energ\\n\\nDoc ID: luccioni2023\\nURL: https://arxiv.org/pdf/2302.08476\\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\\nText: omparing the carbon emissions of two or more models and approaches. The\\nfirst paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large\\nTransformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the\\nlifetime emissions of five US cars. [48]. This perspective has since been explored further via analyses of the carbon\\nfootprint of different neural network architectures [31, 37, 38] and the relative efficiency of different methods [35, 56].\\nThese empirical studies are very recent (post-2019), remain relatively sparse and biased towards certain research\\nareas (i.e. Natural Language Processing), and there are many aspects of the emissions of model training that remain\\nunexplored. In sum, there is a need for a more broad and multi-faceted analysis in order to better understand the scale\\nand variation of carbon emissions in our community.\\nTools and approaches for measuring carbon emissions. Developing standardized approaches for estimating the carbon\\nemissions of model training has also been the focus of much work [5, 20, 26, 27, 30, 45, 51]. As a result, there are several\\ntools that exist for this purpose, such as Code Carbon and the Experiment Impact Tracker, which can be used during the\\nmodel training process, or the ML CO2 Calculator, which can be used after training, all of which provide an estimate\\nof the amount of carbon emitted. However, a recent study on different ca\\n\\nDoc ID: luccioni2023\\nURL: https://arxiv.org/pdf/2302.08476\\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\\nText: e to ML model are rising.\\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\\nwhereas simpler and shallower models such as convolutional neural networks te\\n\\nDoc ID: luccioni2025c\\nURL: https://arxiv.org/pdf/2506.15572\\nTitle: Misinformation by Omission: The Need for More Environmental Transparency in AI\\nText:  31\\nIn the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions\\nintensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons)\\nCO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.\\nThe research article was written for a specialized audience of AI and NLP researchers, who would have the background\\nknowledge to understand the appropriate scoping for the estimate. However, an author‚Äôs tweet publicizing the paper and\\nfeaturing a table containing the ‚Äúfive cars‚Äù estimate was widely shared on social media, leading to the publication being picked\\nup by numerous media outlets (including MIT Technology Review32 and Forbes33). The ‚Äúfive cars‚Äù number has since been\\nmisinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of\\narchitectures, training approaches and electricity sources used for powering AI model training; the original article reports AI\\ntraining workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity),\\nand AI model training more broadly often requires even less energy and corresponding emissions.\\nFurther, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much\\nless frequently than the average AI model training w\\n\\nDoc ID: wu2021a\\nURL: https://arxiv.org/pdf/2111.00364\\nTitle: Sustainable AI: Environmental Implications, Challenges and Opportunities\\nText:  carbon footprint of large-scale ML tasks (Figure 4). Taking into\\naccount carbon-free energy, such as solar, the operational energy consumption\\ncan be signiÔ¨Åcantly reduced, leaving the manufacturing carbon cost as the\\ndominating source of AI‚Äôs carbon footprint.\\nBoth Training and Inference can contribute signiÔ¨Åcantly to the\\noverall carbon footprint of machine learning tasks at Facebook.\\nThe exact breakdown between the two phases varies across\\nML use cases.\\nThe overall operational carbon footprint is categorized into\\nofÔ¨Çine training, online training, and inference. OfÔ¨Çine training\\nencompasses both experimentation and training models with\\nhistorical data. Online training is particularly relevant to\\nrecommendation models where parameters are continuously\\nupdated based on recent data. The inference footprint represents\\nthe emission from serving production trafÔ¨Åc. The online training\\nand inference emissions are considered over the period of\\nofÔ¨Çine training. For recommendation use cases, we Ô¨Ånd the\\ncarbon footprint is split evenly between training and inference.\\nOn the other hand, the carbon footprint of LM is dominated\\nby the inference phase, using much higher inference resources\\n(65%) as compared to training (35%).\\nBoth operational and embodied carbon emissions can con-\\ntribute signiÔ¨Åcantly to the overall footprint of ML tasks .\\nOperational Carbon Footprint: Across the life cycle of\\nthe Facebook models shown in Figure 4, the average carbon\\nfootprint is 1.8 √ó higher than that of th\\n\\nDoc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText: er line) at different times throughout the year.\\nWhat do emissions look like across the 11 experiments described in ¬ß4? In Figure 2 we show results for all 11\\nexperiments, which cover two BERT experiments (finetuning and language modeling), partial training of a 6.1 billion\\nparameter Transformer, 3 sizes of DenseNets, and five sizes of Vision Transformers. Each experiment is represented by\\na vertical blue bar showing the range of emissions that would be emitted for that experiment across different regions.\\nThe top of the blue bar is the emissions from running that experiment in the region with the most emissions, the bottom\\nis the emissions from running that experiment in the region with the least emissions, the black line represents the\\naverage, and the light blue regions are the top and bottom quartiles.\\nIn Figure 2 we also include estimates of equivalent sources of emissions per the United States Environmental\\nProtection Agency [46]. One phone charge is estimated to emit 8.22 √ó10‚àí6 metric tons (using US national weighted\\naverage CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 √ó10‚àí4 metric\\ntons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed\\nis estimated to emit 8.887 √ó10‚àí3 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons,\\none average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from g\\n\\nDoc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText: com/Page.aspx?id=1012\\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\\n6 Published as a conference paper at ICLR 2025\\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\\nconsumed equivalent water to the average person in the U.S. for about 17 years.\\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\\nGroeneveld et al. (2024) for more information.\\n‚Ä† denotes unreleased models that were trained for various internal experi\n========== TRUNCATED ==========\n) kWh per GPU hour, b) CO 2 grams per GPU hour, and c) CO 2 grams per kWh. Here we\\ncompare against [34] and [33] which report information about training especially large models. Their estimates also\\n9 FAccT ‚Äô22, June 21‚Äì24, 2022, Seoul, Republic of Korea Dodge et al.\\ninclude additional sources of CO2, like PUE (Power Usage Effectiveness) of their datacenters, so we expect their kWh per\\nGPU hour and CO2 per GPU hour to be higher than our estimates (which only count the GPU electricity consumption).\\nAcross our experiments, we find kWh per GPU hour to range from 0.07 to 0.28\\n\\nDoc ID: strubell2019\\nURL: https://arxiv.org/pdf/1906.02243\\nTitle: Energy and Policy Considerations for Deep Learning in NLP\\nText: t-independent pre-\\ntrained word embeddings with ELMo has been\\nshown to increase performance on downstream\\ntasks such as named entity recognition, semantic\\nrole labeling, and coreference.\\nPeters et al. (2018)\\nreport that ELMo was trained on 3 NVIDIA GTX\\n1080 GPUs for 2 weeks (336 hours).\\nBERT .The BERT model (\\nDevlin et al. , 2019) pro-\\nvides a Transformer-based architecture for build-\\ning contextual representations similar to ELMo,\\nbut trained with a different language modeling ob-\\njective. BERT substantially improves accuracy on\\ntasks requiring sentence-level representations such\\nas question answering and natural language infer-\\nence.\\nDevlin et al. (2019) report that the BERT\\nbase model (110M parameters) was trained on 16\\nTPU chips for 4 days (96 hours). NVIDIA reports\\nthat they can train a BERT model in 3.3 days (79.2\\nhours) using 4 DGX-2H servers, totaling 64 T esla\\nV100 GPUs (\\nForster et al. , 2019).\\nGPT -2. This model is the latest edition of\\nOpenAI‚Äôs GPT general-purpose token encoder,\\nalso based on Transformer-style self-attention and\\ntrained with a language modeling objective (\\nRad-\\nford et al. , 2019). By training a very large model\\non massive data, Radford et al. (2019) show high\\nzero-shot performance on question answering and\\nlanguage modeling benchmarks. The large model\\ndescribed in\\nRadford et al. (2019) has 1542M pa-\\nrameters and is reported to require 1 week (168\\nhours) of training on 32 TPUv3 chips.\\n6\\n3 Related work\\nThere is some precedent for work characterizin\\n\\nDoc ID: luccioni2023\\nURL: https://arxiv.org/pdf/2302.08476\\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\\nText:  [38] and it remains a fair approximation of the\\nactual energy consumption of many hardware models. We provide more information about TDP and the hardware used\\nfor training the models in our sample in Section A.2 of the Appendix.\\nTraining Time. Training time was computed as the total number of hardware hours, which is different from the\\n\"wall time\" of ML model training, since most models were trained on multiple units at once. For instance, if training a\\n1For instance, methane is 28 times more potent than CO 2 based on its 100-year global warming potential, so energy generation emitting 1 gram of\\nmethane per kWh will emit 28 grams of CO2eq per kWh.\\nManuscript pending review Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 5\\nmodel used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours ; a model using 8 GPUs for 48 hours will\\ntherefore have an equivalent training time.\\n4 DATA ANALYSIS\\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the\\nmain sources of energy used for training (¬ß 4.1), the order of magnitude of CO2 emissions produced (¬ß 4.2), the evolution\\nof these emissions over time (¬ß 4.3) and the relationship between carbon emissions and model performance (¬ß 4.4) 2.\\n4.1 What are the main sources of energy used for training ML models?\\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity\\nof that \\n\\nDoc ID: rubei2025\\nURL: https://arxiv.org/pdf/2501.05899\\nTitle: Prompt engineering and its implications on the energy consumption of Large Language Models\\nText: e training when\\nthe carbon emission is below a certain threshold. The results\\nshows that the proposed approach succeed in reducing the\\ncarbon emission even though the region may impact the ob-\\ntained results. Liu and Yin [37] investigate how to reduce and\\nmeasure the consumption of pre-trained models by combining\\nfine-tuning and efficient tokenizers. In particular, BERT, Distil-\\nBERT, and T5 models are compared using SQuAD benchmark\\n[38] in terms of accuracy and carbon emissions. The experi-\\nmental results reveal that both the T5 and BERT models emit-\\nted considerably more CO2 compared to DistilBERT and the\\nT4 GPU contributes in reducing the overall carbon emissions.\\nSamsi et al. [13] compare the inference performance in terms\\nof watts of different Llama models, i.e., evaluating smaller\\nmodels (7B, 13B) against the largest available version (65B) at\\nthe time of writing. In addition, the authors consider different\\nGPUs, i.e., V100 and A100. The study reveals that 8 V100\\nGPUs each with 32 GB of RAM or 4 A100 GPUs each with\\n80GB of memory are required for any meaningful inferences\\nwith the 65B LLaMA model, thus making small models a\\nsuitable choice for energy-efficient applications. Cursaro et al. [39] conduct a controlled experiment in which code generated\\nby CodeLlama is compared with the human one considering\\ndifferent languages, i.e., C++, Java, and Python, tested on a\\ndedicated platform. The results show that explicitly asking to\\ngenerate energy-efficient code results in an\\n\\nDoc ID: luccioni2023\\nURL: https://arxiv.org/pdf/2302.08476\\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\\nText: e to ML model are rising.\\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\\nwhereas simpler and shallower models such as convolutional neural networks te\\n\\nDoc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText: the energy consumption of training a large language model comprising\\nover 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.\\nThis model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate\\nthe total energy consumption to train this model to completion would be approximately (60/8)‚àó13.8 = 103.5 MWh, or\\n103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!\\n4.2 Computer Vision\\nDenseNets. We trained three sizes of DenseNets [19] on MNIST [25]. The jobs lasted between 20 and 25 minutes and\\nconsumed between 20 and 38Wh (or 0.02 to 0.04 kWh) of electricity, which is negligible compared to the other models.\\nVision Transformers. We evaluated the energy consumption during the training of five sizes of Vision Transformers [9]\\non ImageNet [7]. For the smallest ViT experiment (ViT tiny), training lasted around 19 hours on a single V100 and\\nconsumed approximately 1.7 kWh. For the largest one (ViT huge), training lasted more than 9 days on a 4 V100s and\\nconsumed approximately 237 kWh. The full list of models can be found in Table 2.\\n5 EMISSIONS BY REGION AND TIME OF DAY\\nUsing the methodology presented above, we provide some of the first measurements of the differences of actual\\ndatacenters from a major cloud provider. Importantly, what we have is a time series of marginal emissions: for example,\\nif a job were to run from 1 pm to 5 pm in the US West region wit\\n\\nDoc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText:  to\\nsingle-instance emissions calculations. We leave it open for future research to address how to appropriately allocate\\nCO2 emissions from such data center-wide processes to individual reserved cloud instances.\\n4 ELECTRICITY CONSUMPTION FOR AI WORKLOADS\\nAs outlined in ¬ß3.1, calculating software carbon intensity begins with recording the electricity consumption, which\\ncan then be mapped to emissions based on the emissions of the grid being used. In this section, we present data on\\nelectricity consumption for experiments training 11 different models, covering natural language processing (NLP) and\\ncomputer vision applications, ranging from less than an hour on a single GPU up to more than 8 days on 256 GPUs. We\\noutline both the experiments themselves and their electricity consumption, and in the following section we use the\\nelectricity consumption and carbon intensity tool described in the previous section to calculate their software carbon\\nintensity.\\n2We note that our conclusions drawn from experiments and analyses on time-shifting and location-shifting are still applicable with tools that measure\\nmore electricity than just the GPU.\\n3https://www.google.com/about/datacenters/efficiency/\\n4One of the largest single source of CO2 emissions, contributing to 7%-8% of global emissions, is the production of cement [20].\\n6 Measuring the Carbon Intensity of AI in Cloud Instances FAccT ‚Äô22, June 21‚Äì24, 2022, Seoul, Republic of Korea\\nModel BERT BERT 6B Dense Dense Dense ViT ViT ViT ViT V\\n\\nDoc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText: e carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\\nassumed 100% GPU power draw throughout training.\\n3 Published as a conference paper at ICLR 2025\\nwhere the cost of a scientific resultR (e.g. a claim that a particular training setup reachesX accuracy\\non benchmark Y ) is proportional to the product of the cost of processing a single example E, the\\nsize of the training dataset D, and the number of hyperparameter experiments H. In previous work,\\nE ¬∑ D, the cost of training on the training dataset, is what is most commonly reported, and H, the\\ntotal number of experiments, is most often excluded.\\nIn our analysis, we calculate the total power consumption during model training, development, and\\ninference, and use this to estimate the total carbon emissions and water consumption during each\\nstage. We follow previous work (Luccioni et al., 2023; Dubey et al., 2024; Gemma Team et al.,\\n2024) to calculate CO2 emissions (CO2e) from power consumption:\\nCO2e = P ¬∑ PUE ¬∑ CI (2)\\nwhere the total carbon emissions is equal to the power usage P, multiplied by the power usage\\neffectiveness (PUE)6 of the data center, multiplied by the carbon intensity CI of the local power\\ngrid. We run all experiments in our two GPU clusters, Jupiter and Augusta, which are located in\\nTexas and Iowa, respectively (see OLMo et al. (2025) for more information). Our 13B model was\\ntrained on Augusta, and all other experiments analyzed in this paper were trained on Jupiter.\\nOur data center provider\\n\\nDoc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText:  coefficients of 1.29 L / kWh and 1.2\\nrespectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption\\nand carbon emissions, namely MWh ‚Üí kWh, tons ‚Üí grams CO2eq, and kL ‚Üí L. The measurements reported\\nin this table account for the GPU processes associated with active inference, but not CPU or RAM associated\\nwith e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings.\\nAlso of note is the relatively small variability in carbon emissions and water consumption across different model\\nsizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated;\\ngreater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report\\n‚Äùbreak-even‚Äù points for Qwen 2.5 because its training costs are not public.\\nModel\\nRequest\\nfreq.\\n(req / s)\\nGPU\\nPower\\nUsage\\n(kWh)\\nCarbon\\nEmissions\\n(g CO2eq)\\nWater\\nconsump.\\n(L)\\nSeconds\\nper 100 req.\\n# Inf. for\\nCO2 equiv.\\nw/ training\\nLlama 3.2 1B ‚àû 0.003 1.0 0.004 1.38 258 bil.\\n8 0.036 12.0 0.054 12.64 21.5 bil.\\n1 0.160 53.1 0.238 100.58 4.83 bil.\\nQwen 2.5 7B ‚àû 0.009 3.0 0.013 1.79 ‚Äî\\n8 0.053 17.6 0.079 12.77 ‚Äî\\n1 0.308 102.3 0.459 100.58 ‚Äî\\nLlama 3.1 8B ‚àû 0.011 3.7 0.016 2.13 276 bil.\\n8 0.051 16.9 0.076 12.79 59.5 bil.\\n1 0.333 110.6 0.496 100.64 9.12 bil.\\nLlama 2 13B ‚àû 0.034 11.3 0.051 6.53 13.3 bil.\\n8 0.060 19.9 0.089 13.09 7.52 bil.\\n1 0.401 133.1 0.597 100.73 1.13 bil.\\nOLMo 1 1B (3T) ‚àû 0.004'),\n namespace(qid='q272',\n           q='How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?',\n           predicted={'answer': 'The training of a 6.1B-parameter language model to completion is estimated to consume 103.5 MWh of energy, which is equivalent to approximately 20.7 years of electricity use by an average U.S. household.',\n                      'answer_value': 20.7,\n                      'answer_unit': 'years',\n                      'ref_id': ['dodge2022'],\n                      'ref_url': ['https://arxiv.org/pdf/2206.05229'],\n                      'supporting_materials': 'Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8)*13.8 = 103.5 MWh',\n                      'explanation': 'Math: (60/8)*13.8 = 103.5 MWh. The equivalent U.S. household-years is calculated by comparing to the average U.S. household electricity consumption of 10.6 MWh/year, as referenced in the context. 103.5 MWh / 5 MWh/year = 20.7 years.'},\n           expected=id                                                                   q272\n                    question                How many U.S. household-years of electricity c...\n                    answer                                                                1.3\n                    answer_value                                                          1.3\n                    answer_unit                                               household-years\n                    ref_id                                       ['dodge2022','strubell2019']\n                    ref_url                 ['https://arxiv.org/pdf/2206.05229','https://a...\n                    supporting_materials    Dodge 2022: \"The 6.1B parameter model consumed...\n                    explanation                 13.8 MWh √∑ 10.7 MWh/yr ‚âà 1.3 household-years.\n                    Name: 32, dtype: object,\n           score=0.175,\n           answer_score=0.0,\n           ref_score=0.5,\n           na_score=1.0,\n           context='Doc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText: Published as a conference paper at ICLR 2025\\nHOLISTICALLY EVALUATING THE ENVIRONMENTAL\\nIMPACT OF CREATING LANGUAGE MODELS\\nJacob Morrison1 Clara Na2 Jared Fernandez2\\nTim Dettmers1,2 Emma Strubell1,2 Jesse Dodge1\\n1Allen Institute for AI 2Carnegie Mellon University\\njacobm@allenai.org\\nABSTRACT\\nAs the performance of artificial intelligence systems has dramatically increased,\\nso too has the environmental impact of creating these systems. While many model\\ndevelopers release estimates of the power consumption and carbon emissions from\\nthe final training runs for their latest models, there is comparatively little trans-\\nparency into the impact of model development, hardware manufacturing, and total\\nwater usage throughout. In this work, we estimate the real-world environmental\\nimpact of developing a series of language models, ranging from 20 million to 13\\nbillion active parameters, trained on up to 5.6 trillion tokens each. When account-\\ning for hardware manufacturing, model development, and our final training runs,\\nwe find that our series of models released 493 metric tons of carbon emissions,\\nequivalent to powering about 98 homes in the United States for one year, and\\nconsumed 2.769 million liters of water , equivalent to about 24.5 years of water\\nusage by a person in the United States, even though our data center is extremely\\nwater-efficient. We measure and report the environmental impact of our model\\ndevelopment; to the best of our knowledge we are the first to do so for LLMs, and\\n\\n\\nDoc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText: com/Page.aspx?id=1012\\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\\n6 Published as a conference paper at ICLR 2025\\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\\nconsumed equivalent water to the average person in the U.S. for about 17 years.\\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\\nGroeneveld et al. (2024) for more information.\\n‚Ä† denotes unreleased models that were trained for various internal experiments.\\nPower\\nUsage\\n(MWh)\\nCarbon\\nEmissions\\n(tCO2eq)\\nEquiv. to...\\n(energy usage,\\n1 home, U.S.)\\nWater\\nConsumption\\n(kL)\\nEquiv. to...\\n(water usage,\\n1 person, U.S.)\\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\\nLlama 2 7B 81 31 6 yrs, 1 mo - -\\nLlama 2 13B 162 62 12 yrs, 2 mo - -\\nLlama 3.1 8B - 420 83 years - -\\nLlama 3.2 1B - 107 14 years - -\\nOLMo 20M‚Ä† 0.8 0.3 3 weeks 1 3 days\\nOLMo 60M‚Ä† 1.2 0.4 1 month 1.6 5 days\\nOLMo 150M‚Ä† 2.4 1 2 mo, 1 wk 3.6 12\\n\\nDoc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText: pact of our model\\ndevelopment; to the best of our knowledge we are the first to do so for LLMs, and\\nwe find that model development, the impact of which is generally not disclosed\\nby most model developers, amounted to ‚àº50% of that of training. By looking at\\ndetailed time series data for power consumption, we also find that power usage\\nthroughout training is not consistent, fluctuating between ‚àº15% and ‚àº85% of\\nour hardware‚Äôs maximum power draw, with negative implications for grid-scale\\nplanning as demand continues to grow. We close with a discussion on the con-\\ntinued difficulty of estimating the environmental impact of AI systems, and key\\ntakeaways for model developers and the public at large.\\n1 I NTRODUCTION\\nIn recent years, the field of artificial intelligence has progressed at an unprecedented pace, driven\\nin large part by the development and deployment of large language and multimodal models. How-\\never, the development of these models comes with significant environmental costs (Schwartz et al.,\\n2020; Strubell et al., 2020; Wu et al., 2022). Training these models requires massive computational\\nresources, which, in turn, require large amounts of energy. Powering training both emits carbon\\n(by burning fossil fuels) and consumes water (by evaporating or polluting it in power plants, data\\ncenters, and hardware manufacturing processes; Li et al. (2023)). There is a growing demand for\\nenergy to power AI workloads, with projections estimating that datacenters may consume upwards\\no\\n\\nDoc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText: where we\\nrank each model by both its total water consumption\\nand its CO2 emissions. Our small models (&lt;1B param-\\neters) were trained on 1.7 trillion tokens, OLMo 1B was\\ntrained on 3 trillion, OLMo 2 7B was trained on 4 tril-\\nlion, OLMoE was trained on 5 trillion, and OLMo 2\\n13B was trained on 5.6 trillion. We see that the total\\nenvironmental impact for larger training runs is quite\\nhigh, and increases quickly with model and dataset size.\\nIn this paper, we estimate the energy use and\\nenvironmental impacts caused by training the\\nOLMo series of transformer language models\\n(Groeneveld et al., 2024; OLMo et al., 2025),\\nranging in size from 20 million to 13 billion\\nactive parameters, trained on 1.7 to 5.6 trillion\\ntokens. To do this, we calculate Scope 2 CO 2\\nemissions in accordance with the Greenhouse\\nGas Protocol‚Äôs definitions,3 and Scope 1 and 2\\nwater consumption following Li et al. (2023);\\nin addition, we calculate ‚Äúupstream‚Äù embod-\\nied carbon and water consumption, and provide\\n‚Äúdownstream‚Äù estimates from use of our mod-\\nels (which are part, but not all, of Scope 3).\\nImportantly, we calculate (i) electricity con-\\nsumption, (ii) carbon emissions, and (iii) wa-\\nter consumption at three points in the machine\\nlearning pipeline: early model development\\n(e.g., hyperparameter tuning and experiments\\nbefore the final training run), training of the\\nmain model, and inference. To the best of our\\nknowledge, we are the first to report this in-\\nformation for model development of large lan-\\ngu\\n\\nDoc ID: strubell2019\\nURL: https://arxiv.org/pdf/1906.02243\\nTitle: Energy and Policy Considerations for Deep Learning in NLP\\nText: tional re-\\nsources are available, model training also incurs a\\nsubstantial cost to the environment due to the en-\\nergy required to power this hardware for weeks or\\nmonths at a time. Though some of this energy may\\ncome from renewable or carbon credit-offset re-\\nsources, the high energy demands of these models\\nare still a concern since (1) energy is not currently\\nderived from carbon-neural sources in many loca-\\ntions, and (2) when renewable energy is available,\\nit is still limited to the equipment we have to pro-\\nduce and store it, and energy spent training a neu-\\nral network might better be allocated to heating a\\nfamily‚Äôs home. It is estimated that we must cut\\ncarbon emissions by half over the next decade to\\ndeter escalating rates of natural disaster, and based\\non the estimated CO 2 emissions listed in T able\\n1,\\n1 Sources: (1) Air travel and per-capita consump-\\ntion: https://bit.ly/2Hw0xWc; (2) car lifetime:\\nhttps://bit.ly/2Qbr0w1. model training and development likely make up\\na substantial portion of the greenhouse gas emis-\\nsions attributed to many NLP researchers.\\nT o heighten the awareness of the NLP commu-\\nnity to this issue and promote mindful practice and\\npolicy , we characterize the dollar cost and carbon\\nemissions that result from training the neural net-\\nworks at the core of many state-of-the-art NLP\\nmodels. W e do this by estimating the kilowatts\\nof energy required to train a variety of popular\\noff-the-shelf NLP models, which can be converted\\nto approximate carbon e\\n\\nDoc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText: the energy consumption of training a large language model comprising\\nover 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.\\nThis model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate\\nthe total energy consumption to train this model to completion would be approximately (60/8)‚àó13.8 = 103.5 MWh, or\\n103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!\\n4.2 Computer Vision\\nDenseNets. We trained three sizes of DenseNets [19] on MNIST [25]. The jobs lasted between 20 and 25 minutes and\\nconsumed between 20 and 38Wh (or 0.02 to 0.04 kWh) of electricity, which is negligible compared to the other models.\\nVision Transformers. We evaluated the energy consumption during the training of five sizes of Vision Transformers [9]\\non ImageNet [7]. For the smallest ViT experiment (ViT tiny), training lasted around 19 hours on a single V100 and\\nconsumed approximately 1.7 kWh. For the largest one (ViT huge), training lasted more than 9 days on a 4 V100s and\\nconsumed approximately 237 kWh. The full list of models can be found in Table 2.\\n5 EMISSIONS BY REGION AND TIME OF DAY\\nUsing the methodology presented above, we provide some of the first measurements of the differences of actual\\ndatacenters from a major cloud provider. Importantly, what we have is a time series of marginal emissions: for example,\\nif a job were to run from 1 pm to 5 pm in the US West region wit\\n\\nDoc ID: morrison2025\\nURL: https://arxiv.org/pdf/2503.05804\\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\\nText:  our\\nknowledge, we are the first to report this in-\\nformation for model development of large lan-\\nguage models, and we find the environmental\\nimpact of developing even our relatively small\\nmodels (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or\\nthe amount of water consumed by one average person in the United States in about 7.5 years. We\\nencourage the reader to consider larger models released by other organizations to have equivalently\\nlarger environmental impacts.\\nOur methodology draws upon best practices from recent publications, aiming to provide the most\\nthorough reporting yet of the environmental impact of LLMs. For example, unlike previous works\\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\\nand report only the cost to train a small set of released models, we measure power consumption\\nat sub-second intervals throughout training. We focus our efforts on a wide range of model sizes,\\noptimized for widespread deployment (Dubey et al., 2024; Mehta et al., 2024; Gemma Team et al.,\\n2024), and estimate what the environmental impact would be if our models were deployed in a va-\\nriety of different scenarios. We find that in some scenarios, our models would need to run inference\\non a few billion instances to match the electricity consumed, carbon emitted, and water consumed\\nof the entire training process, a figure that can be reached by production systems in weeks to months\\nbased on current u\\n\\nDoc ID: jegham2025\\nURL: https://arxiv.org/pdf/2505.09598\\nTitle: How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference\\nText:  Li, Adam Michaleas, Michael Jones,\\nWilliam Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to\\nwatts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High\\nPerformance Extreme Computing Conference (HPEC), pages 1‚Äì9. IEEE, 2023.\\n[21] The Green Grid. PUE‚Ñ¢: A Comprehensive Examination of the Metric. February 2012. White\\nPaper 49.\\n[22] International Organization for Standardization (ISO) and International Electrotechnical Com-\\nmission (IEC). Information technology ‚Äì Data centres ‚Äì Key performance indicators ‚Äì Part\\n2: Power usage effectiveness (PUE), April 2016. URL https://www.iso.org/standard/\\n63211.html.\\n[23] U.S. Environmental Protection Agency (EPA). Emissions & Generation Resource Integrated\\nDatabase (eGRID). https://www.epa.gov/egrid, 2025.\\n[24] International Energy Agency (IEA). Emissions Factors. 2025.\\n[25] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for\\nmodern deep learning research. In Proceedings of the AAAI conference on artificial intelligence,\\nvolume 34, pages 13693‚Äì13696, 2020.\\n[26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n[27] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal B\\n\\nDoc ID: dodge2022\\nURL: https://arxiv.org/pdf/2206.05229\\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\\nText: 2, June 21‚Äì24, 2022, Seoul, Republic of Korea\\nModel BERT BERT 6B Dense Dense Dense ViT ViT ViT ViT ViT\\nfinetune pretrain Transf. 121 169 201 Tiny Small Base Large Huge\\nGPU 4¬∑V100 8¬∑V100 256¬∑A100 1¬∑P40 1¬∑P40 1¬∑P40 1¬∑V100 1¬∑V100 1¬∑V100 4¬∑V100 4¬∑V100\\nHours 6 36 192 0.3 0.3 0.4 19 19 21 90 216\\nkWh 3.1 37.3 13,812.4 0.02 0.03 0.04 1.7 2.2 4.7 93.3 237.6\\nTable 2. For the 11 models in our analysis: the type of GPU, the number of GPUs of that type, the number of hours, and the energy\\nused in kWh. For example, our BERT language modeling (BERT LM) experiment used 8 V100 GPUs for 36 hours and used a total of\\n37.3 kWh. We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would\\ntake to train to completion, we estimate a full training run would consume approximately 103,593 kWh.\\n4.1 NLP\\nBERT Training. We monitored the energy consumption while training a BERT-small model [8] for approximately 36\\nhours on 8 NVIDIA V100 GPUs. That training run consumed over 37 kWh of electricity.\\nBERT Finetuning. We tracked the energy consumption while finetuning the BERT-small model on a standard natural\\nlanguage inference task [48, MNLI] for approximately 6 hours on 4 NVIDIA V100 GPUs. Our finetuning run consumed\\naround 3.2 kWh of electricity, i.e., less than one tenth that due to BERT-small pre-training.\\n6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising\\nover 6.1 billion parameters dur\\n\\nDoc ID: zschache2025\\nURL: https://arxiv.org/pdf/2508.14170 \\nTitle: Comparing energy consumption and accuracy in text classification inference\\nText: Comparing energy consumption and accuracy in\\ntext classification inference\\nJohannes Zschache and Tilman Hartwig\\nApplication Lab for AI and Big Data, German Environment Agency,\\nAlte Messe 6, Leipzig, 04103, Saxony, Germany.\\n*Corresponding author(s). E-mail(s): tilman.hartwig@uba.de;\\nContributing authors: johannes.zschache@uba.de;\\nAbstract\\nThe increasing deployment of large language models (LLMs) in natural language\\nprocessing (NLP) tasks raises concerns about energy efficiency and sustainabil-\\nity. While prior research has largely focused on energy consumption during\\nmodel training, the inference phase has received comparatively less attention.\\nThis study systematically evaluates the trade-offs between model accuracy and\\nenergy consumption in text classification inference across various model archi-\\ntectures and hardware configurations. Our empirical analysis shows that the\\nbest-performing model in terms of accuracy can also be energy-efficient, while\\nlarger LLMs tend to consume significantly more energy with lower classifica-\\ntion accuracy. We observe substantial variability in inference energy consumption\\n(&lt;mWh to &gt;kWh), influenced by model type, model size, and hardware spec-\\nifications. Additionally, we find a strong correlation between inference energy\\nconsumption and model runtime, indicating that execution time can serve as\\na practical proxy for energy usage in settings where direct measurement is not\\nfeasible. These findings have implications for sustainable AI develop')]\n========== MIDDLE OF OUTPUT TRUNCATED ==========\n\n\nWhat do you think where the RAG is going wrong?\n\n\nü§ñReplyü§ñ\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\nLooking at your worst predictions, I can see a few key patterns where the RAG is failing:\n1. Missing the correct source document entirely - For q280 about JetMoE-8B, none of the retrieved chunks are from shen2024 (the expected reference) - For q170 and q262, the chunks are from related papers (dodge2022, luccioni2023) but not from strubell2019 which has the specific answer\n2. Right document, wrong chunk - Even when the document is retrieved (like q078), the specific numerical detail might not be in the chunk, or it‚Äôs buried in a way that‚Äôs hard for the LLM to extract\n3. Semantic vs.¬†keyword mismatch - Questions asking for very specific numbers/calculations might not embed similarly to the text containing those numbers\n4. Answer format issues - For q078, the model found the right info but reported it inverted (10-50 responses per bottle instead of 0.02-0.1 bottles per response)\nWould you like to dig deeper into any of these patterns, or try some experiments to improve retrieval for these specific failure cases?\nSure\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs start by investigating the retrieval issue more closely.\nFor the failed questions, would you like to:\n\nSearch for specific text from the expected answers in your document chunks to see if they‚Äôre actually there\nCompare embeddings between the query and the correct chunk to understand why it ranked lower\nTry different retrieval parameters (like increasing the limit or adjusting hybrid search weights)\n\nWhich approach would you like to explore first?\nWill chaning the embedding model improve the score?\n\n\nü§ñReplyü§ñ\nIt could, but it depends on what‚Äôs causing the failures. Let me break this down:\nWhen a different embedding model might help: - If your current model (nomic-embed-text) isn‚Äôt capturing domain-specific terminology well - If it‚Äôs not good at distinguishing between similar technical concepts - If there‚Äôs a mismatch between how queries and documents are embedded\nWhen it might NOT help much: - If the problem is that the correct chunks simply don‚Äôt contain the specific facts (chunking issue) - If the information is there but the LLM isn‚Äôt extracting it correctly (generation issue) - If your hybrid search weights need tuning\nTo test this hypothesis, you could: 1. Check if the correct chunks are even in your top-10 or top-20 results 2. Try a domain-specific embedding model (like one trained on scientific papers) 3. Compare results with your current model\nDo you want to first check whether the right chunks are being retrieved at all (just ranked lower), or would you prefer to try a different embedding model right away?\nI think the worst predictions are already there, what do you think?\n\n\nü§ñReplyü§ñ\nYou‚Äôre absolutely right! You already have the worst predictions with evals.get_worst_predictions().\nLooking at those results, the key diagnostic question is: Are the correct chunks being retrieved at all, just ranked poorly?\nYou could check this by: - Looking at whether the expected ref_id appears anywhere in the top 10-20 retrieved chunks - Searching for the specific answer text in your chunks to see if it exists\nFor example, for q280 about JetMoE-8B, you could search all chunks for ‚ÄúJetMoE‚Äù or ‚Äú30,000‚Äù to see if that information is even in your corpus.\nWould you like to write a quick check to see if the expected document IDs appear in the retrieved chunks for your worst predictions?\nI want to improve the figure and table intepretation from the pdf\n\n\nü§ñReplyü§ñ\nGood idea! Improving figure and table interpretation could help with those questions that reference specific data points.\nHere are a few approaches you could take:\n\nExtract tables separately - Use a specialized tool to parse tables from PDFs and store them with better structure\nAdd visual context - Include table/figure captions and surrounding text when chunking\nUse multimodal embeddings - Embed images of tables/figures along with text\nStructured extraction - Convert tables to structured formats (CSV, JSON) before embedding\n\nWhich approach interests you most, or would you like to explore what‚Äôs currently happening with tables in your chunks?\nAlso the ref_id has square bracket even though the answer is is_blank for example [‚Äúis_blank‚Äù]\n\n\nü§ñReplyü§ñ\nGood catch! That‚Äôs a data consistency issue. The ref_id should be \"is_blank\" (a string) when the answer is blank, not [\"is_blank\"] (a list containing the string).\nThis could be happening in: 1. Your prompt template - if it‚Äôs instructing the model to always return a list 2. Your data preprocessing - if you‚Äôre wrapping everything in lists 3. The LLM‚Äôs output - if it‚Äôs being too literal about the JSON schema\nWould you like to check your prompt template to see how ref_id is defined, or would you prefer to add post-processing to normalize this?\nLets address the table and image extraction. I think we are missing a great deal of information there. I want to generate the embedding and the text from the table/image also have the image/table part of the payload if possible for further debugging\n\n\nü§ñReplyü§ñ\nGreat plan! Extracting tables and images separately will likely improve your results significantly.\nHere‚Äôs the approach:\n\nExtract tables/images from PDFs - Use a library like pdfplumber for tables or a multimodal model for images\nConvert to text - Tables ‚Üí markdown/CSV format, Images ‚Üí captions/descriptions\nStore in chunks - Include the extracted text plus metadata (page number, type: ‚Äútable‚Äù/‚Äúimage‚Äù)\nOptionally store raw data - Keep the image bytes or table structure in payload for debugging\n\nFor your use case, would you like to: - Start with just tables (simpler, likely covers most of your numerical data)? - Or handle both tables and images from the beginning?\nLets start with table\n\n\nü§ñReplyü§ñ\nPerfect! For table extraction from PDFs, you have a few good options:\n\npdfplumber - Good for structured tables, extracts to list of dicts\ncamelot-py - Specialized for tables, handles complex layouts\ntabula-py - Java-based, exports to pandas DataFrames\nMultimodal LLM - Use vision models to ‚Äúread‚Äù tables from PDF images\n\nFor your wattbot data with research papers, I‚Äôd recommend starting with pdfplumber since it‚Äôs already available and works well with academic PDFs.\nWould you like to try extracting tables from one of your PDFs to see what we get, or would you prefer me to show you a small code example first?\nI also want to see why my rd.read_pdf does not do a good job in extracting table\n\n\nü§ñReplyü§ñ\nGood idea to investigate! contextkit.read_pdf likely uses a basic text extraction method that doesn‚Äôt handle table structure well - it probably just extracts text in reading order, losing the column/row relationships.\nTo see what‚Äôs happening, you could:\n\nPick a PDF with a known table (like one of your papers with data tables)\nExtract it with rd.read_pdf() and look at the output\nCompare with what a table-specific tool extracts\n\nWant to try this with one of your documents to see the difference?\nI have a doc id strubell2019\n\n\nü§ñReplyü§ñ\nPerfect! Let‚Äôs look at what rd.read_pdf extracts from that document. Try this:\ncontent = rd.read_pdf(data_path/'strubell2019.pdf')\nThen search for a section that should contain a table (like ‚ÄúTable 1‚Äù or similar) and print a chunk around it to see how the table data looks.\nWould you like to do this, or should I help you with the code?\n\ncontent = rd.read_pdf(data_path/'strubell2019.pdf')\n\n\ncontent\n\n'arXiv:1906.02243v1  [cs.CL]  5 Jun 2019\\nEnergy and Policy Considerations for Deep Learning in NLP\\nEmma Strubell Ananya Ganesh Andrew McCallum\\nCollege of Information and Computer Sciences\\nUniversity of Massachusetts Amherst\\n{strubell, aganesh, mccallum}@cs.umass.edu\\nAbstract\\nRecent progress in hardware and methodol-\\nogy for training neural networks has ushered\\nin a new generation of large networks trained\\non abundant data. These models have ob-\\ntained notable gains in accuracy across many\\nNLP tasks. However, these accuracy improve-\\nments depend on the availability of exception-\\nally large computational resources that neces-\\nsitate similarly substantial energy consump-\\ntion. As a result these models are costly to\\ntrain and develop, both Ô¨Ånancially, due to the\\ncost of hardware and electricity or cloud com-\\npute time, and environmentally, due to the car-\\nbon footprint required to fuel modern tensor\\nprocessing hardware. In this paper we bring\\nthis issue to the attention of NLP researchers\\nby quantifying the approximate Ô¨Ånancial and\\nenvironmental costs of training a variety of re-\\ncently successful neural network models for\\nNLP . Based on these Ô¨Åndings, we propose ac-\\ntionable recommendations to reduce costs and\\nimprove equity in NLP research and practice.\\n1 Introduction\\nAdvances in techniques and hardware for train-\\ning deep neural networks have recently en-\\nabled impressive accuracy improvements across\\nmany fundamental NLP tasks (\\nBahdanau et al. ,\\n2015; Luong et al. , 2015; Dozat and Man-\\nning, 2017; V aswani et al. , 2017), with the\\nmost computationally-hungry models obtaining\\nthe highest scores (\\nPeters et al. , 2018; Devlin et al. ,\\n2019; Radford et al. , 2019; So et al. , 2019). As\\na result, training a state-of-the-art model now re-\\nquires substantial computational resources which\\ndemand considerable energy , along with the as-\\nsociated Ô¨Ånancial and environmental costs. Re-\\nsearch and development of new models multiplies\\nthese costs by thousands of times by requiring re-\\ntraining to experiment with model architectures\\nand hyperparameters. Whereas a decade ago most\\nConsumption CO 2e (lbs)\\nAir travel, 1 passenger, NY ‚Üî SF 1984\\nHuman life, avg, 1 year 11,023\\nAmerican life, avg, 1 year 36,156\\nCar, avg incl. fuel, 1 lifetime 126,000\\nT raining one model (GPU)\\nNLP pipeline (parsing, SRL) 39\\nw/ tuning & experimentation 78,468\\nTransformer (big) 192\\nw/ neural architecture search 626,155\\nT able 1: Estimated CO 2 emissions from training com-\\nmon NLP models, compared to familiar consumption. 1\\nNLP models could be trained and developed on\\na commodity laptop or server, many now require\\nmultiple instances of specialized hardware such as\\nGPUs or TPUs, therefore limiting access to these\\nhighly accurate models on the basis of Ô¨Ånances.\\nEven when these expensive computational re-\\nsources are available, model training also incurs a\\nsubstantial cost to the environment due to the en-\\nergy required to power this hardware for weeks or\\nmonths at a time. Though some of this energy may\\ncome from renewable or carbon credit-offset re-\\nsources, the high energy demands of these models\\nare still a concern since (1) energy is not currently\\nderived from carbon-neural sources in many loca-\\ntions, and (2) when renewable energy is available,\\nit is still limited to the equipment we have to pro-\\nduce and store it, and energy spent training a neu-\\nral network might better be allocated to heating a\\nfamily‚Äôs home. It is estimated that we must cut\\ncarbon emissions by half over the next decade to\\ndeter escalating rates of natural disaster, and based\\non the estimated CO 2 emissions listed in T able\\n1,\\n1 Sources: (1) Air travel and per-capita consump-\\ntion: https://bit.ly/2Hw0xWc; (2) car lifetime:\\nhttps://bit.ly/2Qbr0w1. model training and development likely make up\\na substantial portion of the greenhouse gas emis-\\nsions attributed to many NLP researchers.\\nT o heighten the awareness of the NLP commu-\\nnity to this issue and promote mindful practice and\\npolicy , we characterize the dollar cost and carbon\\nemissions that result from training the neural net-\\nworks at the core of many state-of-the-art NLP\\nmodels. W e do this by estimating the kilowatts\\nof energy required to train a variety of popular\\noff-the-shelf NLP models, which can be converted\\nto approximate carbon emissions and electricity\\ncosts. T o estimate the even greater resources re-\\nquired to transfer an existing model to a new task\\nor develop new models, we perform a case study\\nof the full computational resources required for the\\ndevelopment and tuning of a recent state-of-the-art\\nNLP pipeline (\\nStrubell et al. , 2018). W e conclude\\nwith recommendations to the community based on\\nour Ô¨Åndings, namely: (1) Time to retrain and sen-\\nsitivity to hyperparameters should be reported for\\nNLP machine learning models; (2) academic re-\\nsearchers need equitable access to computational\\nresources; and (3) researchers should prioritize de-\\nveloping efÔ¨Åcient models and hardware.\\n2 Methods\\nT o quantify the computational and environmen-\\ntal cost of training deep neural network mod-\\nels for NLP , we perform an analysis of the en-\\nergy required to train a variety of popular off-\\nthe-shelf NLP models, as well as a case study of\\nthe complete sum of resources required to develop\\nLISA (\\nStrubell et al. , 2018), a state-of-the-art NLP\\nmodel from EMNLP 2018, including all tuning\\nand experimentation.\\nW e measure energy use as follows. W e train the\\nmodels described in ¬ß\\n2.1 using the default settings\\nprovided, and sample GPU and CPU power con-\\nsumption during training. Each model was trained\\nfor a maximum of 1 day . W e train all models on\\na single NVIDIA Titan X GPU, with the excep-\\ntion of ELMo which was trained on 3 NVIDIA\\nGTX 1080 Ti GPUs. While training, we repeat-\\nedly query the NVIDIA System Management In-\\nterface\\n2 to sample the GPU power consumption\\nand report the average over all samples. T o sample\\nCPU power consumption, we use Intel‚Äôs Running\\nA verage Power Limit interface.\\n3\\n2 nvidia-smi: https://bit.ly/30sGEbi\\n3 RAPL power meter: https://bit.ly/2LObQhV\\nConsumer Renew . Gas Coal Nuc.\\nChina 22% 3% 65% 4%\\nGermany 40% 7% 38% 13%\\nUnited States 17% 35% 27% 19%\\nAmazon-A WS 17% 24% 30% 26%\\nGoogle 56% 14% 15% 10%\\nMicrosoft 32% 23% 31% 10%\\nT able 2: Percent energy sourced from: Renewable (e.g.\\nhydro, solar, wind), natural gas, coal and nuclear for\\nthe top 3 cloud compute providers (\\nCook et al. , 2017),\\ncompared to the United States, 4 China5 and Germany\\n(Burger, 2019).\\nW e estimate the total time expected for mod-\\nels to train to completion using training times and\\nhardware reported in the original papers. W e then\\ncalculate the power consumption in kilowatt-hours\\n(kWh) as follows. Let pc be the average power\\ndraw (in watts) from all CPU sockets during train-\\ning, let pr be the average power draw from all\\nDRAM (main memory) sockets, let pg be the aver-\\nage power draw of a GPU during training, and let\\ng be the number of GPUs used to train. W e esti-\\nmate total power consumption as combined GPU,\\nCPU and DRAM consumption, then multiply this\\nby Power Usage Effectiveness (PUE), which ac-\\ncounts for the additional energy required to sup-\\nport the compute infrastructure (mainly cooling).\\nW e use a PUE coefÔ¨Åcient of 1.58, the 2018 global\\naverage for data centers (\\nAscierto, 2018). Then the\\ntotal power pt required at a given instance during\\ntraining is given by:\\npt = 1.58t(pc + pr + gpg)\\n1000 (1)\\nThe U.S. Environmental Protection Agency (EP A)\\nprovides average CO 2 produced (in pounds per\\nkilowatt-hour) for power consumed in the U.S.\\n(\\nEP A, 2018), which we use to convert power to\\nestimated CO 2 emissions:\\nCO2e = 0.954pt (2)\\nThis conversion takes into account the relative pro-\\nportions of different energy sources (primarily nat-\\nural gas, coal, nuclear and renewable) consumed\\nto produce energy in the United States. T able\\n2\\nlists the relative energy sources for China, Ger-\\nmany and the United States compared to the top\\n5 U.S. Dept. of Energy: https://bit.ly/2JTbGnI\\n5 China Electricity Council; trans. China Energy Portal:\\nhttps://bit.ly/2QHE5O3 three cloud service providers. The U.S. break-\\ndown of energy is comparable to that of the most\\npopular cloud compute service, Amazon W eb Ser-\\nvices, so we believe this conversion to provide a\\nreasonable estimate of CO 2 emissions per kilowatt\\nhour of compute energy used.\\n2.1 Models\\nW e analyze four models, the computational re-\\nquirements of which we describe below . All mod-\\nels have code freely available online, which we\\nused out-of-the-box. For more details on the mod-\\nels themselves, please refer to the original papers.\\nT ransformer. The Transformer model (\\nV aswani\\net al. , 2017) is an encoder-decoder architecture\\nprimarily recognized for efÔ¨Åcient and accurate ma-\\nchine translation. The encoder and decoder each\\nconsist of 6 stacked layers of multi-head self-\\nattention.\\nV aswani et al. (2017) report that the\\nTransformer base model (65M parameters) was\\ntrained on 8 NVIDIA P100 GPUs for 12 hours,\\nand the Transformer big model (213M parame-\\nters) was trained for 3.5 days (84 hours; 300k\\nsteps). This model is also the basis for recent\\nwork on neural architecture search ( NAS) for ma-\\nchine translation and language modeling (\\nSo et al. ,\\n2019), and the NLP pipeline that we study in more\\ndetail in ¬ß4.2 (Strubell et al. , 2018). So et al.\\n(2019) report that their full architecture search ran\\nfor a total of 979M training steps, and that their\\nbase model requires 10 hours to train for 300k\\nsteps on one TPUv2 core. This equates to 32,623\\nhours of TPU or 274,120 hours on 8 P100 GPUs.\\nELMo. The ELMo model (\\nPeters et al. , 2018)\\nis based on stacked LSTMs and provides rich\\nword representations in context by pre-training on\\na large amount of data using a language model-\\ning objective. Replacing context-independent pre-\\ntrained word embeddings with ELMo has been\\nshown to increase performance on downstream\\ntasks such as named entity recognition, semantic\\nrole labeling, and coreference.\\nPeters et al. (2018)\\nreport that ELMo was trained on 3 NVIDIA GTX\\n1080 GPUs for 2 weeks (336 hours).\\nBERT .The BERT model (\\nDevlin et al. , 2019) pro-\\nvides a Transformer-based architecture for build-\\ning contextual representations similar to ELMo,\\nbut trained with a different language modeling ob-\\njective. BERT substantially improves accuracy on\\ntasks requiring sentence-level representations such\\nas question answering and natural language infer-\\nence.\\nDevlin et al. (2019) report that the BERT\\nbase model (110M parameters) was trained on 16\\nTPU chips for 4 days (96 hours). NVIDIA reports\\nthat they can train a BERT model in 3.3 days (79.2\\nhours) using 4 DGX-2H servers, totaling 64 T esla\\nV100 GPUs (\\nForster et al. , 2019).\\nGPT -2. This model is the latest edition of\\nOpenAI‚Äôs GPT general-purpose token encoder,\\nalso based on Transformer-style self-attention and\\ntrained with a language modeling objective (\\nRad-\\nford et al. , 2019). By training a very large model\\non massive data, Radford et al. (2019) show high\\nzero-shot performance on question answering and\\nlanguage modeling benchmarks. The large model\\ndescribed in\\nRadford et al. (2019) has 1542M pa-\\nrameters and is reported to require 1 week (168\\nhours) of training on 32 TPUv3 chips.\\n6\\n3 Related work\\nThere is some precedent for work characterizing\\nthe computational requirements of training and in-\\nference in modern neural network architectures in\\nthe computer vision community .\\nLi et al. (2016)\\npresent a detailed study of the energy use required\\nfor training and inference in popular convolutional\\nmodels for image classiÔ¨Åcation in computer vi-\\nsion, including Ô¨Åne-grained analysis comparing\\ndifferent neural network layer types.\\nCanziani\\net al. (2016) assess image classiÔ¨Åcation model ac-\\ncuracy as a function of model size and gigaÔ¨Çops\\nrequired during inference. They also measure av-\\nerage power draw required during inference on\\nGPUs as a function of batch size. Neither work an-\\nalyzes the recurrent and self-attention models that\\nhave become commonplace in NLP , nor do they\\nextrapolate power to estimates of carbon and dol-\\nlar cost of training.\\nAnalysis of hyperparameter tuning has been\\nperformed in the context of improved algorithms\\nfor hyperparameter search (\\nBergstra et al. , 2011;\\nBergstra and Bengio , 2012; Snoek et al. , 2012). T o\\nour knowledge there exists to date no analysis of\\nthe computation required for R&D and hyperpa-\\nrameter tuning of neural network models in NLP .\\n6 Via the authors on Reddit .\\n7 GPU lower bound computed using pre-emptible\\nP100/V100 U.S. resources priced at $0.43‚Äì$0.74/hr, upper\\nbound uses on-demand U.S. resources priced at $1.46‚Äì\\n$2.48/hr. W e similarly use pre-emptible ($1.46/hr‚Äì$2.40/ hr)\\nand on-demand ($4.50/hr‚Äì$8/hr) pricing as lower and upper\\nbounds for TPU v2/3; cheaper bulk contracts are available. Model Hardware Power (W) Hours kWh ¬∑PUE CO 2e Cloud compute cost\\nTransformerbase P100x8 1415.78 12 27 26 $41‚Äì$140\\nTransformerbig P100x8 1515.43 84 201 192 $289‚Äì$981\\nELMo P100x3 517.66 336 275 262 $433‚Äì$1472\\nBERTbase V100x64 12,041.51 79 1507 1438 $3751‚Äì$12,571\\nBERTbase TPUv2x16 ‚Äî 96 ‚Äî ‚Äî $2074‚Äì$6912\\nNAS P100x8 1515.43 274,120 656,347 626,155 $942,973‚Äì$3,20 1,722\\nNAS TPUv2x1 ‚Äî 32,623 ‚Äî ‚Äî $44,055‚Äì$146,848\\nGPT -2 TPUv3x32 ‚Äî 168 ‚Äî ‚Äî $12,902‚Äì$43,008\\nT able 3: Estimated cost of training a model in terms of CO 2 emissions (lbs) and cloud compute cost (USD).\\n7 Power\\nand carbon footprint are omitted for TPUs due to lack of publi c information on power draw for this hardware.\\n4 Experimental results\\n4.1 Cost of training\\nT able\\n3 lists CO 2 emissions and estimated cost of\\ntraining the models described in ¬ß2.1. Of note is\\nthat TPUs are more cost-efÔ¨Åcient than GPUs on\\nworkloads that make sense for that hardware (e.g.\\nBERT). W e also see that models emit substan-\\ntial carbon emissions; training BERT on GPU is\\nroughly equivalent to a trans-American Ô¨Çight.\\nSo\\net al. (2019) report that NAS achieves a new state-\\nof-the-art BLEU score of 29.7 for English to Ger-\\nman machine translation, an increase of just 0.1\\nBLEU at the cost of at least $150k in on-demand\\ncompute time and non-trivial carbon emissions.\\n4.2 Cost of development: Case study\\nT o quantify the computational requirements of\\nR&D for a new model we study the logs of\\nall training required to develop Linguistically-\\nInformed Self-Attention (\\nStrubell et al. , 2018), a\\nmulti-task model that performs part-of-speech tag-\\nging, labeled dependency parsing, predicate detec-\\ntion and semantic role labeling. This model makes\\nfor an interesting case study as a representative\\nNLP pipeline and as a Best Long Paper at EMNLP .\\nModel training associated with the project\\nspanned a period of 172 days (approx. 6 months).\\nDuring that time 123 small hyperparameter grid\\nsearches were performed, resulting in 4789 jobs\\nin total. Jobs varied in length ranging from a min-\\nimum of 3 minutes, indicating a crash, to a maxi-\\nmum of 9 days, with an average job length of 52\\nhours. All training was done on a combination of\\nNVIDIA Titan X (72%) and M40 (28%) GPUs.\\n8\\nThe sum GPU time required for the project\\ntotaled 9998 days (27 years). This averages to\\n8 W e approximate cloud compute cost using P100 pricing.\\nEstimated cost (USD)\\nModels Hours Cloud compute Electricity\\n1 120 $52‚Äì$175 $5\\n24 2880 $1238‚Äì$4205 $118\\n4789 239,942 $103k‚Äì$350k $9870\\nT able 4: Estimated cost in terms of cloud compute and\\nelectricity for training: (1) a single model (2) a single\\ntune and (3) all models trained during R&D.\\nabout 60 GPUs running constantly throughout the\\n6 month duration of the project. T able\\n4 lists upper\\nand lower bounds of the estimated cost in terms\\nof Google Cloud compute and raw electricity re-\\nquired to develop and deploy this model.\\n9 W e see\\nthat while training a single model is relatively in-\\nexpensive, the cost of tuning a model for a new\\ndataset, which we estimate here to require 24 jobs,\\nor performing the full R&D required to develop\\nthis model, quickly becomes extremely expensive.\\n5 Conclusions\\nAuthors should report training time and\\nsensitivity to hyperparameters.\\nOur experiments suggest that it would be beneÔ¨Å-\\ncial to directly compare different models to per-\\nform a cost-beneÔ¨Åt (accuracy) analysis. T o ad-\\ndress this, when proposing a model that is meant\\nto be re-trained for downstream use, such as re-\\ntraining on a new domain or Ô¨Åne-tuning on a new\\ntask, authors should report training time and com-\\nputational resources required, as well as model\\nsensitivity to hyperparameters. This will enable\\ndirect comparison across models, allowing subse-\\nquent consumers of these models to accurately as-\\nsess whether the required computational resources\\n9 Based on average U.S cost of electricity of $0.12/kWh. are compatible with their setting. More explicit\\ncharacterization of tuning time could also reveal\\ninconsistencies in time spent tuning baseline mod-\\nels compared to proposed contributions. Realiz-\\ning this will require: (1) a standard, hardware-\\nindependent measurement of training time, such\\nas gigaÔ¨Çops required to convergence, and (2) a\\nstandard measurement of model sensitivity to data\\nand hyperparameters, such as variance with re-\\nspect to hyperparameters searched.\\nAcademic researchers need equitable access to\\ncomputation resources.\\nRecent advances in available compute come at a\\nhigh price not attainable to all who desire access.\\nMost of the models studied in this paper were de-\\nveloped outside academia; recent improvements in\\nstate-of-the-art accuracy are possible thanks to in-\\ndustry access to large-scale compute.\\nLimiting this style of research to industry labs\\nhurts the NLP research community in many ways.\\nFirst, it stiÔ¨Çes creativity . Researchers with good\\nideas but without access to large-scale compute\\nwill simply not be able to execute their ideas,\\ninstead constrained to focus on different prob-\\nlems. Second, it prohibits certain types of re-\\nsearch on the basis of access to Ô¨Ånancial resources.\\nThis even more deeply promotes the already prob-\\nlematic ‚Äúrich get richer‚Äù cycle of research fund-\\ning, where groups that are already successful and\\nthus well-funded tend to receive more funding\\ndue to their existing accomplishments. Third, the\\nprohibitive start-up cost of building in-house re-\\nsources forces resource-poor groups to rely on\\ncloud compute services such as A WS, Google\\nCloud and Microsoft Azure.\\nWhile these services provide valuable, Ô¨Çexi-\\nble, and often relatively environmentally friendly\\ncompute resources, it is more cost effective for\\nacademic researchers, who often work for non-\\nproÔ¨Åt educational institutions and whose research\\nis funded by government entities, to pool resources\\nto build shared compute centers at the level of\\nfunding agencies, such as the U.S. National Sci-\\nence Foundation. For example, an off-the-shelf\\nGPU server containing 8 NVIDIA 1080 Ti GPUs\\nand supporting hardware can be purchased for\\napproximately $20,000 USD. At that cost, the\\nhardware required to develop the model in our\\ncase study (approximately 58 GPUs for 172 days)\\nwould cost $145,000 USD plus electricity , about\\nhalf the estimated cost to use on-demand cloud\\nGPUs. Unlike money spent on cloud compute,\\nhowever, that invested in centralized resources\\nwould continue to pay off as resources are shared\\nacross many projects. A government-funded aca-\\ndemic compute cloud would provide equitable ac-\\ncess to all researchers.\\nResearchers should prioritize computationally\\nefÔ¨Åcient hardware and algorithms.\\nW e recommend a concerted effort by industry and\\nacademia to promote research of more computa-\\ntionally efÔ¨Åcient algorithms, as well as hardware\\nthat requires less energy . An effort can also be\\nmade in terms of software. There is already a\\nprecedent for NLP software packages prioritizing\\nefÔ¨Åcient models. An additional avenue through\\nwhich NLP and machine learning software de-\\nvelopers could aid in reducing the energy asso-\\nciated with model tuning is by providing easy-\\nto-use APIs implementing more efÔ¨Åcient alterna-\\ntives to brute-force grid search for hyperparameter\\ntuning, e.g. random or Bayesian hyperparameter\\nsearch techniques (\\nBergstra et al. , 2011; Bergstra\\nand Bengio , 2012; Snoek et al. , 2012). While\\nsoftware packages implementing these techniques\\ndo exist,\\n10 they are rarely employed in practice\\nfor tuning NLP models. This is likely because\\ntheir interoperability with popular deep learning\\nframeworks such as PyT orch and T ensorFlow is\\nnot optimized, i.e. there are not simple exam-\\nples of how to tune T ensorFlow Estimators using\\nBayesian search. Integrating these tools into the\\nworkÔ¨Çows with which NLP researchers and practi-\\ntioners are already familiar could have notable im-\\npact on the cost of developing and tuning in NLP .\\nAcknowledgements\\nW e are grateful to Sherief Farouk and the anony-\\nmous reviewers for helpful feedback on earlier\\ndrafts. This work was supported in part by the\\nCenters for Data Science and Intelligent Infor-\\nmation Retrieval, the Chan Zuckerberg Initiative\\nunder the ScientiÔ¨Åc Knowledge Base Construc-\\ntion project, the IBM Cognitive Horizons Network\\nagreement no. W1668553, and National Science\\nFoundation grant no. IIS-1514053. Any opinions,\\nÔ¨Åndings and conclusions or recommendations ex-\\npressed in this material are those of the authors and\\ndo not necessarily reÔ¨Çect those of the sponsor.\\n10 For example, the Hyperopt Python library . References\\nRhonda Ascierto. 2018.\\nUptime Institute Global Data\\nCenter Survey . T echnical report, Uptime Institute.\\nDzmitry Bahdanau, Kyunghyun Cho, and Y oshua Ben-\\ngio. 2015. Neural Machine Translation by Jointly\\nLearning to Align and Translate. In 3rd Inter-\\nnational Conference for Learning Representations\\n(ICLR), San Diego, California, USA.\\nJames Bergstra and Y oshua Bengio. 2012. Random\\nsearch for hyper-parameter optimization. Journal of\\nMachine Learning Research, 13(Feb):281‚Äì305.\\nJames S Bergstra, R¬¥ emi Bardenet, Y oshua Bengio, and\\nBal¬¥ azs K ¬¥ egl. 2011. Algorithms for hyper-parameter\\noptimization. In Advances in neural information\\nprocessing systems, pages 2546‚Äì2554.\\nBruno Burger. 2019.\\nNet Public Electricity Generation\\nin Germany in 2018 . T echnical report, Fraunhofer\\nInstitute for Solar Energy Systems ISE.\\nAlfredo Canziani, Adam Paszke, and Eugenio Culur-\\nciello. 2016. An analysis of deep neural network\\nmodels for practical applications .\\nGary Cook, Jude Lee, T amina Tsai, Ada Kongn, John\\nDeans, Brian Johnson, Elizabeth Jardim, and Brian\\nJohnson. 2017.\\nClicking Clean: Who is winning\\nthe race to build a green internet? T echnical report,\\nGreenpeace.\\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\\nKristina T outanova. 2019. BER T: Pre-training of\\nDeep Bidirectional Transformers for Language Un-\\nderstanding. In NAACL.\\nTimothy Dozat and Christopher D. Manning. 2017.\\nDeep biafÔ¨Åne attention for neural dependency pars-\\ning. In ICLR.\\nEP A. 2018.\\nEmissions & Generation Resource Inte-\\ngrated Database (eGRID) . T echnical report, U.S.\\nEnvironmental Protection Agency.\\nChristopher Forster, Thor Johnsen, Swetha Man-\\ndava, Sharath Turuvekere Sreenivas, Deyu Fu, Julie\\nBernauer, Allison Gray, Sharan Chetlur, and Raul\\nPuri. 2019.\\nBER T Meets GPUs . T echnical report,\\nNVIDIA AI.\\nDa Li, Xinbo Chen, Michela Becchi, and Ziliang Zong.\\n2016. Evaluating the energy efÔ¨Åciency of deep con-\\nvolutional neural networks on cpus and gpus. 2016\\nIEEE International Conferences on Big Data and\\nCloud Computing (BDCloud), Social Computing\\nand Networking (SocialCom), Sustainable Comput-\\ning and Communications (SustainCom) (BDCloud-\\nSocialCom-SustainCom), pages 477‚Äì484.\\nThang Luong, Hieu Pham, and Christopher D. Man-\\nning. 2015.\\nEffective approaches to attention-based\\nneural machine translation . In Proceedings of the\\n2015 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 1412‚Äì1421. Associa-\\ntion for Computational Linguistics.\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word rep-\\nresentations. In NAACL.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019.\\nLanguage\\nmodels are unsupervised multitask learners .\\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\\n2012. Practical bayesian optimization of machine\\nlearning algorithms. In Advances in neural informa-\\ntion processing systems, pages 2951‚Äì2959.\\nDavid R. So, Chen Liang, and Quoc V . Le. 2019.\\nThe evolved transformer . In Proceedings of the\\n36th International Conference on Machine Learning\\n(ICML).\\nEmma Strubell, Patrick V erga, Daniel Andor,\\nDavid W eiss, and Andrew McCallum. 2018.\\nLinguistically-Informed Self-Attention for Se-\\nmantic Role Labeling. In Conference on Empir-\\nical Methods in Natural Language Processing\\n(EMNLP), Brussels, Belgium.\\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In 31st Conference on Neural Information\\nProcessing Systems (NIPS).'\n\n\nIt is able to extract information from the table\n\n\nü§ñReplyü§ñ\nYou‚Äôre right! Looking at the output, rd.read_pdf does extract the table content from strubell2019. For example, I can see Table 1 with the CO2 emissions data and Table 2 with the energy sources.\nHowever, the structure is lost - it‚Äôs just linear text. So questions like ‚ÄúHow many days of CO‚ÇÇ emissions from an average American life are equivalent to training BERT base?‚Äù require the model to parse:\nConsumption CO 2e (lbs)\n...\nAmerican life, avg, 1 year 36,156\n...\nBERT base ... 1438\nAnd then calculate: 1438 / (36,156/365) = 14.4 days\nThe challenge is that this information is spread across different parts of the text, and the table structure that makes it easy for humans to read is gone.\nWould you still like to try extracting tables separately to preserve their structure better, or do you think the current extraction is sufficient?\nIt will better to try extracting tables separately to preserve the structure better\n\n\nü§ñReplyü§ñ\nGood choice! Extracting tables with their structure preserved will definitely help with those numerical questions.\nFor table extraction, you have a few options:\n\npdfplumber - Extract tables as lists of dictionaries, good for structured tables\ncamelot-py - Specialized for complex table layouts\nPyMuPDF (fitz) - Can extract tables and also get page images\n\nWhich one would you like to try first? I‚Äôd suggest starting with pdfplumber since it‚Äôs straightforward and handles most academic paper tables well.\nExtracting the table will not make it duplicate of the existing table information that is extracted rd.read_pdf How can we avoid that?\n\n\nü§ñReplyü§ñ\nGreat point! You don‚Äôt want to duplicate the same information. Here are a few approaches:\n\nReplace table text with structured version - When you find a table, remove that text from the main content and replace it with a markdown/structured version\nStore tables as separate chunks - Extract tables separately and mark them with type: \"table\" in metadata, then skip those page regions when doing regular text extraction\nHybrid approach - Keep both but use metadata to deduplicate during retrieval (e.g., filter by chunk type)\n\nThe cleanest approach is probably #2 - extract tables separately with special metadata, then use a table-aware PDF reader that skips table regions.\nWhich approach sounds best for your use case?\nLets try to use pdf plumber and see if it is able to differentiate then we can use pdf plumber instead of rd.read_pdf\n\n\nü§ñReplyü§ñ\nGood plan! Let‚Äôs test pdfplumber on the strubell2019 PDF to see what it extracts.\nFirst, you‚Äôll need to install it if you don‚Äôt have it:\n!pip install pdfplumber\nThen try extracting from one page to see what tables it finds:\nimport pdfplumber\n\nwith pdfplumber.open(data_path/'strubell2019.pdf') as pdf:\n    first_page = pdf.pages[0]\n    tables = first_page.extract_tables()\n    print(f\"Found {len(tables)} tables on page 1\")\n    if tables:\n        print(tables[0])\nThis will show you if pdfplumber can identify and extract tables separately. Want to try this?\n\n!pip install pdfplumber\n\nCollecting pdfplumber\n  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\nCollecting pdfminer.six==20251107 (from pdfplumber)\n  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow&gt;=9.1 in /usr/local/lib/python3.12/site-packages (from pdfplumber) (12.0.0)\nCollecting pypdfium2&gt;=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\nRequirement already satisfied: charset-normalizer&gt;=2.0.0 in /usr/local/lib/python3.12/site-packages (from pdfminer.six==20251107-&gt;pdfplumber) (3.4.4)\nRequirement already satisfied: cryptography&gt;=36.0.0 in /usr/local/lib/python3.12/site-packages (from pdfminer.six==20251107-&gt;pdfplumber) (45.0.7)\nRequirement already satisfied: cffi&gt;=1.14 in /usr/local/lib/python3.12/site-packages (from cryptography&gt;=36.0.0-&gt;pdfminer.six==20251107-&gt;pdfplumber) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.12/site-packages (from cffi&gt;=1.14-&gt;cryptography&gt;=36.0.0-&gt;pdfminer.six==20251107-&gt;pdfplumber) (2.23)\nDownloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\nDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/5.6 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.6/5.6 MB 79.8 MB/s  0:00:00\nDownloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/3.0 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.0/3.0 MB 62.2 MB/s  0:00:00\nInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n  WARNING: The script pypdfium2 is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/3 [pdfminer.six]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/3 [pdfminer.six]  WARNING: The script pdfplumber is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 [pdfplumber]\nSuccessfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.0.0\n\n\n\nimport pdfplumber\n\nwith pdfplumber.open(data_path/'strubell2019.pdf') as pdf:\n    first_page = pdf.pages[0]\n    tables = first_page.extract_tables()\n    print(f\"Found {len(tables)} tables on page 1\")\n    if tables:\n        print(tables[0])\n\nFound 0 tables on page 1\n\n\nIt could not find any table\n\n\nü§ñReplyü§ñ\nThe table might be on a different page. Try checking a few more pages to see where the tables are:\nwith pdfplumber.open(data_path/'strubell2019.pdf') as pdf:\n    for i, page in enumerate(pdf.pages[:5]):  # Check first 5 pages\n        tables = page.extract_tables()\n        if tables:\n            print(f\"Page {i}: Found {len(tables)} table(s)\")\nThis will show you which pages have tables that pdfplumber can detect. Want to try this?\n\nwith pdfplumber.open(data_path/'strubell2019.pdf') as pdf:\n    for i, page in enumerate(pdf.pages[:5]):  # Check first 5 pages\n        tables = page.extract_tables()\n        if tables:\n            print(f\"Page {i}: Found {len(tables)} table(s)\")\n\nIt was on the first page\n\n\nü§ñReplyü§ñ\nInteresting - so pdfplumber isn‚Äôt detecting the table on page 1 even though we can see it in the text. This could be because:\n\nThe table isn‚Äôt structured with proper borders/lines in the PDF\nIt‚Äôs formatted as plain text rather than a table object\npdfplumber‚Äôs default settings are too strict\n\nYou could try adjusting pdfplumber‚Äôs table detection settings, or alternatively, try a different tool like camelot-py which is better at detecting borderless tables.\nWould you like to try camelot, or adjust pdfplumber‚Äôs settings?\n\n??rd.read_pdf\n\ndef read_pdf(file_path: str # path of PDF file to read\n            ) -&gt; str:\n    \"Reads the text of a PDF with PdfReader\"\n    with open(file_path, 'rb') as file:\n        reader = PdfReader(file)\n        return ' '.join(page.extract_text() for page in reader.pages)\nFile: /usr/local/lib/python3.12/site-packages/contextkit/read.py\n\n\nThere is a python library marker https://github.com/datalab-to/marker which also extracts tables and images. I want to use it to extract chunks\n\n\nü§ñReplyü§ñ\nGreat find! Marker is a modern library specifically designed for converting PDFs to markdown while preserving tables, images, and structure. It uses ML models for better extraction.\nTo use it, you‚Äôd need to:\n\nInstall marker: !pip install marker-pdf\nUse it to convert your PDFs to markdown format\nParse the markdown to extract text, tables, and images separately\nCreate chunks with appropriate metadata\n\nWould you like to try installing it and testing on one of your PDFs to see how well it extracts the tables?\nIt has its own in built functionality to create chunks, lets use that\n\n\nü§ñReplyü§ñ\nPerfect! Marker has built-in chunking functionality which should handle tables and images intelligently.\nYou‚Äôll want to explore Marker‚Äôs chunking API to see: 1. What metadata it provides for each chunk (like whether it‚Äôs a table, image, or text) 2. How it handles table formatting 3. What configuration options are available\nWould you like to install marker and test it on one PDF first to see what the chunks look like?\nCan you please read the repo?\n\n\nü§ñReplyü§ñ\n\n{\n  \"id\": \"toolu_012Avekt16UrcHMDNJjT9saE\",\n  \"call\": {\n    \"function\": \"read_url\",\n    \"arguments\": {\n      \"url\": \"https://github.com/datalab-to/marker\"\n    }\n  },\n  \"result\": \"'Skip to content\\\\n\\\\n## Navigation Menu\\\\n\\\\nToggle navigation\\\\n\\\\n[ ](/)\\\\n\\\\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fdatalab-to%2Fmarker)\\\\n\\\\nAppearance settings\\\\n\\\\n  * Platform \\\\n\\\\n    * [ GitHub Copilot  Write better code with AI  ](https://github.com/features/copilot)\\\\n    * [ GitHub Spark  New  Build and deploy intelligent apps  ](https://github.com/features/spark)\\\\n    * [ GitHub Models  New  Manage and compare prompts  ](https://github.com/features/models)\\\\n    * [ GitHub Advanced Security  Find and fix vulnerabilities  ](https://github.com/security/advanced-security)\\\\n    * [ Actions  Automate any workflow  ](https://github.com/features/actions)\\\\n\\\\n    * [ Codespaces  Instant dev environments  ](https://github.com/features/codespaces)\\\\n    * [ Issues  Plan and track work  ](https://github.com/features/issues)\\\\n    * [ Code Review  Manage code changes  ](https://github.com/features/code-review)\\\\n    * [ Discussions  Collaborate outside of code  ](https://github.com/features/discussions)\\\\n    * [ Code Search  Find more, search less  ](https://github.com/features/code-search)\\\\n\\\\nExplore\\\\n\\\\n    * [ Why GitHub ](https://github.com/why-github)\\\\n    * [ Documentation  ](https://docs.github.com)\\\\n    * [ GitHub Skills  ](https://skills.github.com)\\\\n    * [ Blog  ](https://github.blog)\\\\n\\\\nIntegrations\\\\n\\\\n    * [ GitHub Marketplace ](https://github.com/marketplace)\\\\n    * [ MCP Registry ](https://github.com/mcp)\\\\n\\\\n[ View all features  ](https://github.com/features)\\\\n\\\\n  * Solutions \\\\n\\\\nBy company size\\\\n\\\\n    * [ Enterprises ](https://github.com/enterprise)\\\\n    * [ Small and medium teams ](https://github.com/team)\\\\n    * [ Startups ](https://github.com/enterprise/startups)\\\\n    * [ Nonprofits ](/solutions/industry/nonprofits)\\\\n\\\\nBy use case\\\\n\\\\n    * [ App Modernization ](/solutions/use-case/app-modernization)\\\\n    * [ DevSecOps ](/solutions/use-case/devsecops)\\\\n    * [ DevOps ](/solutions/use-case/devops)\\\\n    * [ CI/CD ](/solutions/use-case/ci-cd)\\\\n    * [ View all use cases ](/solutions/use-case)\\\\n\\\\nBy industry\\\\n\\\\n    * [ Healthcare ](/solutions/industry/healthcare)\\\\n    * [ Financial services ](/solutions/industry/financial-services)\\\\n    * [ Manufacturing ](/solutions/industry/manufacturing)\\\\n    * [ Government ](/solutions/industry/government)\\\\n    * [ View all industries ](/solutions/industry)\\\\n\\\\n[ View all solutions  ](/solutions)\\\\n\\\\n  * Resources \\\\n\\\\nTopics\\\\n\\\\n    * [ AI ](/resources/articles?topic=ai)\\\\n    * [ DevOps ](/resources/articles?topic=devops)\\\\n    * [ Security ](/resources/articles?topic=security)\\\\n    * [ Software Development ](/resources/articles?topic=software-development)\\\\n    * [ View all ](/resources/articles)\\\\n\\\\nExplore\\\\n\\\\n    * [ Learning Pathways  ](https://resources.github.com/learn/pathways)\\\\n    * [ Events & Webinars ](https://github.com/resources/events)\\\\n    * [ Ebooks & Whitepapers ](https://github.com/resources/whitepapers)\\\\n    * [ Customer Stories ](https://github.com/customer-stories)\\\\n    * [ Partners ](https://github.com/partners)\\\\n    * [ Executive Insights ](https://github.com/solutions/executive-insights)\\\\n\\\\n  * Open Source \\\\n\\\\n    * [ GitHub Sponsors  Fund open source developers  ](/sponsors)\\\\n\\\\n    * [ The ReadME Project  GitHub community articles  ](https://github.com/readme)\\\\n\\\\nRepositories\\\\n\\\\n    * [ Topics ](https://github.com/topics)\\\\n    * [ Trending ](https://github.com/trending)\\\\n    * [ Collections ](https://github.com/collections)\\\\n\\\\n  * Enterprise \\\\n\\\\n    * [ Enterprise platform  AI-powered developer platform  ](/enterprise)\\\\n\\\\nAvailable add-ons\\\\n\\\\n    * [ GitHub Advanced Security  Enterprise-grade security features  ](https://github.com/security/advanced-security)\\\\n    * [ Copilot for business  Enterprise-grade AI features  ](/features/copilot/copilot-business)\\\\n    * [ Premium Support  Enterprise-grade 24/7 support  ](/premium-support)\\\\n\\\\n  * [Pricing](https://github.com/pricing)\\\\n\\\\nSearch or jump to...\\\\n\\\\n# Search code, repositories, users, issues, pull requests...\\\\n\\\\nSearch\\\\n\\\\nClear\\\\n\\\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\\\n\\\\n#  Provide feedback\\\\n\\\\nWe read every piece of feedback, and take your input very seriously.\\\\n\\\\nInclude my email address so I can be contacted\\\\n\\\\nCancel  Submit feedback\\\\n\\\\n#  Saved searches\\\\n\\\\n## Use saved searches to filter your results more quickly\\\\n\\\\nName\\\\n\\\\nQuery\\\\n\\\\nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\\\\n\\\\nCancel  Create saved search\\\\n\\\\n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fdatalab-to%2Fmarker)\\\\n\\\\n[ Sign up ](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=datalab-to%2Fmarker)\\\\n\\\\nAppearance settings\\\\n\\\\nResetting focus\\\\n\\\\nYou signed in with another tab or window. [Reload]() to refresh your session. You signed out in another tab or window. [Reload]() to refresh your session. You switched accounts on another tab or window. [Reload]() to refresh your session. Dismiss alert\\\\n\\\\n{{ message }}\\\\n\\\\n[ datalab-to ](/datalab-to) / **[marker](/datalab-to/marker) ** Public\\\\n\\\\n  * [ Notifications ](/login?return_to=%2Fdatalab-to%2Fmarker) You must be signed in to change notification settings\\\\n  * [ Fork 2k ](/login?return_to=%2Fdatalab-to%2Fmarker)\\\\n  * [ Star  29.9k ](/login?return_to=%2Fdatalab-to%2Fmarker)\\\\n\\\\nConvert PDF to markdown + JSON quickly with high accuracy\\\\n\\\\n[www.datalab.to](https://www.datalab.to \\\"https://www.datalab.to\\\")\\\\n\\\\n### License\\\\n\\\\n[ GPL-3.0 license ](/datalab-to/marker/blob/master/LICENSE)\\\\n\\\\n[ 29.9k stars ](/datalab-to/marker/stargazers) [ 2k forks ](/datalab-to/marker/forks) [ Branches ](/datalab-to/marker/branches) [ Tags ](/datalab-to/marker/tags) [ Activity ](/datalab-to/marker/activity)\\\\n\\\\n[ Star  ](/login?return_to=%2Fdatalab-to%2Fmarker)\\\\n\\\\n[ Notifications ](/login?return_to=%2Fdatalab-to%2Fmarker) You must be signed in to change notification settings\\\\n\\\\n  * [ Code ](/datalab-to/marker)\\\\n  * [ Issues 298 ](/datalab-to/marker/issues)\\\\n  * [ Pull requests 46 ](/datalab-to/marker/pulls)\\\\n  * [ Actions ](/datalab-to/marker/actions)\\\\n  * [ Projects 0 ](/datalab-to/marker/projects)\\\\n  * [ Security Uh oh!  There was an error while loading. [Please reload this page](). ](/datalab-to/marker/security)\\\\n  * [ Insights ](/datalab-to/marker/pulse)\\\\n\\\\nAdditional navigation options\\\\n\\\\n  * [ Code  ](/datalab-to/marker)\\\\n  * [ Issues  ](/datalab-to/marker/issues)\\\\n  * [ Pull requests  ](/datalab-to/marker/pulls)\\\\n  * [ Actions  ](/datalab-to/marker/actions)\\\\n  * [ Projects  ](/datalab-to/marker/projects)\\\\n  * [ Security  ](/datalab-to/marker/security)\\\\n  * [ Insights  ](/datalab-to/marker/pulse)\\\\n\\\\n# datalab-to/marker\\\\n\\\\nmaster\\\\n\\\\n[Branches](/datalab-to/marker/branches)[Tags](/datalab-to/marker/tags)\\\\n\\\\n[](/datalab-to/marker/branches)[](/datalab-to/marker/tags)\\\\n\\\\nGo to file\\\\n\\\\nCode\\\\n\\\\nOpen more actions menu\\\\n\\\\n## Folders and files\\\\n\\\\nName| Name| Last commit message| Last commit date  \\\\n---|---|---|---  \\\\n  \\\\n## Latest commit\\\\n\\\\n## History\\\\n\\\\n[1,339 Commits](/datalab-to/marker/commits/master/)[](/datalab-to/marker/commits/master/)  \\\\n[.github](/datalab-to/marker/tree/master/.github \\\".github\\\")| [.github](/datalab-to/marker/tree/master/.github \\\".github\\\")|  |   \\\\n[benchmarks](/datalab-to/marker/tree/master/benchmarks \\\"benchmarks\\\")| [benchmarks](/datalab-to/marker/tree/master/benchmarks \\\"benchmarks\\\")|  |   \\\\n[data](/datalab-to/marker/tree/master/data \\\"data\\\")| [data](/datalab-to/marker/tree/master/data \\\"data\\\")|  |   \\\\n[examples](/datalab-to/marker/tree/master/examples \\\"examples\\\")| [examples](/datalab-to/marker/tree/master/examples \\\"examples\\\")|  |   \\\\n[marker](/datalab-to/marker/tree/master/marker \\\"marker\\\")| [marker](/datalab-to/marker/tree/master/marker \\\"marker\\\")|  |   \\\\n[signatures/version1](/datalab-to/marker/tree/master/signatures/version1 \\\"This path skips through empty directories\\\")| [signatures/version1](/datalab-to/marker/tree/master/signatures/version1 \\\"This path skips through empty directories\\\")|  |   \\\\n[static/fonts](/datalab-to/marker/tree/master/static/fonts \\\"This path skips through empty directories\\\")| [static/fonts](/datalab-to/marker/tree/master/static/fonts \\\"This path skips through empty directories\\\")|  |   \\\\n[tests](/datalab-to/marker/tree/master/tests \\\"tests\\\")| [tests](/datalab-to/marker/tree/master/tests \\\"tests\\\")|  |   \\\\n[.gitignore](/datalab-to/marker/blob/master/.gitignore \\\".gitignore\\\")| [.gitignore](/datalab-to/marker/blob/master/.gitignore \\\".gitignore\\\")|  |   \\\\n[.pre-commit-config.yaml](/datalab-to/marker/blob/master/.pre-commit-config.yaml \\\".pre-commit-config.yaml\\\")| [.pre-commit-config.yaml](/datalab-to/marker/blob/master/.pre-commit-config.yaml \\\".pre-commit-config.yaml\\\")|  |   \\\\n[CLA.md](/datalab-to/marker/blob/master/CLA.md \\\"CLA.md\\\")| [CLA.md](/datalab-to/marker/blob/master/CLA.md \\\"CLA.md\\\")|  |   \\\\n[LICENSE](/datalab-to/marker/blob/master/LICENSE \\\"LICENSE\\\")| [LICENSE](/datalab-to/marker/blob/master/LICENSE \\\"LICENSE\\\")|  |   \\\\n[MODEL_LICENSE](/datalab-to/marker/blob/master/MODEL_LICENSE \\\"MODEL_LICENSE\\\")| [MODEL_LICENSE](/datalab-to/marker/blob/master/MODEL_LICENSE \\\"MODEL_LICENSE\\\")|  |   \\\\n[README.md](/datalab-to/marker/blob/master/README.md \\\"README.md\\\")| [README.md](/datalab-to/marker/blob/master/README.md \\\"README.md\\\")|  |   \\\\n[chunk_convert.py](/datalab-to/marker/blob/master/chunk_convert.py \\\"chunk_convert.py\\\")| [chunk_convert.py](/datalab-to/marker/blob/master/chunk_convert.py \\\"chunk_convert.py\\\")|  |   \\\\n[convert.py](/datalab-to/marker/blob/master/convert.py \\\"convert.py\\\")| [convert.py](/datalab-to/marker/blob/master/convert.py \\\"convert.py\\\")|  |   \\\\n[convert_single.py](/datalab-to/marker/blob/master/convert_single.py \\\"convert_single.py\\\")| [convert_single.py](/datalab-to/marker/blob/master/convert_single.py \\\"convert_single.py\\\")|  |   \\\\n[extraction_app.py](/datalab-to/marker/blob/master/extraction_app.py \\\"extraction_app.py\\\")| [extraction_app.py](/datalab-to/marker/blob/master/extraction_app.py \\\"extraction_app.py\\\")|  |   \\\\n[marker_app.py](/datalab-to/marker/blob/master/marker_app.py \\\"marker_app.py\\\")| [marker_app.py](/datalab-to/marker/blob/master/marker_app.py \\\"marker_app.py\\\")|  |   \\\\n[marker_server.py](/datalab-to/marker/blob/master/marker_server.py \\\"marker_server.py\\\")| [marker_server.py](/datalab-to/marker/blob/master/marker_server.py \\\"marker_server.py\\\")|  |   \\\\n[poetry.lock](/datalab-to/marker/blob/master/poetry.lock \\\"poetry.lock\\\")| [poetry.lock](/datalab-to/marker/blob/master/poetry.lock \\\"poetry.lock\\\")|  |   \\\\n[pyproject.toml](/datalab-to/marker/blob/master/pyproject.toml \\\"pyproject.toml\\\")| [pyproject.toml](/datalab-to/marker/blob/master/pyproject.toml \\\"pyproject.toml\\\")|  |   \\\\n[pytest.ini](/datalab-to/marker/blob/master/pytest.ini \\\"pytest.ini\\\")| [pytest.ini](/datalab-to/marker/blob/master/pytest.ini \\\"pytest.ini\\\")|  |   \\\\nView all files  \\\\n  \\\\n## Repository files navigation\\\\n\\\\n  * README\\\\n  * GPL-3.0 license\\\\n\\\\n# Marker\\\\n\\\\nMarker converts documents to markdown, JSON, chunks, and HTML quickly and accurately.\\\\n\\\\n  * Converts PDF, image, PPTX, DOCX, XLSX, HTML, EPUB files in all languages\\\\n  * Formats tables, forms, equations, inline math, links, references, and code blocks\\\\n  * Extracts and saves images\\\\n  * Removes headers/footers/other artifacts\\\\n  * Extensible with your own formatting and logic\\\\n  * Does structured extraction, given a JSON schema (beta)\\\\n  * Optionally boost accuracy with LLMs (and your own prompt)\\\\n  * Works on GPU, CPU, or MPS\\\\n\\\\nFor our managed API or on-prem document intelligence solution, check out [our platform here](https://datalab.to?utm_source=gh-marker).\\\\n\\\\n## Performance\\\\n\\\\n[![](/datalab-to/marker/raw/master/data/images/overall.png)](/datalab-to/marker/blob/master/data/images/overall.png)\\\\n\\\\nMarker benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools.\\\\n\\\\nThe above results are running single PDF pages serially. Marker is significantly faster when running in batch mode, with a projected throughput of 25 pages/second on an H100.\\\\n\\\\nSee below for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.\\\\n\\\\n## Hybrid Mode\\\\n\\\\nFor the highest accuracy, pass the `--use_llm` flag to use an LLM alongside marker. This will do things like merge tables across pages, handle inline math, format tables properly, and extract values from forms. It can use any gemini or ollama model. By default, it uses `gemini-2.0-flash`. See below for details.\\\\n\\\\nHere is a table benchmark comparing marker, gemini flash alone, and marker with use_llm:\\\\n\\\\n[![](/datalab-to/marker/raw/master/data/images/table.png)](/datalab-to/marker/blob/master/data/images/table.png)\\\\n\\\\nAs you can see, the use_llm mode offers higher accuracy than marker or gemini alone.\\\\n\\\\n## Examples\\\\n\\\\nPDF | File type | Markdown | JSON  \\\\n---|---|---|---  \\\\n[Think Python](https://greenteapress.com/thinkpython/thinkpython.pdf) | Textbook | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/thinkpython/thinkpython.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/thinkpython.json)  \\\\n[Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/switch_transformers/switch_trans.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/switch_trans.json)  \\\\n[Multi-column CNN](https://arxiv.org/pdf/1804.07821.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/multicolcnn/multicolcnn.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/multicolcnn.json)  \\\\n  \\\\n# Commercial usage\\\\n\\\\nOur model weights use a modified AI Pubs Open Rail-M license (free for research, personal use, and startups under $2M funding/revenue) and our code is GPL. For broader commercial licensing or to remove GPL requirements, visit our pricing page [here](https://www.datalab.to/pricing?utm_source=gh-marker).\\\\n\\\\n# Hosted API & On-prem\\\\n\\\\nThere\\\\'s a [hosted API](https://www.datalab.to?utm_source=gh-marker) and [painless on-prem solution](https://www.datalab.to/blog/self-serve-on-prem-licensing) for marker - it\\\\'s free to sign up, and we\\\\'ll throw in credits for you to test it out.\\\\n\\\\nThe API:\\\\n\\\\n  * Supports PDF, image, PPT, PPTX, DOC, DOCX, XLS, XLSX, HTML, EPUB files\\\\n  * Is 1/4th the price of leading cloud-based competitors\\\\n  * Fast - ~15s for a 250 page PDF\\\\n  * Supports LLM mode\\\\n  * High uptime (99.99%)\\\\n\\\\n# Community\\\\n\\\\n[Discord](https://discord.gg//KuZwXNGnfH) is where we discuss future development.\\\\n\\\\n# Installation\\\\n\\\\nYou\\\\'ll need python 3.10+ and [PyTorch](https://pytorch.org/get-started/locally/).\\\\n\\\\nInstall with:\\\\n\\\\n```\\\\npip install marker-pdf\\\\n```\\\\n\\\\nIf you want to use marker on documents other than PDFs, you will need to install additional dependencies with:\\\\n\\\\n```\\\\npip install marker-pdf[full]\\\\n```\\\\n\\\\n# Usage\\\\n\\\\nFirst, some configuration:\\\\n\\\\n  * Your torch device will be automatically detected, but you can override this. For example, `TORCH_DEVICE=cuda`.\\\\n  * Some PDFs, even digital ones, have bad text in them. Set `--force_ocr` to force OCR on all lines, or the `strip_existing_ocr` to keep all digital text, and strip out any existing OCR text.\\\\n  * If you care about inline math, set `force_ocr` to convert inline math to LaTeX.\\\\n\\\\n## Interactive App\\\\n\\\\nI\\\\'ve included a streamlit app that lets you interactively try marker with some basic options. Run it with:\\\\n\\\\n```\\\\npip install streamlit streamlit-ace\\\\nmarker_gui\\\\n```\\\\n\\\\n## Convert a single file\\\\n\\\\n```\\\\nmarker_single /path/to/file.pdf\\\\n```\\\\n\\\\nYou can pass in PDFs or images.\\\\n\\\\nOptions:\\\\n\\\\n  * `--page_range TEXT`: Specify which pages to process. Accepts comma-separated page numbers and ranges. Example: `--page_range \\\"0,5-10,20\\\"` will process pages 0, 5 through 10, and page 20.\\\\n  * `--output_format [markdown|json|html|chunks]`: Specify the format for the output results.\\\\n  * `--output_dir PATH`: Directory where output files will be saved. Defaults to the value specified in settings.OUTPUT_DIR.\\\\n  * `--paginate_output`: Paginates the output, using `\\\\\\\\n\\\\\\\\n{PAGE_NUMBER}` followed by `-` * 48, then `\\\\\\\\n\\\\\\\\n`\\\\n  * `--use_llm`: Uses an LLM to improve accuracy. You will need to configure the LLM backend - see below.\\\\n  * `--force_ocr`: Force OCR processing on the entire document, even for pages that might contain extractable text. This will also format inline math properly.\\\\n  * `--block_correction_prompt`: if LLM mode is active, an optional prompt that will be used to correct the output of marker. This is useful for custom formatting or logic that you want to apply to the output.\\\\n  * `--strip_existing_ocr`: Remove all existing OCR text in the document and re-OCR with surya.\\\\n  * `--redo_inline_math`: If you want the absolute highest quality inline math conversion, use this along with `--use_llm`.\\\\n  * `--disable_image_extraction`: Don\\\\'t extract images from the PDF. If you also specify `--use_llm`, then images will be replaced with a description.\\\\n  * `--debug`: Enable debug mode for additional logging and diagnostic information.\\\\n  * `--processors TEXT`: Override the default processors by providing their full module paths, separated by commas. Example: `--processors \\\"module1.processor1,module2.processor2\\\"`\\\\n  * `--config_json PATH`: Path to a JSON configuration file containing additional settings.\\\\n  * `config --help`: List all available builders, processors, and converters, and their associated configuration. These values can be used to build a JSON configuration file for additional tweaking of marker defaults.\\\\n  * `--converter_cls`: One of `marker.converters.pdf.PdfConverter` (default) or `marker.converters.table.TableConverter`. The `PdfConverter` will convert the whole PDF, the `TableConverter` will only extract and convert tables.\\\\n  * `--llm_service`: Which llm service to use if `--use_llm` is passed. This defaults to `marker.services.gemini.GoogleGeminiService`.\\\\n  * `--help`: see all of the flags that can be passed into marker. (it supports many more options then are listed above)\\\\n\\\\nThe list of supported languages for surya OCR is [here](https://github.com/VikParuchuri/surya/blob/master/surya/recognition/languages.py). If you don\\\\'t need OCR, marker can work with any language.\\\\n\\\\n## Convert multiple files\\\\n\\\\n```\\\\nmarker /path/to/input/folder\\\\n```\\\\n\\\\n  * `marker` supports all the same options from `marker_single` above.\\\\n  * `--workers` is the number of conversion workers to run simultaneously. This is automatically set by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Marker will use 5GB of VRAM per worker at the peak, and 3.5GB average.\\\\n\\\\n## Convert multiple files on multiple GPUs\\\\n\\\\n```\\\\nNUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out\\\\n```\\\\n\\\\n  * `NUM_DEVICES` is the number of GPUs to use. Should be `2` or greater.\\\\n  * `NUM_WORKERS` is the number of parallel processes to run on each GPU.\\\\n\\\\n## Use from python\\\\n\\\\nSee the `PdfConverter` class at `marker/converters/pdf.py` function for additional arguments that can be passed.\\\\n\\\\n```\\\\nfrom marker.converters.pdf import PdfConverter\\\\nfrom marker.models import create_model_dict\\\\nfrom marker.output import text_from_rendered\\\\n\\\\nconverter = PdfConverter(\\\\n    artifact_dict=create_model_dict(),\\\\n)\\\\nrendered = converter(\\\"FILEPATH\\\")\\\\ntext, _, images = text_from_rendered(rendered)\\\\n```\\\\n\\\\n`rendered` will be a pydantic basemodel with different properties depending on the output type requested. With markdown output (default), you\\\\'ll have the properties `markdown`, `metadata`, and `images`. For json output, you\\\\'ll have `children`, `block_type`, and `metadata`.\\\\n\\\\n### Custom configuration\\\\n\\\\nYou can pass configuration using the `ConfigParser`. To see all availa&lt;TRUNCATED&gt;\"\n}\n\nPerfect! I‚Äôve read the Marker repo. Key findings for your use case:\nMarker has built-in chunking support! There‚Äôs a chunk_convert.py file and you can output in ‚Äúchunks‚Äù format using --output_format chunks.\nThe chunks format: - Flattens everything into a single list (not a tree) - Only top-level blocks from each page - Includes full HTML of each block - Designed specifically for RAG applications\nFor tables specifically: - Marker extracts and formats tables automatically - Can use --use_llm flag to improve table accuracy significantly (0.816 ‚Üí 0.907 score) - Has a dedicated TableConverter for table-only extraction\nWould you like to try using Marker‚Äôs chunking functionality to replace your current document processing?\nI want to first try, lets try it on the strubell2019 document\n\n\nü§ñReplyü§ñ\nGreat! Let‚Äôs start by installing marker and testing it on that document.\nFirst, install marker:\n!pip install marker-pdf\nThen try converting the strubell2019.pdf with chunking output:\n!marker_single wattbot_data/strubell2019.pdf --output_format chunks --output_dir marker_output\nThis will create chunks and save them to the marker_output directory. Want to give it a try and see what the output looks like?\n\n!pip install marker-pdf\n\nCollecting marker-pdf\n  Downloading marker_pdf-1.10.1-py3-none-any.whl.metadata (30 kB)\nCollecting Pillow&lt;11.0.0,&gt;=10.1.0 (from marker-pdf)\n  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nCollecting anthropic&lt;0.47.0,&gt;=0.46.0 (from marker-pdf)\n  Downloading anthropic-0.46.0-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: click&lt;9.0.0,&gt;=8.2.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (8.3.1)\nRequirement already satisfied: filetype&lt;2.0.0,&gt;=1.2.0 in ./.local/lib/python3.12/site-packages (from marker-pdf) (1.2.0)\nCollecting ftfy&lt;7.0.0,&gt;=6.1.1 (from marker-pdf)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: google-genai&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (1.51.0)\nCollecting markdown2&lt;3.0.0,&gt;=2.5.2 (from marker-pdf)\n  Downloading markdown2-2.5.4-py3-none-any.whl.metadata (2.1 kB)\nCollecting markdownify&lt;2.0.0,&gt;=1.1.0 (from marker-pdf)\n  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)\nCollecting openai&lt;2.0.0,&gt;=1.65.2 (from marker-pdf)\n  Using cached openai-1.109.1-py3-none-any.whl.metadata (29 kB)\nCollecting pdftext&lt;0.7.0,&gt;=0.6.3 (from marker-pdf)\n  Downloading pdftext-0.6.3-py3-none-any.whl.metadata (8.5 kB)\nCollecting pre-commit&lt;5.0.0,&gt;=4.2.0 (from marker-pdf)\n  Downloading pre_commit-4.4.0-py2.py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: pydantic&lt;3.0.0,&gt;=2.4.2 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (2.12.4)\nRequirement already satisfied: pydantic-settings&lt;3.0.0,&gt;=2.0.3 in ./.local/lib/python3.12/site-packages (from marker-pdf) (2.10.1)\nRequirement already satisfied: python-dotenv&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (1.2.1)\nCollecting rapidfuzz&lt;4.0.0,&gt;=3.8.1 (from marker-pdf)\n  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nCollecting regex&lt;2025.0.0,&gt;=2024.4.28 (from marker-pdf)\n  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\nRequirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.6.1 in ./.local/lib/python3.12/site-packages (from marker-pdf) (1.7.2)\nCollecting surya-ocr&lt;0.18.0,&gt;=0.17.0 (from marker-pdf)\n  Downloading surya_ocr-0.17.0-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: torch&lt;3.0.0,&gt;=2.7.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (2.9.1+cpu)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (4.67.1)\nRequirement already satisfied: transformers&lt;5.0.0,&gt;=4.45.2 in ./.local/lib/python3.12/site-packages (from marker-pdf) (4.56.2)\nRequirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (4.11.0)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (1.9.0)\nRequirement already satisfied: httpx&lt;1,&gt;=0.23.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (0.28.1)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (0.12.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (1.3.1)\nRequirement already satisfied: typing-extensions&lt;5,&gt;=4.10 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (4.15.0)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.12/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (3.11)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/site-packages (from ftfy&lt;7.0.0,&gt;=6.1.1-&gt;marker-pdf) (0.2.14)\nRequirement already satisfied: google-auth&lt;3.0.0,&gt;=2.14.1 in /usr/local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (2.43.0)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.28.1 in /usr/local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (2.32.5)\nRequirement already satisfied: tenacity&lt;9.2.0,&gt;=8.2.3 in ./.local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (9.1.2)\nRequirement already satisfied: websockets&lt;15.1.0,&gt;=13.0.0 in /usr/local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (15.0.1)\nRequirement already satisfied: cachetools&lt;7.0,&gt;=2.0.0 in /usr/local/lib/python3.12/site-packages (from google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (6.2.2)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.12/site-packages (from google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (0.4.2)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.12/site-packages (from google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (4.9.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (1.0.9)\nRequirement already satisfied: h11&gt;=0.16 in /usr/local/lib/python3.12/site-packages (from httpcore==1.*-&gt;httpx&lt;1,&gt;=0.23.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (0.16.0)\nRequirement already satisfied: beautifulsoup4&lt;5,&gt;=4.9 in /usr/local/lib/python3.12/site-packages (from markdownify&lt;2.0.0,&gt;=1.1.0-&gt;marker-pdf) (4.14.2)\nRequirement already satisfied: six&lt;2,&gt;=1.15 in /usr/local/lib/python3.12/site-packages (from markdownify&lt;2.0.0,&gt;=1.1.0-&gt;marker-pdf) (1.17.0)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.12/site-packages (from beautifulsoup4&lt;5,&gt;=4.9-&gt;markdownify&lt;2.0.0,&gt;=1.1.0-&gt;marker-pdf) (2.8)\nCollecting pypdfium2==4.30.0 (from pdftext&lt;0.7.0,&gt;=0.6.3-&gt;marker-pdf)\n  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\nCollecting cfgv&gt;=2.0.0 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting identify&gt;=1.0.0 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)\n  Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting nodeenv&gt;=0.11.1 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)\n  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.12/site-packages (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf) (6.0.3)\nCollecting virtualenv&gt;=20.10.0 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)\n  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic&lt;3.0.0,&gt;=2.4.2-&gt;marker-pdf) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/site-packages (from pydantic&lt;3.0.0,&gt;=2.4.2-&gt;marker-pdf) (2.41.5)\nRequirement already satisfied: typing-inspection&gt;=0.4.2 in /usr/local/lib/python3.12/site-packages (from pydantic&lt;3.0.0,&gt;=2.4.2-&gt;marker-pdf) (0.4.2)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.28.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (3.4.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.28.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (2.5.0)\nRequirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.12/site-packages (from rsa&lt;5,&gt;=3.1.4-&gt;google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (0.6.1)\nRequirement already satisfied: numpy&gt;=1.22.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (2.3.5)\nRequirement already satisfied: scipy&gt;=1.8.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (1.16.3)\nRequirement already satisfied: joblib&gt;=1.2.0 in ./.local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (1.5.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in ./.local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (3.6.0)\nCollecting einops&lt;0.9.0,&gt;=0.8.1 (from surya-ocr&lt;0.18.0,&gt;=0.17.0-&gt;marker-pdf)\n  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\nCollecting opencv-python-headless==4.11.0.86 (from surya-ocr&lt;0.18.0,&gt;=0.17.0-&gt;marker-pdf)\n  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: platformdirs&lt;5.0.0,&gt;=4.3.6 in /usr/local/lib/python3.12/site-packages (from surya-ocr&lt;0.18.0,&gt;=0.17.0-&gt;marker-pdf) (4.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.20.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (80.9.0)\nRequirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (1.14.0)\nRequirement already satisfied: networkx&gt;=2.5.1 in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.1.6)\nRequirement already satisfied: fsspec&gt;=0.8.5 in ./.local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (2025.3.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.34.0 in ./.local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (0.36.0)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (25.0)\nRequirement already satisfied: tokenizers&lt;=0.23.0,&gt;=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (0.22.1)\nRequirement already satisfied: safetensors&gt;=0.4.3 in ./.local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (0.6.2)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (1.2.0)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy&gt;=1.13.3-&gt;torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (1.3.0)\nCollecting distlib&lt;1,&gt;=0.3.7 (from virtualenv&gt;=20.10.0-&gt;pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)\n  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2-&gt;torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.0.3)\nDownloading marker_pdf-1.10.1-py3-none-any.whl (188 kB)\nDownloading anthropic-0.46.0-py3-none-any.whl (223 kB)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\nDownloading markdown2-2.5.4-py3-none-any.whl (49 kB)\nDownloading markdownify-1.2.2-py3-none-any.whl (15 kB)\nUsing cached openai-1.109.1-py3-none-any.whl (948 kB)\nDownloading pdftext-0.6.3-py3-none-any.whl (23 kB)\nDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/2.8 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.8/2.8 MB 86.5 MB/s  0:00:00\nDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/4.5 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫ 4.5/4.5 MB 280.7 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4.5/4.5 MB 16.0 MB/s  0:00:00\nDownloading pre_commit-4.4.0-py2.py3-none-any.whl (226 kB)\nDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/3.2 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏ 3.1/3.2 MB 233.7 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.2/3.2 MB 12.3 MB/s  0:00:00\nDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/796.9 kB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 796.9/796.9 kB 36.4 MB/s  0:00:00\nDownloading surya_ocr-0.17.0-py3-none-any.whl (183 kB)\nDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/50.0 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 38.3/50.0 MB 191.7 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏ 49.8/50.0 MB 176.3 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50.0/50.0 MB 98.1 MB/s  0:00:00\nDownloading einops-0.8.1-py3-none-any.whl (64 kB)\nDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\nDownloading identify-2.6.15-py2.py3-none-any.whl (99 kB)\nDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\nDownloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/6.0 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.0/6.0 MB 49.7 MB/s  0:00:00\nDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\nInstalling collected packages: distlib, virtualenv, regex, rapidfuzz, pypdfium2, Pillow, opencv-python-headless, nodeenv, markdown2, identify, ftfy, einops, cfgv, pre-commit, markdownify, openai, anthropic, pdftext, surya-ocr, marker-pdf\n   ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  1/20 [virtualenv]  WARNING: The script virtualenv is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]  Attempting uninstall: pypdfium2\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]    Found existing installation: pypdfium2 5.0.0\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]    Uninstalling pypdfium2-5.0.0:\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]      Successfully uninstalled pypdfium2-5.0.0\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  4/20 [pypdfium2]  WARNING: The script pypdfium2 is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  4/20 [pypdfium2]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  5/20 [Pillow]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  5/20 [Pillow]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  7/20 [nodeenv]  WARNING: The script nodeenv is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  7/20 [nodeenv]  WARNING: The script markdown2 is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The script identify-cli is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The script ftfy is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10/20 [ftfy]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10/20 [ftfy]  WARNING: The script pre-commit is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]  WARNING: The script markdownify is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  Attempting uninstall: openai\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]    Found existing installation: openai 2.7.1\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]    Uninstalling openai-2.7.1:\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]      Successfully uninstalled openai-2.7.1\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]  WARNING: The script openai is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 16/20 [anthropic]  WARNING: The script pdftext is installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ 18/20 [surya-ocr]  WARNING: The scripts surya_detect, surya_gui, surya_latex_ocr, surya_layout, surya_ocr, surya_table and texify_gui are installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ 19/20 [marker-pdf]  WARNING: The scripts marker, marker_chunk_convert, marker_extract, marker_gui, marker_server and marker_single are installed in '/app/data/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 [marker-pdf]\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nreward-kit 0.4.1 requires openai==1.78.1, but you have openai 1.109.1 which is incompatible.\nclaudette 0.3.10 requires anthropic&gt;=0.52, but you have anthropic 0.46.0 which is incompatible.\nSuccessfully installed Pillow-10.4.0 anthropic-0.46.0 cfgv-3.4.0 distlib-0.4.0 einops-0.8.1 ftfy-6.3.1 identify-2.6.15 markdown2-2.5.4 markdownify-1.2.2 marker-pdf-1.10.1 nodeenv-1.9.1 openai-1.109.1 opencv-python-headless-4.11.0.86 pdftext-0.6.3 pre-commit-4.4.0 pypdfium2-4.30.0 rapidfuzz-3.14.3 regex-2024.11.6 surya-ocr-0.17.0 virtualenv-20.35.4\n\n\nIs it possible to use python equivalent instead of cli command of marker_single\n\n\nü§ñReplyü§ñ\nYes! From the README I read earlier, you can use Marker directly in Python. Here‚Äôs the basic approach:\nfrom marker.converters.pdf import PdfConverter\nfrom marker.models import create_model_dict\n\nconverter = PdfConverter(\n    artifact_dict=create_model_dict(),\n)\nrendered = converter(\"wattbot_data/strubell2019.pdf\")\nFor chunks specifically, you‚Äôd need to specify the output format. Would you like to try this and see what properties the rendered object has?\n\nfrom marker.converters.pdf import PdfConverter\nfrom marker.models import create_model_dict\n\n/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nconverter = PdfConverter(\n    artifact_dict=create_model_dict(),\n)\nrendered = converter(\"wattbot_data/strubell2019.pdf\")\n\n\nlen(rendered)\n\nlets try fitz, i want to know if we can give chunk size and overlap\n\n\nü§ñReplyü§ñ\nPyMuPDF (fitz) is primarily a PDF rendering library - it doesn‚Äôt have built-in chunking with overlap like some text processing libraries. However, you can:\n\nExtract text from the PDF with fitz\nManually implement chunking with overlap on the extracted text\n\nWould you like to try extracting text with fitz first to see what you get, or would you prefer to continue exploring Marker‚Äôs chunking capabilities since it‚Äôs already designed for this use case?\nlets try with fitz because marker is difficult to run, takes lot of resource. we can try marker if fitz does not workout\n\n\nü§ñReplyü§ñ\nGood call - fitz is much lighter weight. Let‚Äôs start by extracting text from the strubell2019 PDF with fitz:\nimport fitz\ndoc = fitz.open(data_path/'strubell2019.pdf')\nThen you can extract text page by page or all at once. Would you like to try extracting from one page first to see what the output looks like?\n\nimport fitz\ndoc = fitz.open(data_path/'strubell2019.pdf')\n\nsure\n\n\nü§ñReplyü§ñ\nTry extracting text from the first page:\npage = doc[0]\ntext = page.get_text()\nprint(text)\nThis will show you what fitz extracts. Take a look at the output and see how it compares to what you got with rd.read_pdf.\n\npage = doc[0]\ntext = page.get_text()\n\n\nprint(text)\n\narXiv:1906.02243v1  [cs.CL]  5 Jun 2019\nEnergy and Policy Considerations for Deep Learning in NLP\nEmma Strubell\nAnanya Ganesh\nAndrew McCallum\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{strubell, aganesh, mccallum}@cs.umass.edu\nAbstract\nRecent progress in hardware and methodol-\nogy for training neural networks has ushered\nin a new generation of large networks trained\non abundant data.\nThese models have ob-\ntained notable gains in accuracy across many\nNLP tasks. However, these accuracy improve-\nments depend on the availability of exception-\nally large computational resources that neces-\nsitate similarly substantial energy consump-\ntion. As a result these models are costly to\ntrain and develop, both Ô¨Ånancially, due to the\ncost of hardware and electricity or cloud com-\npute time, and environmentally, due to the car-\nbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring\nthis issue to the attention of NLP researchers\nby quantifying the approximate Ô¨Ånancial and\nenvironmental costs of training a variety of re-\ncently successful neural network models for\nNLP. Based on these Ô¨Åndings, we propose ac-\ntionable recommendations to reduce costs and\nimprove equity in NLP research and practice.\n1\nIntroduction\nAdvances in techniques and hardware for train-\ning deep neural networks have recently en-\nabled impressive accuracy improvements across\nmany fundamental NLP tasks (Bahdanau et al.,\n2015; Luong et al., 2015; Dozat and Man-\nning, 2017; Vaswani et al., 2017), with the\nmost computationally-hungry models obtaining\nthe highest scores (Peters et al., 2018; Devlin et al.,\n2019; Radford et al., 2019; So et al., 2019). As\na result, training a state-of-the-art model now re-\nquires substantial computational resources which\ndemand considerable energy, along with the as-\nsociated Ô¨Ånancial and environmental costs.\nRe-\nsearch and development of new models multiplies\nthese costs by thousands of times by requiring re-\ntraining to experiment with model architectures\nand hyperparameters. Whereas a decade ago most\nConsumption\nCO2e (lbs)\nAir travel, 1 passenger, NY‚ÜîSF\n1984\nHuman life, avg, 1 year\n11,023\nAmerican life, avg, 1 year\n36,156\nCar, avg incl. fuel, 1 lifetime\n126,000\nTraining one model (GPU)\nNLP pipeline (parsing, SRL)\n39\nw/ tuning & experimentation\n78,468\nTransformer (big)\n192\nw/ neural architecture search\n626,155\nTable 1: Estimated CO2 emissions from training com-\nmon NLP models, compared to familiar consumption.1\nNLP models could be trained and developed on\na commodity laptop or server, many now require\nmultiple instances of specialized hardware such as\nGPUs or TPUs, therefore limiting access to these\nhighly accurate models on the basis of Ô¨Ånances.\nEven when these expensive computational re-\nsources are available, model training also incurs a\nsubstantial cost to the environment due to the en-\nergy required to power this hardware for weeks or\nmonths at a time. Though some of this energy may\ncome from renewable or carbon credit-offset re-\nsources, the high energy demands of these models\nare still a concern since (1) energy is not currently\nderived from carbon-neural sources in many loca-\ntions, and (2) when renewable energy is available,\nit is still limited to the equipment we have to pro-\nduce and store it, and energy spent training a neu-\nral network might better be allocated to heating a\nfamily‚Äôs home. It is estimated that we must cut\ncarbon emissions by half over the next decade to\ndeter escalating rates of natural disaster, and based\non the estimated CO2 emissions listed in Table 1,\n1Sources:\n(1) Air travel and per-capita consump-\ntion:\nhttps://bit.ly/2Hw0xWc;\n(2) car lifetime:\nhttps://bit.ly/2Qbr0w1.\n\n\n\nThe table information is extracted but how to know rows and column\n\n\nü§ñReplyü§ñ\nGood observation! Fitz extracts the table as plain text without preserving the row/column structure. To get structured table data, you‚Äôd need to:\n\nUse fitz‚Äôs table detection: page.find_tables() - this can identify tables and extract them as structured data\nParse the text manually - look for patterns in spacing/alignment\n\nWould you like to try page.find_tables() to see if it can detect the table structure on this page?\n\ntables = page.find_tables(); tables\n\n&lt;pymupdf.table.TableFinder&gt;\n\n\nwhat is next?\n\n\nü§ñReplyü§ñ\nThe tables object is a TableFinder. To see the actual tables, try:\ntables.tables\nThis will show you a list of detected tables. If there are any, you can then extract them as structured data.\n\ntables.tables\n\n[]\n\n\nit is empty\n\n\nü§ñReplyü§ñ\nFitz‚Äôs table detection didn‚Äôt find the table on that page - probably because it‚Äôs formatted as plain text without borders or clear structure.\nYou have a few options:\n\nTry adjusting fitz‚Äôs table detection settings - it has parameters you can tune\nExtract tables as images and use OCR - fitz can extract page regions as images\nGo back to Marker - it uses ML models specifically trained for table detection\nUse a dedicated table extraction library like camelot or tabula\n\nSince fitz isn‚Äôt detecting the tables automatically, would you like to try one of these alternatives, or would you prefer to work with the text as-is and implement your own chunking strategy?\nusing marker will return markdown. Do you know how to chunk a markdow?\n\n\nü§ñReplyü§ñ\nYes! Markdown can be chunked in several ways:\n\nBy headers - Split on #, ##, ### etc. to keep related sections together\nBy paragraphs - Split on \\n\\n (double newlines)\nBy semantic units - Use a library like langchain or llama-index that has markdown-aware chunkers\nFixed size with overlap - Just like regular text, but preserve markdown formatting\n\nFor tables specifically, Marker converts them to markdown table format, which you could: - Keep as complete units (don‚Äôt split tables across chunks) - Parse into structured data\nWould you like to try getting Marker‚Äôs markdown output first and then explore how to chunk it effectively?\nI have got access to datalab now. I want to use their api for the markdown extraction\n\n\nü§ñReplyü§ñ\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\nGreat! Datalab is the hosted API for Marker. Would you like me to look up the Datalab API documentation to see how to use it for markdown extraction?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! I found the Datalab API documentation. Here‚Äôs the key info:\n* The marker endpoint is at /api/v1/marker, and you can submit requests with parameters like output_format (set to ‚Äòmarkdown‚Äô), force_ocr, use_llm, etc.\nBasic Python example:\nimport requests\n\nurl = \"https://www.datalab.to/api/v1/marker\"\nform_data = {\n    'file': ('test.pdf', open('path/to/file.pdf', 'rb'), 'application/pdf'),\n    'output_format': (None, 'markdown'),  # or 'chunks'\n}\nheaders = {\"X-Api-Key\": \"YOUR_API_KEY\"}\nresponse = requests.post(url, files=form_data, headers=headers)\n* This returns a request_id that you use to retrieve the result later.\nWould you like to try calling the API with one of your PDFs?\nI have the following code #| export class HybridSearch: def init(self, all_chunks, dense_embds, tokenized_corpus): fc.store_attr() self.bm25 = BM25Okapi(self.tokenized_corpus)\ndef search(self, query, limit=5):\n    query_emb = embedder.create_dembs(texts=[query])[0]\n    tokenized_query = query.lower().split()\n    results = self.bm25.get_top_n(tokenized_query, self.all_chunks, n=limit)\n    scores = cosine_similarity([query_emb], self.dense_embds)[0]\n    # print(len(scores), scores[0], len(scores[0]))\n    idx = np.argsort(scores)[-limit:][::-1]\n    results.extend(self.all_chunks[idx])\n    return results\nI want to use qwen reranker, also I want to include the preceding and next chunk for each chunk in the results\n\n\nü§ñReplyü§ñ\nGood improvements! Let me break this down into steps:\nFor reranking with Qwen: You‚Äôll need to combine the BM25 and dense results first, then pass them to the reranker to get the final top-k.\nFor including preceding/next chunks: You‚Äôll need to track chunk indices so you can grab adjacent chunks.\nWhich would you like to tackle first - adding the reranker or implementing the context window (prev/next chunks)?\nIncluding the prev and next chunks\n\n\nü§ñReplyü§ñ\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\nGood choice! To include prev/next chunks, you‚Äôll need to:\n\nTrack chunk indices in your results\nFetch adjacent chunks based on those indices\nHandle edge cases (first/last chunks in a document)\n\nHere‚Äôs a helper method you could add to your HybridSearch class:\ndef get_context_chunks(self, chunk_indices, window=1):\n    \"\"\"Get chunks with surrounding context\"\"\"\n    result_chunks = []\n    seen = set()\n    \n    for idx in chunk_indices:\n        start = max(0, idx - window)\n        end = min(len(self.all_chunks), idx + window + 1)\n        \n        for i in range(start, end):\n            if i not in seen:\n                result_chunks.append((i, self.all_chunks[i]))\n                seen.add(i)\n    \n    return [chunk for _, chunk in sorted(result_chunks)]\nHowever, there‚Äôs one issue - you probably only want to include adjacent chunks from the same document. Should we add a check for doc_id to prevent mixing chunks from different documents?\nYes\n\n\nü§ñReplyü§ñ\nPerfect! Here‚Äôs an updated version that respects document boundaries:\ndef get_context_chunks(self, chunk_indices, window=1):\n    \"\"\"Get chunks with surrounding context, respecting document boundaries\"\"\"\n    result_chunks = []\n    seen = set()\n    \n    for idx in chunk_indices:\n        current_doc_id = self.all_chunks[idx].doc_id\n        \n        # Find adjacent chunks in same document\n        for offset in range(-window, window + 1):\n            new_idx = idx + offset\n            \n            # Check bounds and same document\n            if (0 &lt;= new_idx &lt; len(self.all_chunks) and \n                new_idx not in seen and\n                self.all_chunks[new_idx].doc_id == current_doc_id):\n                \n                result_chunks.append((new_idx, self.all_chunks[new_idx]))\n                seen.add(new_idx)\n    \n    return [chunk for _, chunk in sorted(result_chunks)]\nThen update your search method to use it:\ndef search(self, query, limit=5, window=1):\n    # ... your existing search logic ...\n    \n    # Combine and get top indices\n    combined_indices = list(set(bm25_indices + dense_indices))[:limit]\n    \n    # Get chunks with context\n    return self.get_context_chunks(combined_indices, window=window)\nWould you like to integrate this into your current code?\nPlease follow fastai coding style\n\n\nü§ñReplyü§ñ\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\nHere‚Äôs the fastai-style version:\ndef get_context_chunks(self, idxs, window=1):\n    \"\"\"Get chunks with surrounding context, respecting doc boundaries\"\"\"\n    seen, res = set(), []\n    for i in idxs:\n        doc_id = self.all_chunks[i].doc_id\n        for j in range(max(0, i-window), min(len(self.all_chunks), i+window+1)):\n            if j not in seen and self.all_chunks[j].doc_id == doc_id:\n                res.append((j, self.all_chunks[j]))\n                seen.add(j)\n    return fc.L(sorted(res)).itemgot(1)\nAnd the updated search method:\ndef search(self, query, limit=5, window=1):\n    q_emb = embedder.create_dembs(texts=[query])[0]\n    tok_q = query.lower().split()\n    \n    bm25_res = self.bm25.get_top_n(tok_q, range(len(self.all_chunks)), n=limit)\n    dense_scores = cosine_similarity([q_emb], self.dense_embds)[0]\n    dense_idxs = np.argsort(dense_scores)[-limit:][::-1].tolist()\n    \n    idxs = list(set(bm25_res + dense_idxs))[:limit]\n    return self.get_context_chunks(idxs, window=window)\nNote: I changed the BM25 call to return indices instead of chunks directly so we can use them for context lookup. Would you like to try this?\nCan you please make the get_context_chunks simpler with fixed window of 1?\n\n\nü§ñReplyü§ñ\nSure! Here‚Äôs a simpler version with window=1 fixed:\ndef get_context_chunks(self, idxs):\n    \"\"\"Get chunks with prev/next context, respecting doc boundaries\"\"\"\n    seen, res = set(), []\n    for i in idxs:\n        doc_id = self.all_chunks[i].doc_id\n        for j in [i-1, i, i+1]:\n            if 0 &lt;= j &lt; len(self.all_chunks) and j not in seen and self.all_chunks[j].doc_id == doc_id:\n                res.append((j, self.all_chunks[j]))\n                seen.add(j)\n    return fc.L(sorted(res)).itemgot(1)\nMuch cleaner! This checks only the immediate prev/current/next chunks for each result.\nnow lets use the qwen reranker\n\n\nü§ñReplyü§ñ\nGreat! For Qwen reranker, you‚Äôll need to:\n\nInstall the model (if using locally) or use an API\nGet initial candidates from BM25 + dense search (more than limit, e.g., limit * 2)\nPass candidates to reranker with the query\nReturn top limit reranked results\n\nAre you planning to use: - Local Qwen reranker (via transformers/sentence-transformers)? - API-based (like through Fireworks or another provider)?\nWhich approach would you prefer?\nYes\n\n\nü§ñReplyü§ñ\nüß†üß†üß†üß†üß†üß†üß†üß†\nFound it! * The Fireworks reranking API uses the endpoint https://api.fireworks.ai/inference/v1/rerank with model fireworks/qwen3-reranker-8b.\nHere‚Äôs how to add the reranker to your HybridSearch class:\nimport requests\n\ndef rerank(self, query, chunks, limit=5):\n    \"\"\"Rerank chunks using Fireworks Qwen3 reranker\"\"\"\n    docs = [c.text for c in chunks]\n    res = requests.post(\n        \"https://api.fireworks.ai/inference/v1/rerank\",\n        json={\n            \"model\": \"fireworks/qwen3-reranker-8b\",\n            \"query\": query,\n            \"documents\": docs,\n            \"top_n\": limit,\n            \"return_documents\": True\n        },\n        headers={\n            \"Authorization\": f\"Bearer {os.environ['FIREWORKS_API_KEY']}\",\n            \"Content-Type\": \"application/json\"\n        }\n    )\n    ranked_idxs = [r['index'] for r in res.json()['results']]\n    return fc.L([chunks[i] for i in ranked_idxs])\nThen update your search method to use it:\ndef search(self, query, limit=5):\n    # Get more candidates than needed\n    candidates = self._get_candidates(query, limit=limit*3)\n    # Rerank and return top results\n    return self.rerank(query, candidates, limit=limit)\nWould you like to integrate this into your class?\nYou can also use the openai spec instead of the request library\n\n\nü§ñReplyü§ñ\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\nGood point! Here‚Äôs the simpler version using the OpenAI client:\ndef rerank(self, query, chunks, limit=5):\n    \"\"\"Rerank chunks using Fireworks Qwen3 reranker\"\"\"\n    docs = [c.text for c in chunks]\n    res = openai_cli.post(\n        \"/rerank\",\n        body={\n            \"model\": \"fireworks/qwen3-reranker-8b\",\n            \"query\": query,\n            \"documents\": docs,\n            \"top_n\": limit\n        },\n        cast_to=dict\n    )\n    ranked_idxs = [r['index'] for r in res['results']]\n    return fc.L([chunks[i] for i in ranked_idxs])\nNote: If the .post() method doesn‚Äôt work directly, you might need to use:\nimport httpx\n\ndef rerank(self, query, chunks, limit=5):\n    docs = [c.text for c in chunks]\n    res = httpx.post(\n        f\"{openai_cli.base_url}rerank\",\n        json={\"model\": \"fireworks/qwen3-reranker-8b\", \"query\": query, \"documents\": docs, \"top_n\": limit},\n        headers={\"Authorization\": f\"Bearer {os.environ['FIREWORKS_API_KEY']}\"}\n    ).json()\n    return fc.L([chunks[r['index']] for r in res['results']])\nWould you like to try one of these approaches?",
    "crumbs": [
      "Initial Development with Solveit"
    ]
  },
  {
    "objectID": "submission_v1.html",
    "href": "submission_v1.html",
    "title": "Submissions",
    "section": "",
    "text": "import braintrust\nimport fastcore.all as fc\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\nfrom wattbot import retriever, eda, evaluate, utils, generator\nfrom wattbot.retriever import chunk_markdown\nfrom langchain_text_splitters import MarkdownTextSplitter\nload_dotenv()\n\nTrue\nembedding_model = 'accounts/fireworks/models/qwen3-embedding-8b'\nllm_model = 'accounts/fireworks/models/kimi-k2p5'\nall_chunks = retriever.chunk_all(retriever.chunk_doc)",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v1.html#with-lexical-search",
    "href": "submission_v1.html#with-lexical-search",
    "title": "Submissions",
    "section": "With Lexical Search",
    "text": "With Lexical Search\n\nls = retriever.LexicalSearch(all_chunks)\nrag = generator.RAG(ls, utils.fw(), model=llm_model)\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'pypdf',\n    'chunking': 'character_level',\n    'chunk_size': 1500,\n    'chunk_step': 1400,\n    'retrieval': 'lexical_search'\n}\n\nevaluate.evaluate_train(rag, experiment_metadata, n_rc=20)\n\nProcessing Rows:   2%|‚ñà‚ñà‚ñà‚ñâ                                                                                                                                                            | 1/41 [00:03&lt;02:12,  3.32s/it]Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [01:46&lt;00:00,  2.60s/it]\n\n\n33.199999999999996\n\n\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\n\n\n\nexperiment_metadata['output_path'] = 'submission_v5.csv'\nevaluate.create_submission(rag, experiment_metadata)\n\nAnswering question:   0%|                                                                                                                                                                    | 0/282 [00:00&lt;?, ?it/s]/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:109: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of US data centers increased by about 4% from 2010-2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer'] = str(answer['answer'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:110: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer_value'] = str(answer['answer_value'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:112: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['wu2021b']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_id'] = str(answer['ref_id'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:113: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['https://arxiv.org/pdf/2108.06738']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_url'] = str(answer['ref_url'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:114: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of the US data centers increased by about 4% from 2010-2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:115: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Chunk 9 from the wu2021b document explicitly states the 4% increase in US data center energy consumption between 2010-2014, which directly answers the question about electricity consumption increase.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'explanation'] = str(answer['explanation'])\nAnswering question: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [10:00&lt;00:00,  2.13s/it]",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v1.html#with-semantic-search",
    "href": "submission_v1.html#with-semantic-search",
    "title": "Submissions",
    "section": "With Semantic Search",
    "text": "With Semantic Search\n\nss = retriever.SemanticSearch(all_chunks, model=embedding_model)\nrag = generator.RAG(ss, utils.fw(), model=llm_model)\n\n\nrag.r.chunks_embeddings[-1].shape\n\n(1927, 4096)\n\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'pypdf',\n    'chunking': 'character_level',\n    'chunk_size': 1500,\n    'chunk_step': 1400,\n    'retrieval': 'semantic_search'   \n}\n\n\nevaluate.evaluate_train(rag, experiment_metadata, n_rc=20)\n\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [02:30&lt;00:00,  3.67s/it]\n\n\n32.574999999999996\n\n\n\nexperiment_metadata['output_path'] = 'submission_v6.csv'\nevaluate.create_submission(rag, experiment_metadata, n_rc=20)\n\nAnswering question:   0%|                                                                                                                                                                    | 0/282 [00:00&lt;?, ?it/s]/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:109: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of U.S. data centers increased by about 4% between 2010 and 2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer'] = str(answer['answer'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:110: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer_value'] = str(answer['answer_value'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:112: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['wu2021b']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_id'] = str(answer['ref_id'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:113: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['https://arxiv.org/pdf/2108.06738']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_url'] = str(answer['ref_url'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:114: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of the US data centers increased by about 4% from 2010-2014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:115: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Chunk 18 from wu2021b explicitly states that U.S. data center energy consumption increased by about 4% between 2010 and 2014, providing the exact percentage increase requested.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'explanation'] = str(answer['explanation'])\nAnswering question: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [17:44&lt;00:00,  3.77s/it]",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "submission_v1.html#hybrid-search",
    "href": "submission_v1.html#hybrid-search",
    "title": "Submissions",
    "section": "Hybrid Search",
    "text": "Hybrid Search\n\nls = retriever.LexicalSearch(all_chunks)\nss = retriever.SemanticSearch(all_chunks, model=embedding_model)\nhs = retriever.HybridSearch(ls, ss)\nrag = generator.RAG(hs, utils.fw(), model=llm_model)\n\n\nexperiment_metadata = {\n    'pdf_extraction': 'pypdf',\n    'chunking': 'character_level',\n    'chunk_size': 1500,\n    'chunk_step': 1400,\n    'retrieval': 'hybrid_search'   \n}\n\n\nevaluate.evaluate_train(rag, experiment_metadata, n_rc=15)\n\nProcessing Rows:   2%|‚ñà‚ñà‚ñà‚ñâ                                                                                                                                                            | 1/41 [00:03&lt;02:37,  3.93s/it]Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nProcessing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [03:53&lt;00:00,  5.69s/it]\n\n\n32.349999999999994\n\n\nSkipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist\nRetrying request after error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\nSleeping for 0.5 seconds\n\n\n\nexperiment_metadata['output_path'] = 'submission_v9.csv'\nevaluate.create_submission(rag, experiment_metadata, n_rc=15)\n\nAnswering question:   0%|                                                                                                                                                                    | 0/282 [00:00&lt;?, ?it/s]/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:109: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The average increase in U.S. data center electricity consumption between 2010 and 2014 was 4%.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer'] = str(answer['answer'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:110: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'answer_value'] = str(answer['answer_value'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:112: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['wu2021b']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_id'] = str(answer['ref_id'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:113: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['https://arxiv.org/pdf/2108.06738']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'ref_url'] = str(answer['ref_url'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:114: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'supporting_materials'] = str(answer['supporting_materials'])\n/Users/anubhavmaity/projects/wattbot/wattbot/evaluate.py:115: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'The provided context explicitly states that U.S. data center electricity consumption increased by about 4% from 2010-2014, citing Masanet et al., 2020. This is the only relevant information found in the retrieved documents addressing the specific time period and geographic scope asked in the question.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df.loc[i, 'explanation'] = str(answer['explanation'])\nAnswering question: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [25:47&lt;00:00,  5.49s/it]",
    "crumbs": [
      "Submissions"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "load_dotenv()\n\nTrue\n\n\n\nfw()\n\n&lt;openai.OpenAI&gt;\n\n\n\nranker = Reranker('accounts/fireworks/models/qwen3-reranker-8b')\nquery = \"India's performance in international sports and current economic outlook\"\n\ndocs = ['India clinches 6th Under-19 World Cup title with a dominant 100-run victory over England', \n        \"RBI keeps repo rate unchanged at 5.25% while maintaining 'neutral' policy stance\",\n        'Vaibhav Sooryavanshi smashes a commanding 175 to lead India‚Äôs victory on the grand stage',\n        'Olympic flame arrives in Milan for 2026 Winter Games featuring star-studded guest list',\n        'Singapore woman fined ‚Çπ2 lakh for \"one bad habit\"‚Äîhere is why']\n\n\nReranker().rank(query, docs)\n\n(#5) [0,2,1,3,4]",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA",
    "section": "",
    "text": "I participitated in this competition few months back and was ranked 5th in the Private Leaderboard and 10th in the Public Leaderboard. There were 182 entrants\n\n\n\nLeaderboard Ranking\n\n\n\nLooking at data\nThere are 3 files shared in the competition - metadata.csv - test_Q.csv - train_QA.csv\nI have created a folder data and have downloaded all the 3 files into it\n\nmetadata().head()\n\n\n\n\n\n\n\n\nid\ntype\ntitle\nyear\ncitation\nurl\n\n\n\n\n0\namazon2023\nreport\n2023 Amazon Sustainability Report\n2023\nAmazon Staff. (2023). Amazon Sustainability Re...\nhttps://sustainability.aboutamazon.com/2023-am...\n\n\n1\nchen2024\npaper\nEfficient Heterogeneous Large Language Model D...\n2024\nShaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...\nhttps://arxiv.org/pdf/2405.01814\n\n\n2\nchung2025\npaper\nThe ML.ENERGY Benchmark: Toward Automated Infe...\n2025\nJae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...\nhttps://arxiv.org/pdf/2505.06371\n\n\n3\ncottier2024\npaper\nThe Rising Costs of Training Frontier AI Models\n2024\nBen Cottier, Robi Rahman, Loredana Fattorini, ...\nhttps://arxiv.org/pdf/2405.21015\n\n\n4\ndodge2022\npaper\nMeasuring the Carbon Intensity of AI in Cloud ...\n2022\nJesse Dodge, Taylor Prewitt, Remi Tachet Des C...\nhttps://arxiv.org/pdf/2206.05229\n\n\n\n\n\n\n\n\nlen(metadata())\n\n32\n\n\nThere are 32 files\n\ntrain().head()\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq003\nWhat is the name of the benchmark suite presen...\nThe ML.ENERGY Benchmark\nML.ENERGY Benchmark\nis_blank\n['chung2025']\n['https://arxiv.org/pdf/2505.06371']\nWe present the ML.ENERGY Benchmark, a benchmar...\nQuote\n\n\n1\nq009\nWhat were the net CO2e emissions from training...\n4.3 tCO2e\n4.3\ntCO2e\n['patterson2021']\n['https://arxiv.org/pdf/2104.10350']\n\"Training GShard-600B used 24 MWh and produced...\nQuote\n\n\n2\nq054\nWhat is the model size in gigabytes (GB) for t...\n64.7 GB\n64.7\nGB\n['chen2024']\n['https://arxiv.org/pdf/2405.01814']\nTable 3: Large language models used for evalua...\nTable 3\n\n\n3\nq062\nWhat was the total electricity consumption of ...\nUnable to answer with confidence based on the ...\nis_blank\nMWh\nis_blank\nis_blank\nis_blank\nis_blank\n\n\n4\nq075\nTrue or False: Hyperscale data centers in 2020...\nTRUE\n1\nis_blank\n['wu2021b','patterson2021']\n['https://arxiv.org/abs/2108.06738','https://a...\nWu 2021, body text near Fig. 1: \"‚Ä¶between trad...\nThe &gt;40% statement is explicit in Wu. Patterso...\n\n\n\n\n\n\n\n\nget_train_data()\n\nnamespace(id='q166',\n          question='Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?',\n          answer='GPT-3',\n          answer_value='GPT-3',\n          answer_unit='is_blank',\n          ref_id=\"['patterson2021']\",\n          ref_url=\"['https://arxiv.org/pdf/2104.10350']\",\n          supporting_materials='Figure 3',\n          explanation='Figure')\n\n\n\nget_train_data(0)\n\nnamespace(id='q003',\n          question='What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?',\n          answer='The ML.ENERGY Benchmark',\n          answer_value='ML.ENERGY Benchmark',\n          answer_unit='is_blank',\n          ref_id=\"['chung2025']\",\n          ref_url=\"['https://arxiv.org/pdf/2505.06371']\",\n          supporting_materials='We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments...',\n          explanation='Quote')\n\n\n\nget_value(train().iloc[0])\n\nnamespace(id='q003',\n          question='What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?',\n          answer='The ML.ENERGY Benchmark',\n          answer_value='ML.ENERGY Benchmark',\n          answer_unit='is_blank',\n          ref_id=\"['chung2025']\",\n          ref_url=\"['https://arxiv.org/pdf/2505.06371']\",\n          supporting_materials='We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments...',\n          explanation='Quote')\n\n\n\ntest().head()\n\n\n\n\n\n\n\n\nid\nquestion\nanswer\nanswer_value\nanswer_unit\nref_id\nref_url\nsupporting_materials\nexplanation\n\n\n\n\n0\nq001\nWhat was the average increase in U.S. data cen...\nNaN\nNaN\npercent\nNaN\nNaN\nNaN\nNaN\n\n\n1\nq002\nIn 2023, what was the estimated amount of cars...\nNaN\nNaN\ncars\nNaN\nNaN\nNaN\nNaN\n\n\n2\nq004\nHow many data centers did AWS begin using recy...\nNaN\nNaN\ndata centers\nNaN\nNaN\nNaN\nNaN\n\n\n3\nq005\nSince NVIDIA doesn't release the embodied carb...\nNaN\nNaN\nkg/GPU\nNaN\nNaN\nNaN\nNaN\n\n\n4\nq006\nBy what factor was the estimated amortized tra...\nNaN\nNaN\nratio\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nHow many papers and reports?\n\nmetadata()['type'].hist()\n\n\n\n\n\n\n\n\n\nmetadata()[metadata()['type'] == 'report']\n\n\n\n\n\n\n\n\nid\ntype\ntitle\nyear\ncitation\nurl\n\n\n\n\n0\namazon2023\nreport\n2023 Amazon Sustainability Report\n2023\nAmazon Staff. (2023). Amazon Sustainability Re...\nhttps://sustainability.aboutamazon.com/2023-am...\n\n\n\n\n\n\n\n\n\nHow is the distribution of arxiv?\n\nis_arxiv(metadata().iloc[0]['url'])\n\nFalse\n\n\n\nCounter(metadata()['url'].map(lambda x: is_arxiv(x)))\n\nCounter({True: 31, False: 1})\n\n\nMost of them are arxiv\n\n\nHow is the distribution of title length?\n\nmetadata()['title'].map(lambda x: len(x)).hist()\n\n\n\n\n\n\n\n\n\n\nDownload the pdf\n\nmd = metadata()\n\n\nurl = md.iloc[0].url\ndownload_pdf(url)\n\nPath('../data/2023-amazon-sustainability-report.pdf')\n\n\n\ndownload_pdf('https://arxiv.org/pdf/2405.01814v2')\n\nPath('../data/2405.01814v2.pdf')\n\n\n\nmd['filepath'] = md['url'].map(download_pdf)\n\n\n\nRead the pdfs into dataframe\n\nmd['content'] = md['filepath'].map(rd.read_pdf)\n\n\nmd.head()\n\n\n\n\n\n\n\n\nid\ntype\ntitle\nyear\ncitation\nurl\nfilepath\ncontent\n\n\n\n\n0\namazon2023\nreport\n2023 Amazon Sustainability Report\n2023\nAmazon Staff. (2023). Amazon Sustainability Re...\nhttps://sustainability.aboutamazon.com/2023-am...\n../data/2023-amazon-sustainability-report.pdf\nAmazon \\nSustainability \\nReport\\n2023 Content...\n\n\n1\nchen2024\npaper\nEfficient Heterogeneous Large Language Model D...\n2024\nShaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...\nhttps://arxiv.org/pdf/2405.01814\n../data/2405.01814.pdf\nEfficient Heterogeneous Large Language Model D...\n\n\n2\nchung2025\npaper\nThe ML.ENERGY Benchmark: Toward Automated Infe...\n2025\nJae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...\nhttps://arxiv.org/pdf/2505.06371\n../data/2505.06371.pdf\nThe ML.ENERGY Benchmark: Toward Automated\\nInf...\n\n\n3\ncottier2024\npaper\nThe Rising Costs of Training Frontier AI Models\n2024\nBen Cottier, Robi Rahman, Loredana Fattorini, ...\nhttps://arxiv.org/pdf/2405.21015\n../data/2405.21015.pdf\nTHE RISING COSTS OF TRAINING FRONTIER AI MODEL...\n\n\n4\ndodge2022\npaper\nMeasuring the Carbon Intensity of AI in Cloud ...\n2022\nJesse Dodge, Taylor Prewitt, Remi Tachet Des C...\nhttps://arxiv.org/pdf/2206.05229\n../data/2206.05229.pdf\nMeasuring the Carbon Intensity of AI in Cloud ...\n\n\n\n\n\n\n\n\nmd['content_length'] = md['content'].map(lambda x: len(x))\n\n\nmd['content_length'].hist()\n\n\n\n\n\n\n\n\n\n\nContent Tokens\n\nencoding = tiktoken.get_encoding(\"cl100k_base\")\n\n\nmsg = md.iloc[0]['content']\ncompute_tokens(msg)\n\n95785\n\n\n\nmd['token_count'] = md['content'].map(compute_tokens)\n\n\nmd['token_count'].hist()\n\n\n\n\n\n\n\n\n\n\nSave metadata file\n\nmd.to_csv(data_path/'complete_metadata.csv', index=False)\n\n\n\nCreate submission file\n\ndef create_submission(answers_list, output_path='submission.csv'):\n    df = test()\n    \n    for i, answer in enumerate(answers_list):\n        df.loc[i, 'answer'] = answer['answer']\n        df.loc[i, 'answer_value'] = answer['answer_value']\n        df.loc[i, 'answer_unit'] = answer['answer_unit']\n        df.loc[i, 'ref_id'] = answer['ref_id']\n        df.loc[i, 'ref_url'] = answer['ref_url']\n        df.loc[i, 'supporting_materials'] = answer['supporting_materials']\n        df.loc[i, 'explanation'] = answer['explanation']\n    \n    df = df.fillna('is_blank')\n    df.to_csv(output_path, index=False)",
    "crumbs": [
      "EDA"
    ]
  },
  {
    "objectID": "generator.html",
    "href": "generator.html",
    "title": "Generator",
    "section": "",
    "text": "load_dotenv()\n\nTrue",
    "crumbs": [
      "Generator"
    ]
  },
  {
    "objectID": "generator.html#prompt",
    "href": "generator.html#prompt",
    "title": "Generator",
    "section": "Prompt",
    "text": "Prompt\nI have saved a prompt in braintrust via its UI.\nIn the following code I am extracting it\n\nbraintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt-v2\")\n\n&lt;braintrust.logger.Prompt&gt;\n\n\n\nPrompt Template\n\nbraintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt-v2\").template()[:100]\n\n\"You are analyzing academic papers about AI's environmental impact. Answer the question using ONLY th\"\n\n\n\n\nSystem Prompt\n\nsys_prompt = braintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt-v2\").system_prompt()\nsys_prompt[:100]\n\n'You are an expert research assistant specializing in AI environmental impact analysis. Your role is '\n\n\n\n\nFill Template\n\nall_chunks = retriever.chunk_all(retriever.chunk_doc)\ncontext = create_context(all_chunks[:2])\nprint(context[:100])\n\n### Chunk 1\n            Text: Amazon \nSustainability \nReport\n2023 Contents\nOverview\n3 Introduction\n4\n\n\n\nquery = eda.get_train_data().question\nquery\n\n\"True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\"\n\n\n\nprint(braintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt-v2\").fill_template(query, all_chunks[:2])[:100])\n\nYou are analyzing academic papers about AI's environmental impact. Answer the question using ONLY th",
    "crumbs": [
      "Generator"
    ]
  },
  {
    "objectID": "generator.html#llm",
    "href": "generator.html#llm",
    "title": "Generator",
    "section": "LLM",
    "text": "LLM\n\nllm_model = 'accounts/fireworks/models/gpt-oss-20b'\n\n\nls = retriever.LexicalSearch(all_chunks)\n\n\nrc = ls.search(query)\nquery, rc[0].text\n\n(\"True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\",\n 'to   a   better   understanding   of   cost-accuracy   tradeoffs   \\nin   ML   models,   potentially   further   reducing   overall   emissions   by   empowering   more   informed   ML   model   \\nselection,   as   the   next   subsection   explains.   \\n    \\nFigure   4:   Reproduction   of   Figure   4   from   So    et   al.    Dots   on   the   blue   line   represent   various   sizes   of   plain   \\nTransformer   NLP   models,   while   dots   on   the   red   line   represent   various   sizes   of   the   open-sourced   \\nEvolved   Transformer   architecture   that   was   discovered   by   the   neural   architecture   search   run   in   [So19] .   \\nRed   arrows   are   at   131M   and   210M   parameters   and   show   that   an   Evolved   Transformer   can   achieve   \\nhigher   accuracy   at   less   cost:   it   runs   1.3X   faster   and   produces   1.3x   less   CO 2 e.     \\n4.2   There   are   more   resources   used   for   training   than   the   only   final   training   run  \\xa0\\n[Str19]   and   others   point   out   that   it   often   takes   many   attempts   to   get   everything   set   up   correctly   before   the   \\nfinal   training   run,   so   the   final   training   run   does   not   reflect   the   total   cost.   Since   it‚Äôs   hard   to   improve   what   you   \\ncan‚Äôt   measure,   one   issue   is   how   to   account   for   such   costs   accurately.   Fortunately,   an   internal   Google   product   \\nis   underway   that   will   record   i')\n\n\n\nprompt_msg = braintrust.load_prompt(project=\"wattbot\", slug=\"wattbot-prompt-v2\").fill_template(query, rc)\n\n\nmk_msg(\"how many r in strawberry?\", \"You are a helpful assistant that provides concise and professional answers.\")\n\n[{'role': 'system',\n  'content': 'You are a helpful assistant that provides concise and professional answers.'},\n {'role': 'user', 'content': 'how many r in strawberry?'}]\n\n\n\nutils.fw()\n\n&lt;openai.OpenAI&gt;\n\n\n\nutils.fw().generate(llm_model, \"how many r in strawberry?\", \"You are a helpful assistant that provides concise and professional answers.\")\n\n'{\"count\":3}'\n\n\n\nfc.loads(utils.fw().generate(llm_model, prompt_msg, sys_prompt))\n\n{'answer': 'FALSE',\n 'answer_value': '0',\n 'answer_unit': 'is_blank',\n 'ref_id': ['patterson2021'],\n 'ref_url': ['https://arxiv.org/pdf/2104.10350'],\n 'supporting_materials': '\"Red arrows are at 131M and 210M parameters and show that an Evolved Transformer can achieve higher accuracy at less cost\"',\n 'explanation': \"The provided excerpts state that the Evolved Transformer consistently delivers higher accuracy (than the plain Transformer) while using fewer resources; there is no evidence that the plain Transformer later surpasses it in the WMT'24 EN‚ÄëDE BLEU task as model size grows.\"}\n\n\n\ndefault_answer()\n\n{'answer': 'is_blank',\n 'answer_value': 'is_blank',\n 'answer_unit': 'is_blank',\n 'ref_id': 'is_blank',\n 'ref_url': 'is_blank',\n 'supporting_materials': 'is_blank',\n 'explanation': 'is_blank'}\n\n\n\nanswer = {'answer_value': '40',\n 'answer_unit': 'is_blank',\n 'ref_id': ['372'],\n 'ref_url': ['https://arxiv.org/pdf/2505.06371']}\nreplace_missing_keys(answer)\n\n{'answer': 'is_blank',\n 'answer_value': '40',\n 'answer_unit': 'is_blank',\n 'ref_id': ['372'],\n 'ref_url': ['https://arxiv.org/pdf/2505.06371'],\n 'supporting_materials': 'is_blank',\n 'explanation': 'is_blank'}\n\n\n\npost_process(answer)\n\n{'answer': 'is_blank',\n 'answer_value': '40',\n 'answer_unit': 'is_blank',\n 'ref_id': ['372'],\n 'ref_url': ['https://arxiv.org/pdf/2505.06371'],\n 'supporting_materials': 'is_blank',\n 'explanation': 'is_blank'}\n\n\n\nrag = RAG(ls, utils.fw())\nresponse = rag.answer(query)\nresponse.ans\n\n{'answer': 'FALSE',\n 'answer_value': '0',\n 'answer_unit': 'is_blank',\n 'ref_id': ['patterson2021'],\n 'ref_url': ['https://arxiv.org/pdf/2104.10350'],\n 'supporting_materials': '\"Figure\\xa04 shows that the Evolved Transformer, found by NAS [So19], has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure (see Table\\xa01) than the vanilla Transformer (Big) model on WMT English to German translation.\"',\n 'explanation': 'The document reports that the Evolved Transformer achieves the same accuracy as the vanilla Transformer with fewer parameters and lower energy, but does not indicate that larger plain Transformers eventually outperform the Evolved Transformer on the WMT EN‚ÄëDE BLEU task. Hence the claim is unsupported and therefore false.'}\n\n\n\nlen(response.rc), response.rc[0]\n\n(10,\n namespace(text='to   a   better   understanding   of   cost-accuracy   tradeoffs   \\nin   ML   models,   potentially   further   reducing   overall   emissions   by   empowering   more   informed   ML   model   \\nselection,   as   the   next   subsection   explains.   \\n    \\nFigure   4:   Reproduction   of   Figure   4   from   So    et   al.    Dots   on   the   blue   line   represent   various   sizes   of   plain   \\nTransformer   NLP   models,   while   dots   on   the   red   line   represent   various   sizes   of   the   open-sourced   \\nEvolved   Transformer   architecture   that   was   discovered   by   the   neural   architecture   search   run   in   [So19] .   \\nRed   arrows   are   at   131M   and   210M   parameters   and   show   that   an   Evolved   Transformer   can   achieve   \\nhigher   accuracy   at   less   cost:   it   runs   1.3X   faster   and   produces   1.3x   less   CO 2 e.     \\n4.2   There   are   more   resources   used   for   training   than   the   only   final   training   run  \\xa0\\n[Str19]   and   others   point   out   that   it   often   takes   many   attempts   to   get   everything   set   up   correctly   before   the   \\nfinal   training   run,   so   the   final   training   run   does   not   reflect   the   total   cost.   Since   it‚Äôs   hard   to   improve   what   you   \\ncan‚Äôt   measure,   one   issue   is   how   to   account   for   such   costs   accurately.   Fortunately,   an   internal   Google   product   \\nis   underway   that   will   record   i',\n           chunk_id=1374,\n           id='patterson2021',\n           type='paper',\n           title='Carbon Emissions and Large Neural Network Training',\n           year=2021,\n           citation='David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, Jeff Dean (2021). Carbon Emissions and Large Neural Network Training. arXiv. https://arxiv.org/pdf/2104.10350',\n           url='https://arxiv.org/pdf/2104.10350'))\n\n\n\nlen(response.pm)\n\n22465\n\n\n\nrag.answer(query, json=False).ans\n\n'{\"answer\":\"False\",\"answer_value\":\"0\",\"answer_unit\":\"is_blank\",\"ref_id\":[\"patterson2021\"],\"ref_url\":[\"https://arxiv.org/pdf/2104.10350\"],\"supporting_materials\":\"\\\\\"Red arrows are at 131M and 210M parameters and show that an Evolved Transformer can achieve higher accuracy at less cost: it runs 1.3X faster and produces 1.3x less CO 2 e.\\\\\"\",\"explanation\":\"The excerpts state that at the examined model sizes (131M and 210M) the Evolved Transformer achieves higher accuracy than the plain Transformer. No evidence is provided that the plain Transformer eventually surpasses it as model size increases; thus the statement is false.\"}'\n\n\n\nrag.answer(eda.get_train_data().question).ans\n\n{'answer': 'The estimated CO2 emissions are 1,438\\u202flbs.',\n 'answer_value': '1438',\n 'answer_unit': 'lbs',\n 'ref_id': ['dodge2022'],\n 'ref_url': ['https://arxiv.org/pdf/2206.05229'],\n 'supporting_materials': '\"BERTbase V100x64 ... 79 1507 1438\"',\n 'explanation': 'In Table\\u202f3 of the paper, the entry for BERT base trained on 64 V100 GPUs for 79\\u202fh lists a CO2e of 1,438\\u202flbs.'}\n\n\n\nls = retriever.LexicalSearch(all_chunks, neighbour_chunks=True)\nrag = RAG(ls, utils.fw())\nlen(rag.answer(query).rc)\n\n26",
    "crumbs": [
      "Generator"
    ]
  }
]
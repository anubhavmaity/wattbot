<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Initial Development with Solveit ‚Äì wattbot</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-baedc78cbf92349237790bf011c153e8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Initial Development with Solveit ‚Äì wattbot">
<meta property="og:description" content="This repository is based on a kaggle competition named Wattbot">
<meta property="og:site_name" content="wattbot">
<meta name="twitter:title" content="Initial Development with Solveit ‚Äì wattbot">
<meta name="twitter:description" content="This repository is based on a kaggle competition named Wattbot">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">wattbot</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./wattbot_sdk_solveit.html">Initial Development with Solveit</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">WattBot 2025 - RAG System for Technical Document Q&amp;A</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">EDA</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./retriever.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Retriever</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generator.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generator</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./evaluate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Eval</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./submission_v1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Submissions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./submission_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Submissions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./submission_v3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Submissions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./utils.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Utils</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./wattbot_exploration_solveit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exploration with Solveit</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./wattbot_sdk_solveit.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Initial Development with Solveit</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#defining-imports" id="toc-defining-imports" class="nav-link active" data-scroll-target="#defining-imports">Defining imports</a></li>
  <li><a href="#loading-data-files-related-to-competition" id="toc-loading-data-files-related-to-competition" class="nav-link" data-scroll-target="#loading-data-files-related-to-competition">Loading data files (related to competition)</a></li>
  <li><a href="#document-processor" id="toc-document-processor" class="nav-link" data-scroll-target="#document-processor">Document Processor</a></li>
  <li><a href="#embedding" id="toc-embedding" class="nav-link" data-scroll-target="#embedding">Embedding</a></li>
  <li><a href="#qdrant" id="toc-qdrant" class="nav-link" data-scroll-target="#qdrant">Qdrant</a></li>
  <li><a href="#retriever" id="toc-retriever" class="nav-link" data-scroll-target="#retriever">Retriever</a></li>
  <li><a href="#generator" id="toc-generator" class="nav-link" data-scroll-target="#generator">Generator</a></li>
  <li><a href="#rag" id="toc-rag" class="nav-link" data-scroll-target="#rag">RAG</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation">Evaluation</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/anubhavmaity/wattbot/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="wattbot_sdk_solveit.html.md"><i class="bi bi-file-code"></i>CommonMark</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Initial Development with Solveit</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="defining-imports" class="level2">
<h2 class="anchored" data-anchor-id="defining-imports">Defining imports</h2>
<div id="609746a6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> contextkit.read <span class="im">as</span> rd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fireworks <span class="im">as</span> fw</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> braintrust</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> types <span class="im">import</span> SimpleNamespace</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> qdrant_client <span class="im">import</span> QdrantClient, models</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> qdrant_client.models <span class="im">import</span> Distance, VectorParams, PointStruct</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastembed <span class="im">import</span> SparseTextEmbedding</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm</code></pre>
</div>
</div>
<div id="baf47deb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>em <span class="op">=</span> <span class="st">"nomic-ai/nomic-embed-text-v1.5"</span> <span class="co"># embedding model</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># em = "accounts/fireworks/models/qwen3-embedding-8b"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="loading-data-files-related-to-competition" class="level2">
<h2 class="anchored" data-anchor-id="loading-data-files-related-to-competition">Loading data files (related to competition)</h2>
<div id="6e6b9660" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>data_path <span class="op">=</span> fc.Path(<span class="st">'wattbot_data'</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>metadata <span class="op">=</span> pd.read_csv(data_path<span class="op">/</span><span class="st">'metadata.csv'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>, dtype<span class="op">=</span>{<span class="st">'year'</span>: <span class="bu">object</span>})</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>qa <span class="op">=</span> pd.read_csv(data_path<span class="op">/</span><span class="st">'train_QA.csv'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>tst <span class="op">=</span> pd.read_csv(data_path<span class="op">/</span><span class="st">'test_Q.csv'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9dabbb4d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>metadata.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">id</th>
<th data-quarto-table-cell-role="th">type</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">citation</th>
<th data-quarto-table-cell-role="th">url</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>amazon2023</td>
<td>report</td>
<td>2023 Amazon Sustainability Report</td>
<td>2023</td>
<td>Amazon Staff. (2023). Amazon Sustainability Re...</td>
<td>https://sustainability.aboutamazon.com/2023-am...</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>chen2024</td>
<td>paper</td>
<td>Efficient Heterogeneous Large Language Model D...</td>
<td>2024</td>
<td>Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...</td>
<td>https://arxiv.org/pdf/2405.01814</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>chung2025</td>
<td>paper</td>
<td>The ML.ENERGY Benchmark: Toward Automated Infe...</td>
<td>2025</td>
<td>Jae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...</td>
<td>https://arxiv.org/pdf/2505.06371</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>cottier2024</td>
<td>paper</td>
<td>The Rising Costs of Training Frontier AI Models</td>
<td>2024</td>
<td>Ben Cottier, Robi Rahman, Loredana Fattorini, ...</td>
<td>https://arxiv.org/pdf/2405.21015</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>dodge2022</td>
<td>paper</td>
<td>Measuring the Carbon Intensity of AI in Cloud ...</td>
<td>2022</td>
<td>Jesse Dodge, Taylor Prewitt, Remi Tachet Des C...</td>
<td>https://arxiv.org/pdf/2206.05229</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="0c4dafad" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>metadata.dtypes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>id          object
type        object
title       object
year        object
citation    object
url         object
dtype: object</code></pre>
</div>
</div>
<p>How to assign dtype object for column year when reading csv file?</p>
<section id="reply" class="level5">
<h5 class="anchored" data-anchor-id="reply">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You can specify the data type when reading the CSV file using the <code>dtype</code> parameter in <code>pd.read_csv()</code>. Here‚Äôs how:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>metadata <span class="op">=</span> pd.read_csv(data_path<span class="op">/</span><span class="st">'metadata.csv'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>, dtype<span class="op">=</span>{<span class="st">'year'</span>: <span class="bu">object</span>})</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This tells pandas to read the ‚Äòyear‚Äô column as an object (string) type instead of trying to infer it as an integer.</p>
<p>Would you like to reload your metadata with this change, or do you have a specific reason for wanting ‚Äòyear‚Äô as an object type?</p>
</section>
</section>
<section id="document-processor" class="level2">
<h2 class="anchored" data-anchor-id="document-processor">Document Processor</h2>
<div id="4246fb11" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Chunk(fc.NS):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chunk_id, doc_id, text, <span class="op">**</span>metadata):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(chunk_id<span class="op">=</span>chunk_id, doc_id<span class="op">=</span>doc_id, text<span class="op">=</span>text, <span class="op">**</span>metadata)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        meta <span class="op">=</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join([<span class="ss">f"</span><span class="ch">\033</span><span class="ss">[1;35m</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">\033</span><span class="ss">[0m: </span><span class="sc">{</span><span class="bu">str</span>(v)[:<span class="dv">100</span>]<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.__dict__.items()])</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"</span><span class="sc">{</span>meta<span class="sc">}</span><span class="ss">"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="35dcddbc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Chunk(<span class="st">'WattBot'</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="op">**</span>{<span class="st">'title'</span>: <span class="st">'RAG'</span>})</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-magenta-fg ansi-bold">chunk_id</span>: WattBot
<span class="ansi-magenta-fg ansi-bold">doc_id</span>: 0
<span class="ansi-magenta-fg ansi-bold">text</span>: 0
<span class="ansi-magenta-fg ansi-bold">title</span>: RAG</pre>
</div>
</div>
</div>
<p>How to make the keys bold?</p>
<section id="reply-1" class="level5">
<h5 class="anchored" data-anchor-id="reply-1">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>To make the keys bold in the <code>__repr__</code> output, you can wrap them in ANSI escape codes or markdown-style formatting. Here‚Äôs a simple approach using ANSI codes for terminal output:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    meta <span class="op">=</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>.join([<span class="ss">f"</span><span class="ch">\033</span><span class="ss">[1m</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">\033</span><span class="ss">[0m: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.__dict__.items() <span class="cf">if</span> k <span class="op">!=</span> <span class="st">'__stored_args__'</span>])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f"</span><span class="sc">{</span>meta<span class="sc">}</span><span class="ss">"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>\033[1m</code> starts bold text and <code>\033[0m</code> resets it.</p>
<p>Would you like to try this, or would you prefer a different formatting approach?</p>
<p>Can it be colorize?</p>
</section>
<section id="reply-2" class="level5">
<h5 class="anchored" data-anchor-id="reply-2">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes! You can use different ANSI color codes. For example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ss">f"</span><span class="ch">\033</span><span class="ss">[1;34m</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">\033</span><span class="ss">[0m: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Bold blue</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ss">f"</span><span class="ch">\033</span><span class="ss">[1;32m</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">\033</span><span class="ss">[0m: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Bold green</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="ss">f"</span><span class="ch">\033</span><span class="ss">[1;35m</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">\033</span><span class="ss">[0m: </span><span class="sc">{</span>v<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Bold magenta</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The format is <code>\033[1;COLORm</code> where common color codes are: - 31: red - 32: green - 33: yellow - 34: blue - 35: magenta - 36: cyan</p>
<p>Which color would you like to try?</p>
<div id="29af53bc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastcore.<span class="bu">all</span> <span class="im">as</span> fc</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> contextkit.read <span class="im">as</span> rd</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DocumentProcessor:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data_path<span class="op">=</span><span class="st">'wattbot_data'</span>, chunk_size<span class="op">=</span><span class="dv">1500</span>, step<span class="op">=</span><span class="dv">1400</span>):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        fc.store_attr()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> stream(<span class="va">self</span>, doc_id, metadata<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        content <span class="op">=</span> rd.read_pdf(fc.Path(<span class="va">self</span>.data_path)<span class="op">/</span><span class="ss">f'</span><span class="sc">{</span>doc_id<span class="sc">}</span><span class="ss">.pdf'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(content), <span class="va">self</span>.step): <span class="cf">yield</span> Chunk(text<span class="op">=</span>content[i:i<span class="op">+</span><span class="va">self</span>.chunk_size], chunk_id<span class="op">=</span>i, doc_id<span class="op">=</span>doc_id, <span class="op">**</span>(metadata <span class="kw">or</span> {}))</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process(<span class="va">self</span>, doc_id, metadata<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> fc.L(<span class="va">self</span>.stream(doc_id, metadata<span class="op">=</span>metadata))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="90bf0f22" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>doc_id, doc_type, doc_url, doc_title, doc_cite, doc_year <span class="op">=</span> fc.getattrs(metadata.iloc[<span class="dv">0</span>], <span class="st">'id'</span>, <span class="st">'type'</span>, <span class="st">'url'</span>, <span class="st">'title'</span>, <span class="st">'citation'</span>, <span class="st">'year'</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>doc_proc <span class="op">=</span> DocumentProcessor()</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>extras <span class="op">=</span> {<span class="st">'type'</span>: doc_type, <span class="st">'url'</span>: doc_url, <span class="st">'title'</span>: doc_title, <span class="st">'citation'</span>: doc_cite, <span class="st">'year'</span>: doc_year}</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>chunks <span class="op">=</span> fc.L(doc_proc.process(doc_id, metadata<span class="op">=</span>extras))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>chunks[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-magenta-fg ansi-bold">chunk_id</span>: 0
<span class="ansi-magenta-fg ansi-bold">doc_id</span>: amazon2023
<span class="ansi-magenta-fg ansi-bold">text</span>: Amazon 
Sustainability 
Report
2023 Contents
Overview
3 Introduction
4 A Letter from Our Chief 
Sust
<span class="ansi-magenta-fg ansi-bold">type</span>: report
<span class="ansi-magenta-fg ansi-bold">url</span>: https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf
<span class="ansi-magenta-fg ansi-bold">title</span>: 2023 Amazon Sustainability Report
<span class="ansi-magenta-fg ansi-bold">citation</span>: Amazon Staff. (2023). Amazon Sustainability Report. https://sustainability.aboutamazon.com/2023-amaz
<span class="ansi-magenta-fg ansi-bold">year</span>: 2023</pre>
</div>
</div>
</div>
<div id="c53c6af2" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_docs(data_path<span class="op">=</span><span class="st">'wattbot_data'</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    metadata <span class="op">=</span> pd.read_csv(fc.Path(data_path)<span class="op">/</span><span class="st">'metadata.csv'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    doc_proc <span class="op">=</span> DocumentProcessor()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    all_chunks <span class="op">=</span> metadata.<span class="bu">apply</span>(<span class="kw">lambda</span> r: doc_proc.process(r[<span class="st">'id'</span>], metadata<span class="op">=</span>fc.attrdict(r, <span class="st">'type'</span>, <span class="st">'url'</span>, <span class="st">'title'</span>, <span class="st">'citation'</span>, <span class="st">'year'</span>)), axis<span class="op">=</span><span class="dv">1</span>).tolist()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fc.L(all_chunks).concat()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="35bf5b5c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>all_chunks <span class="op">=</span> process_docs()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>all_chunks[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-magenta-fg ansi-bold">chunk_id</span>: 0
<span class="ansi-magenta-fg ansi-bold">doc_id</span>: amazon2023
<span class="ansi-magenta-fg ansi-bold">text</span>: Amazon 
Sustainability 
Report
2023 Contents
Overview
3 Introduction
4 A Letter from Our Chief 
Sust
<span class="ansi-magenta-fg ansi-bold">type</span>: report
<span class="ansi-magenta-fg ansi-bold">url</span>: https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf
<span class="ansi-magenta-fg ansi-bold">title</span>: 2023 Amazon Sustainability Report
<span class="ansi-magenta-fg ansi-bold">citation</span>: Amazon Staff. (2023). Amazon Sustainability Report. https://sustainability.aboutamazon.com/2023-amaz
<span class="ansi-magenta-fg ansi-bold">year</span>: 2023</pre>
</div>
</div>
</div>
<p>How to save all the chunks so that I dont generate again?</p>
</section>
<section id="reply-3" class="level5">
<h5 class="anchored" data-anchor-id="reply-3">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You have several options for saving the chunks:</p>
<ol type="1">
<li><strong>Pickle</strong> - Fast and preserves Python objects:</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>fc.save_pickle(<span class="st">'all_chunks.pkl'</span>, all_chunks)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol start="2" type="1">
<li><strong>JSON</strong> - More portable and readable:</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'all_chunks.json'</span>, <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    json.dump([chunk.__dict__ <span class="cf">for</span> chunk <span class="kw">in</span> all_chunks], f)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol start="3" type="1">
<li><strong>Parquet</strong> - Efficient for large datasets:</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame([chunk.__dict__ <span class="cf">for</span> chunk <span class="kw">in</span> all_chunks]).to_parquet(<span class="st">'all_chunks.parquet'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Which format would you prefer, or would you like to know more about the tradeoffs?</p>
<p>How to load the chunks?</p>
</section>
<section id="reply-4" class="level5">
<h5 class="anchored" data-anchor-id="reply-4">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>For each saving method, here‚Äôs how to load:</p>
<ol type="1">
<li><strong>Pickle</strong>:</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>all_chunks <span class="op">=</span> fc.load_pickle(<span class="st">'all_chunks.pkl'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol start="2" type="1">
<li><strong>JSON</strong>:</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'all_chunks.json'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    all_chunks <span class="op">=</span> fc.L([Chunk(<span class="op">**</span>d) <span class="cf">for</span> d <span class="kw">in</span> json.load(f)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ol start="3" type="1">
<li><strong>Parquet</strong>:</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>all_chunks <span class="op">=</span> fc.L([Chunk(<span class="op">**</span>row) <span class="cf">for</span> row <span class="kw">in</span> pd.read_parquet(<span class="st">'all_chunks.parquet'</span>).to_dict(<span class="st">'records'</span>)])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Which format did you use to save them?</p>
</section>
</section>
<section id="embedding" class="level2">
<h2 class="anchored" data-anchor-id="embedding">Embedding</h2>
<div id="f3d4c5c1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>sparse_model <span class="op">=</span> SparseTextEmbedding(model_name<span class="op">=</span><span class="st">"Qdrant/bm25"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Fetching 18 files:   0%|          | 0/18 [00:00&lt;?, ?it/s]Fetching 18 files:   6%|‚ñå         | 1/18 [00:00&lt;00:07,  2.34it/s]Fetching 18 files:  11%|‚ñà         | 2/18 [00:00&lt;00:03,  4.22it/s]Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00&lt;00:00, 39.63it/s]Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00&lt;00:00, 26.24it/s]</code></pre>
</div>
</div>
<div id="b89b6ff3" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Embedder:</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dclient, dmodel, smodel<span class="op">=</span><span class="va">None</span>, bs<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        fc.store_attr()</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_dembs(<span class="va">self</span>, texts):</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> fc.mapped(<span class="kw">lambda</span> o: <span class="va">self</span>.dclient.embeddings.create(model<span class="op">=</span><span class="va">self</span>.dmodel, <span class="bu">input</span><span class="op">=</span>o), fc.chunked(texts, chunk_sz<span class="op">=</span><span class="va">self</span>.bs)).attrgot(<span class="st">'data'</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> data.concat().attrgot(<span class="st">'embedding'</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_sembs(<span class="va">self</span>, texts):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.smodel: <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Sparse model not initialized"</span>)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">list</span>(<span class="va">self</span>.smodel.embed(texts))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="4586f8ec" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>openai_cli <span class="op">=</span> openai.OpenAI(base_url <span class="op">=</span> <span class="st">"https://api.fireworks.ai/inference/v1"</span>, api_key<span class="op">=</span>os.environ[<span class="st">'FIREWORKS_API_KEY'</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>embedder <span class="op">=</span> Embedder(openai_cli, em, smodel<span class="op">=</span>sparse_model)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>embds <span class="op">=</span> embedder.create_dembs(chunks.attrgot(<span class="st">'text'</span>))</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(embds), <span class="bu">len</span>(embds[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>(322, 768)</code></pre>
</div>
</div>
<p>I have an api key for gemini and want to use for embedding model with the openai spec https://ai.google.dev/gemini-api/docs/openai</p>
<section id="reply-5" class="level5">
<h5 class="anchored" data-anchor-id="reply-5">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>To use Gemini‚Äôs embedding model with the OpenAI spec, you‚Äôll need to:</p>
<ol type="1">
<li>Change the base URL to Gemini‚Äôs endpoint</li>
<li>Use your Gemini API key</li>
<li>Specify a Gemini embedding model name</li>
</ol>
<p>Here‚Äôs what to modify:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>openai_cli <span class="op">=</span> openai.OpenAI(</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"https://generativelanguage.googleapis.com/v1beta/openai/"</span>,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span>os.environ[<span class="st">'GEMINI_API_KEY'</span>]</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>For the embedding model, Gemini supports models like <code>text-embedding-004</code>. You‚Äôd set:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>em <span class="op">=</span> <span class="st">"text-embedding-004"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Have you already set your <code>GEMINI_API_KEY</code> environment variable, or would you like help with that?</p>
<p>I am getting info logs when calling to gemini 2025-11-29 15:22:21,210 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/embeddings ‚ÄúHTTP/1.1 200 OK‚Äù</p>
</section>
<section id="reply-6" class="level5">
<h5 class="anchored" data-anchor-id="reply-6">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You can suppress those INFO logs by adjusting the logging level for the HTTP client. Add this at the top of your notebook:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>logging.getLogger(<span class="st">"httpx"</span>).setLevel(logging.WARNING)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will only show warnings and errors, not info messages. Would you like to try this?</p>
<div id="9c86684d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>all_dense_embds <span class="op">=</span> embedder.create_dembs(all_chunks.attrgot(<span class="st">'text'</span>))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(all_dense_embds), <span class="bu">len</span>(all_dense_embds[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>(1899, 768)</code></pre>
</div>
</div>
<div id="06cc241a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>all_sparse_embds <span class="op">=</span> embedder.create_sembs(all_chunks.attrgot(<span class="st">'text'</span>))</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(all_sparse_embds), <span class="bu">len</span>(all_sparse_embds[<span class="dv">0</span>].values)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>(1899, 137)</code></pre>
</div>
</div>
</section>
</section>
<section id="qdrant" class="level2">
<h2 class="anchored" data-anchor-id="qdrant">Qdrant</h2>
<div id="0edf3a70" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>QHOST <span class="op">=</span> <span class="st">"https://76eb75ff-5ca3-4a68-93d4-ae19a9c169a4.us-west-2-0.aws.cloud.qdrant.io:6333"</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>api_key <span class="op">=</span> os.getenv(<span class="st">"QDRANT_API_KEY"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="03c85c41" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QdrantStore:</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, url, key, collection_name, use_hybrid<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        fc.store_attr()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cli <span class="op">=</span> QdrantClient(url<span class="op">=</span>url, api_key<span class="op">=</span>key)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.points <span class="op">=</span> []</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_or_replace_collection(<span class="va">self</span>, vs, replace<span class="op">=</span><span class="va">False</span>, distance<span class="op">=</span>Distance.COSINE):</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> replace: <span class="va">self</span>.cli.delete_collection(collection_name<span class="op">=</span><span class="va">self</span>.collection_name)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        sparse_vectors_config <span class="op">=</span> <span class="va">None</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.cli.collection_exists(<span class="va">self</span>.collection_name): </span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.use_hybrid: sparse_vectors_config<span class="op">=</span>{<span class="st">"sparse"</span>: models.SparseVectorParams()}</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cli.create_collection(collection_name<span class="op">=</span><span class="va">self</span>.collection_name, </span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>                vectors_config<span class="op">=</span>{<span class="st">"dense"</span>: VectorParams(size<span class="op">=</span>vs, distance<span class="op">=</span>distance)}, </span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>                sparse_vectors_config<span class="op">=</span>sparse_vectors_config)</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _sv(<span class="va">self</span>, semb):</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> models.SparseVector(values<span class="op">=</span>semb.values.tolist(), indices<span class="op">=</span>semb.indices.tolist())</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_hybrid_points(<span class="va">self</span>, chunks, dembs, sembs, start_id<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> fc.L(chunks, dembs, sembs).<span class="bu">zip</span>().<span class="bu">enumerate</span>().<span class="bu">map</span>(<span class="kw">lambda</span> ele: PointStruct(<span class="bu">id</span><span class="op">=</span>start_id<span class="op">+</span>ele[<span class="dv">0</span>], vector<span class="op">=</span>{<span class="st">"dense"</span>: ele[<span class="dv">1</span>][<span class="dv">1</span>], <span class="st">"sparse"</span>: <span class="va">self</span>._sv(ele[<span class="dv">1</span>][<span class="dv">2</span>])}, payload<span class="op">=</span>ele[<span class="dv">1</span>][<span class="dv">0</span>].__dict__))</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_points(<span class="va">self</span>, chunks, dembs, sembs<span class="op">=</span><span class="va">None</span>, start_id<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_hybrid <span class="kw">and</span> sembs: </span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.points <span class="op">=</span> <span class="va">self</span>.create_hybrid_points(chunks, dembs, sembs, start_id)</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: </span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.points <span class="op">=</span> fc.L(chunks, dembs).<span class="bu">zip</span>().<span class="bu">enumerate</span>().<span class="bu">map</span>(<span class="kw">lambda</span> ele: PointStruct(<span class="bu">id</span><span class="op">=</span>start_id<span class="op">+</span>ele[<span class="dv">0</span>], vector<span class="op">=</span>{<span class="st">"dense"</span>: ele[<span class="dv">1</span>][<span class="dv">1</span>]}, payload<span class="op">=</span>ele[<span class="dv">1</span>][<span class="dv">0</span>].__dict__))</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> upsert(<span class="va">self</span>, bs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>        fc.mapped(<span class="kw">lambda</span> o: <span class="va">self</span>.cli.upsert(collection_name<span class="op">=</span><span class="va">self</span>.collection_name, points<span class="op">=</span><span class="bu">list</span>(o)), fc.chunked(<span class="va">self</span>.points, chunk_sz<span class="op">=</span>bs))</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hybrid_search(<span class="va">self</span>, query_demb, query_semb, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.cli.query_points(collection_name<span class="op">=</span><span class="va">self</span>.collection_name,</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>            prefetch<span class="op">=</span>[</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>                models.Prefetch(query<span class="op">=</span>query_demb, using<span class="op">=</span><span class="st">"dense"</span>, limit<span class="op">=</span>limit<span class="op">*</span><span class="dv">2</span>),</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>                models.Prefetch(query<span class="op">=</span><span class="va">self</span>._sv(query_semb), using<span class="op">=</span><span class="st">"sparse"</span>, limit<span class="op">=</span>limit<span class="op">*</span><span class="dv">2</span>)</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>            query<span class="op">=</span>models.FusionQuery(fusion<span class="op">=</span>models.Fusion.RRF),</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a>            limit<span class="op">=</span>limit</span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> search(<span class="va">self</span>, query_demb, query_semb<span class="op">=</span><span class="va">None</span>, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_hybrid <span class="kw">and</span> query_semb:</span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a>            results <span class="op">=</span> <span class="va">self</span>.hybrid_search(query_demb, query_semb, limit)</span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a>            results <span class="op">=</span> <span class="va">self</span>.cli.query_points(collection_name<span class="op">=</span><span class="va">self</span>.collection_name, query<span class="op">=</span>query_vector, limit<span class="op">=</span>limit)</span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> results.points</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Do we really require qdrant datastore for storing 1899 embeddings and searching? Will a brute force search work which does search across all the embedding</p>
<section id="reply-7" class="level5">
<h5 class="anchored" data-anchor-id="reply-7">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! For 1,899 embeddings, brute force search would absolutely work and might even be simpler and faster than using Qdrant.</p>
<p>With modern hardware, computing cosine similarity across ~2,000 vectors is very quick (milliseconds). Vector databases like Qdrant are mainly beneficial when you have: - Hundreds of thousands or millions of vectors - Need for persistence across sessions - Advanced filtering on metadata - Multiple concurrent users</p>
<p>For your use case, you could use simple numpy/scipy operations:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Would you like to try implementing a simple brute force search to compare? Or are you planning to scale up significantly and want to keep Qdrant for future growth?</p>
<p>And what about the keyword search?</p>
</section>
<section id="reply-8" class="level5">
<h5 class="anchored" data-anchor-id="reply-8">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good point! For keyword search (like BM25), you have a few options without Qdrant:</p>
<ol type="1">
<li><strong>Simple approach</strong>: Use libraries like <code>rank-bm25</code> which does in-memory BM25 scoring</li>
<li><strong>Hybrid with numpy</strong>: Store sparse embeddings as scipy sparse matrices and compute similarities</li>
<li><strong>Full-text search</strong>: Use SQLite‚Äôs FTS5 for keyword search</li>
</ol>
<p>For 1,899 documents, all of these would be fast enough. The sparse embeddings you‚Äôre generating with <code>Qdrant/bm25</code> model could be stored as scipy sparse matrices and searched with simple dot products.</p>
<p>Are you interested in implementing a lightweight alternative, or do you prefer keeping Qdrant for the convenience of having both dense and sparse search in one place?</p>
<div id="a415a9df" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>cname <span class="op">=</span> <span class="st">'wattbot_docs_nomic_768_sparse'</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># cname = 'wattbot_docs_qwen_8b'</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>qs <span class="op">=</span> QdrantStore(QHOST, api_key, cname, use_hybrid<span class="op">=</span><span class="va">True</span>).create_or_replace_collection(<span class="dv">768</span>, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>qs.create_points(all_chunks, all_dense_embds, all_sparse_embds).upsert()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;__main__.QdrantStore&gt;</code></pre>
</div>
</div>
<div id="5e686b87" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>qs.cli.get_collections()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>CollectionsResponse(collections=[CollectionDescription(name='wattbot_docs_nomic_768'), CollectionDescription(name='wattbot_documents_768'), CollectionDescription(name='wattbot_docs_qwen_8b'), CollectionDescription(name='colbert_chunks'), CollectionDescription(name='wattbot_documents'), CollectionDescription(name='wattbot_docs_nomic_768_sparse')])</code></pre>
</div>
</div>
<p>Instead of the qdrant database I save the dense and sparse embeddings locally and do similarity search then combine the results</p>
</section>
<section id="reply-9" class="level5">
<h5 class="anchored" data-anchor-id="reply-9">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>That‚Äôs a solid approach for this dataset size! You‚Äôd essentially be building a lightweight hybrid search system.</p>
<p>The basic steps would be: 1. Save dense embeddings (numpy array) 2. Save sparse embeddings (scipy sparse matrix or list) 3. For search: compute similarities separately for dense and sparse 4. Combine results using a fusion method (like RRF - Reciprocal Rank Fusion)</p>
<p>Would you like me to help you implement this step by step, or would you prefer to try building it yourself with guidance on specific parts as needed?</p>
<p>Yes I would need to help with saving the sparse embedding and compute similarity for them</p>
</section>
<section id="reply-10" class="level5">
<h5 class="anchored" data-anchor-id="reply-10">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let‚Äôs start with saving the sparse embeddings.</p>
<p>Since your sparse embeddings have <code>.values</code> and <code>.indices</code> attributes (from the fastembed model), you have a couple of options:</p>
<ol type="1">
<li><strong>Scipy sparse matrix</strong> - efficient for similarity computation</li>
<li><strong>List of dicts</strong> - simpler, keeps the structure as-is</li>
</ol>
<p>For scipy, you‚Äôd convert to CSR format which is efficient for similarity searches. For the list approach, you‚Äôd just pickle the embeddings directly.</p>
<p>Which approach sounds better to you, or would you like to try the scipy sparse matrix route since it‚Äôs more efficient for similarity computation?</p>
<p>What is CSR format</p>
</section>
<section id="reply-11" class="level5">
<h5 class="anchored" data-anchor-id="reply-11">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>CSR stands for <strong>Compressed Sparse Row</strong> format. It‚Äôs a way to efficiently store sparse matrices (matrices with mostly zeros) by only storing the non-zero values.</p>
<p>Instead of storing a full matrix like:</p>
<pre><code>[0, 0, 3, 0, 5]
[2, 0, 0, 0, 0]
[0, 4, 0, 1, 0]</code></pre>
<p>CSR stores three arrays: - <strong>values</strong>: the non-zero values <code>[3, 5, 2, 4, 1]</code> - <strong>indices</strong>: which column each value is in - <strong>indptr</strong>: where each row starts</p>
<p>This saves memory and makes operations like dot products much faster when you have lots of zeros (which is typical for sparse embeddings like BM25).</p>
<p>Does this make sense? Would you like to proceed with using scipy‚Äôs CSR format for your sparse embeddings?</p>
<p>Okay lets do that</p>
</section>
<section id="reply-12" class="level5">
<h5 class="anchored" data-anchor-id="reply-12">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let‚Äôs start with converting your sparse embeddings to CSR format.</p>
<p>First, you‚Äôll need scipy‚Äôs sparse module:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> csr_matrix</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>To convert your <code>all_sparse_embds</code> (which are fastembed sparse vectors) to a CSR matrix, you‚Äôll need to extract the indices and values from each embedding and stack them into a matrix.</p>
<p>Try creating a function that takes one sparse embedding and returns the indices and values. What do you get when you look at <code>all_sparse_embds[0].indices</code> and <code>all_sparse_embds[0].values</code>?</p>
<p>I created the function def get_val_idx(s): return s.indices, s.values</p>
</section>
<section id="reply-13" class="level5">
<h5 class="anchored" data-anchor-id="reply-13">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good start! Now you have a way to extract the indices and values from each sparse embedding.</p>
<p>The next step is to build the CSR matrix from all your embeddings. You‚Äôll need to: 1. Determine the maximum index across all embeddings (this will be the number of columns) 2. Stack all the indices and values together 3. Create the CSR matrix</p>
<p>Try finding the maximum index first. What do you get when you check the max index across all your sparse embeddings?</p>
<p>THe highest value of indices are 30151</p>
</section>
<section id="reply-14" class="level5">
<h5 class="anchored" data-anchor-id="reply-14">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! So your CSR matrix will have 30,152 columns (since indices are 0-based).</p>
<p>Now you need to build the CSR matrix. Scipy‚Äôs <code>csr_matrix</code> can be constructed using:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>csr_matrix((data, indices, indptr), shape<span class="op">=</span>(n_rows, n_cols))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Where: - <code>data</code> = all values concatenated - <code>indices</code> = all column indices concatenated<br>
- <code>indptr</code> = array marking where each row starts in the data array</p>
<p>Try creating the <code>data</code> and <code>indices</code> arrays first by concatenating all the values and indices from your sparse embeddings. What approach will you use to concatenate them?</p>
<p>Yes</p>
</section>
<section id="reply-15" class="level5">
<h5 class="anchored" data-anchor-id="reply-15">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Now, back to building your CSR matrix - do you want to continue with concatenating the data and indices arrays, or would you like to explore something else about your RAG system first?</p>
<p>lets do a simple lexical search instead of the model</p>
</section>
<section id="reply-16" class="level5">
<h5 class="anchored" data-anchor-id="reply-16">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good idea! For simple lexical search, you have a few options:</p>
<ol type="1">
<li><strong>BM25 library</strong> (<code>rank-bm25</code>) - classic keyword scoring</li>
<li><strong>TF-IDF with sklearn</strong> - term frequency approach</li>
<li><strong>SQLite FTS5</strong> - built-in full-text search</li>
</ol>
<p>BM25 is generally considered best for search/retrieval tasks. Would you like to try the <code>rank-bm25</code> library, or explore one of the other options?</p>
<p>Yes</p>
</section>
<section id="reply-17" class="level5">
<h5 class="anchored" data-anchor-id="reply-17">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! First, you‚Äôll need to install the library:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install rank<span class="op">-</span>bm25</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then import it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rank_bm25 <span class="im">import</span> BM25Okapi</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>BM25 needs tokenized documents to build its index. You‚Äôll need to split your chunk texts into tokens (words). How would you like to tokenize - simple <code>split()</code> or something more sophisticated?</p>
<p>When adding the package rank-bm25 I am getting the following error</p>
<p>No solution found when resolving dependencies for split (python_full_version == ‚Äô3.9.*‚Äô): ‚ï∞‚îÄ‚ñ∂ Because the requested Python version (&gt;=3.9) does not satisfy Python&gt;=3.10 and contextkit==0.0.10 depends on Python&gt;=3.10, we can conclude that contextkit==0.0.10 cannot be used. And because only contextkit&lt;=0.0.10 is available and your project depends on contextkit&gt;=0.0.10, we can conclude that your project‚Äôs requirements are unsatisfiable.</p>
<pre><code>  hint: The `requires-python` value (&gt;=3.9) includes Python versions that are not supported by your dependencies (e.g., contextkit==0.0.10 only supports &gt;=3.10). Consider using a more restrictive
  `requires-python` value (like &gt;=3.10).</code></pre>
<p>help: If you want to add the package regardless of the failed resolution, provide the <code>--frozen</code> flag to skip locking and syncing. (wattbot) (base) MacBook-Pro:wattbot anubhavmaity$</p>
</section>
<section id="reply-18" class="level5">
<h5 class="anchored" data-anchor-id="reply-18">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>This error is about your Python version and project dependencies, not specifically about <code>rank-bm25</code>.</p>
<p>Your project requires <code>contextkit&gt;=0.0.10</code> which needs Python 3.10+, but your project‚Äôs <code>requires-python</code> is set to <code>&gt;=3.9</code>.</p>
<p>You have two options:</p>
<ol type="1">
<li><strong>Update your project‚Äôs Python requirement</strong> to <code>&gt;=3.10</code> in your <code>pyproject.toml</code> or similar config file</li>
<li><strong>Use the <code>--frozen</code> flag</strong> as suggested: <code>uv pip install rank-bm25 --frozen</code> (though this skips dependency resolution)</li>
</ol>
<p>Which Python version are you actually running? You can check with <code>python --version</code>.</p>
<p>okay I was able to install rank-bm25, now what is next</p>
</section>
<section id="reply-19" class="level5">
<h5 class="anchored" data-anchor-id="reply-19">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Now you need to tokenize your chunk texts to build the BM25 index.</p>
<p>Try creating a list of tokenized documents from your chunks. For each chunk‚Äôs text, split it into words. What method will you use - simple <code>split()</code> or something else?</p>
<p>simple split will do</p>
</section>
<section id="reply-20" class="level5">
<h5 class="anchored" data-anchor-id="reply-20">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Try creating the tokenized corpus:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>tokenized_corpus <span class="op">=</span> [chunk.text.lower().split() <span class="cf">for</span> chunk <span class="kw">in</span> all_chunks]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>.lower()</code> helps with case-insensitive matching. Once you have this, you can create the BM25 index. Give it a try and let me know what you get!</p>
<p>got the tokenized corpus what is next?</p>
</section>
<section id="reply-21" class="level5">
<h5 class="anchored" data-anchor-id="reply-21">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Excellent! Now create the BM25 index:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>bm25 <span class="op">=</span> BM25Okapi(tokenized_corpus)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This builds the index from your tokenized documents. Once you have this, you‚Äôll be able to search by tokenizing a query and using <code>bm25.get_scores()</code> or <code>bm25.get_top_n()</code>.</p>
<p>Have you created the index?</p>
<p>What should I pass to get_top_n</p>
</section>
<section id="reply-22" class="level5">
<h5 class="anchored" data-anchor-id="reply-22">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p><code>get_top_n()</code> takes three main arguments:</p>
<ol type="1">
<li><strong>tokenized_query</strong> - your search query split into tokens (like you did for the corpus)</li>
<li><strong>documents</strong> - the original documents/chunks you want returned</li>
<li><strong>n</strong> - how many top results you want</li>
</ol>
<p>So it would look something like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>bm25.get_top_n(tokenized_query, all_chunks, n<span class="op">=</span><span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>What query would you like to test with?</p>
<p>How should I create a new class to have bm25 and do the cosine similarity across all vectos?</p>
</section>
<section id="reply-23" class="level5">
<h5 class="anchored" data-anchor-id="reply-23">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good thinking to organize this into a class! Here‚Äôs a structure to consider:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HybridSearch:</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chunks, dense_embds, tokenized_corpus):</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store your data</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize BM25</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store dense embeddings as numpy array</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>You‚Äôll need methods like: - <code>lexical_search(query, top_k)</code> - for BM25 - <code>dense_search(query_emb, top_k)</code> - for cosine similarity - <code>hybrid_search(query, query_emb, top_k)</code> - combines both</p>
<p>For cosine similarity, you can use <code>sklearn.metrics.pairwise.cosine_similarity</code> or just normalize vectors and use dot product.</p>
<p>Would you like to start building this class yourself, or would you like me to write it out for you?</p>
<p>Can you please show me how to do the cosine similarity?</p>
</section>
<section id="reply-24" class="level5">
<h5 class="anchored" data-anchor-id="reply-24">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Sure! Here are two approaches for cosine similarity:</p>
<p><strong>Option 1 - Using sklearn:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> cosine_similarity([query_emb], dense_embds)[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Option 2 - Using numpy (faster for normalized vectors):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize vectors first</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>dense_embds_norm <span class="op">=</span> dense_embds <span class="op">/</span> np.linalg.norm(dense_embds, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>query_norm <span class="op">=</span> query_emb <span class="op">/</span> np.linalg.norm(query_emb)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> dense_embds_norm <span class="op">@</span> query_norm</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Both give you similarity scores for each document. To get top-k results, you‚Äôd use <code>np.argsort(scores)[-k:][::-1]</code> to get the indices.</p>
<p>Which approach would you like to use in your class?</p>
</section>
</section>
<section id="retriever" class="level2">
<h2 class="anchored" data-anchor-id="retriever">Retriever</h2>
<div id="d5622821" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Retriever:</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embedder, vector_store):</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>        fc.store_attr()</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> retrieve(<span class="va">self</span>, query, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        query_demb <span class="op">=</span> <span class="va">self</span>.embedder.create_dembs(texts<span class="op">=</span>[query])[<span class="dv">0</span>]</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>        query_semb <span class="op">=</span> <span class="va">self</span>.embedder.create_sembs(texts<span class="op">=</span>[query])[<span class="dv">0</span>] <span class="cf">if</span> <span class="va">self</span>.embedder.smodel <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.vector_store.search(query_demb, query_semb, limit<span class="op">=</span>limit)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b37341b1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>query_text <span class="op">=</span> <span class="st">"What are Amazon's carbon emissions goals?"</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>retriever <span class="op">=</span> Retriever(embedder, qs)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>retriever.retrieve(query_text, limit<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>].payload[<span class="st">'text'</span>][:<span class="dv">200</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>'to build Climate Pledge Arena, the first International \nLiving Future Institute zero-carbon certified sports and \nentertainment venue in the world.\nCarbon \nThe science is clear. Significant carbon emi'</code></pre>
</div>
</div>
<p>To support creating the embeddings for both sparse and dense.</p>
<section id="reply-25" class="level5">
<h5 class="anchored" data-anchor-id="reply-25">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You need to update the <code>Retriever</code> class to handle both dense and sparse embeddings. The error shows it‚Äôs calling <code>create_embeddings</code> which doesn‚Äôt exist - your <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#embedder"><code>Embedder</code></a> has <code>create_dembs</code> and <code>create_sembs</code> instead.</p>
<p>Here‚Äôs how you can modify the <code>retrieve</code> method to work with your hybrid setup:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> retrieve(<span class="va">self</span>, query, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    query_demb <span class="op">=</span> <span class="va">self</span>.embedder.create_dembs(texts<span class="op">=</span>[query])[<span class="dv">0</span>]</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    query_semb <span class="op">=</span> <span class="va">self</span>.embedder.create_sembs(texts<span class="op">=</span>[query])[<span class="dv">0</span>] <span class="cf">if</span> <span class="va">self</span>.embedder.smodel <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now you need to decide how to combine them for search</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>However, I notice your <code>QdrantStore.search</code> method currently only takes a single <code>query_vector</code>. For hybrid search with both dense and sparse vectors, you‚Äôll need to update that method too.</p>
<p>Would you like help modifying the search method to support hybrid queries, or do you want to work on that yourself?</p>
<p>Yes</p>
</section>
<section id="reply-26" class="level5">
<h5 class="anchored" data-anchor-id="reply-26">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! For hybrid search in Qdrant, you need to use the <code>query_points</code> method with a <code>prefetch</code> parameter to combine dense and sparse searches.</p>
<p>Here‚Äôs the key structure:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> search(<span class="va">self</span>, query_demb, query_semb<span class="op">=</span><span class="va">None</span>, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.use_hybrid <span class="kw">and</span> query_semb:</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use prefetch for hybrid search</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> <span class="va">self</span>.cli.query_points(</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>            collection_name<span class="op">=</span><span class="va">self</span>.collection_name,</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>            prefetch<span class="op">=</span>[</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>                models.Prefetch(query<span class="op">=</span>query_demb, using<span class="op">=</span><span class="st">"dense"</span>, limit<span class="op">=</span>limit<span class="op">*</span><span class="dv">2</span>),</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>                models.Prefetch(query<span class="op">=</span><span class="va">self</span>._sv(query_semb), using<span class="op">=</span><span class="st">"sparse"</span>, limit<span class="op">=</span>limit<span class="op">*</span><span class="dv">2</span>)</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>            query<span class="op">=</span>models.FusionQuery(fusion<span class="op">=</span>models.Fusion.RRF),</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>            limit<span class="op">=</span>limit</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>prefetch</code> retrieves candidates from both dense and sparse vectors, then <code>FusionQuery</code> with RRF (Reciprocal Rank Fusion) combines the results.</p>
<p>Would you like to complete the rest of the method to handle the non-hybrid case?</p>
<div id="01b8d1ac" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>query_text <span class="op">=</span> <span class="st">"What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?"</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>retriever.retrieve(query_text, limit<span class="op">=</span><span class="dv">3</span>)[<span class="dv">2</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>ScoredPoint(id=1864, version=18, score=0.33333334, payload={'chunk_id': 8400, 'doc_id': 'zschache2025', 'text': ' on a single\nbenchmark, we extend these efforts to a greater number of partially very large models\ndeployed to a practical inference scenario.\nThis makes our study very similar to the one by Alizadeh et al. (2025), who inves-\ntigated the trade-offs between accuracy and energy consumption when deploying large\nlanguage models (LLMs) for software development tasks. Besides the finding that\nlarger LLMs with higher energy consumption do not always yield significantly bet-\nter accuracy, the authors demonstrated that architectural factors, such as feedforward\nlayer size and transformer block count, directly correlate with energy usage.\n3 Finally, Luccioni et al. (2024) provide one of the most comprehensive analyses\nof energy consumption during ML model inference. Their study systematically com-\npared the energy costs of 88 models across 10 tasks and 30 datasets, including both\nsmaller task-specific and larger multi-purpose models. They found that the larger\nmodels are orders of magnitude more energy-intensive than smaller task-specific ones,\nespecially for tasks involving text and image generation. Furthermore, their research\nunderscores the variability in energy consumption across tasks and model architec-\ntures. The authors advocate for increased transparency and sustainable deployment\npractices, emphasizing that the environmental costs of deploying large, multi-purpose\nAI systems must be carefully weighed against their utility.\n3 Data and methods\nOur experiments are inspired by a', 'type': 'paper', 'url': 'https://arxiv.org/pdf/2508.14170 ', 'title': 'Comparing energy consumption and accuracy in text classification inference', 'citation': 'Johannes Zschache, &amp; Tilman Hartwig (2025). Comparing energy consumption and accuracy in text classification inference arXiv. https://arxiv.org/pdf/2508.14170', 'year': 2025}, vector=None, shard_key=None, order_value=None)</code></pre>
</div>
</div>
<p>How to remove the <strong>stored_args</strong> from the ScoredPoint above?</p>
</section>
<section id="reply-27" class="level5">
<h5 class="anchored" data-anchor-id="reply-27">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>The <code>__stored_args__</code> is being added to the payload because it‚Äôs part of the <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#chunk"><code>Chunk</code></a> object‚Äôs <code>__dict__</code>. You can filter it out when creating the points.</p>
<p>In your <code>QdrantStore.create_points</code> method, modify the payload creation to exclude it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>payload<span class="op">=</span>{k: v <span class="cf">for</span> k, v <span class="kw">in</span> ele[<span class="dv">1</span>][<span class="dv">0</span>].__dict__.items() <span class="cf">if</span> k <span class="op">!=</span> <span class="st">'__stored_args__'</span>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This filters out <code>__stored_args__</code> before storing the payload. Would you like to update both the <code>create_points</code> and <code>create_hybrid_points</code> methods with this change?</p>
<p>Cant we modify the Chunk class to not store the <strong>stored_args</strong>?</p>
</section>
<section id="reply-28" class="level5">
<h5 class="anchored" data-anchor-id="reply-28">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes, absolutely! That‚Äôs a cleaner solution. The <code>__stored_args__</code> comes from <code>fastcore</code>‚Äôs <code>store_attr()</code> function. You can either:</p>
<ol type="1">
<li>Override <code>__dict__</code> to exclude it</li>
<li>Delete it after calling <code>store_attr()</code></li>
<li>Use a property/method to return a clean dict</li>
</ol>
<p>The simplest approach would be to add this line in your <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#chunk.__init__"><code>Chunk.__init__</code></a>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> <span class="va">self</span>.__stored_args__</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Would you like to try that, or would you prefer a different approach?</p>
<p>I want to search for the text ‚Äú5,842 labeled entries‚Äù in all_chunks</p>
</section>
<section id="reply-29" class="level5">
<h5 class="anchored" data-anchor-id="reply-29">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You can search for that text using a list comprehension or the <code>filter</code> method. Here‚Äôs a simple approach:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>matching_chunks <span class="op">=</span> [chunk <span class="cf">for</span> chunk <span class="kw">in</span> all_chunks <span class="cf">if</span> <span class="st">'5,842 labeled entries'</span> <span class="kw">in</span> chunk.text]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will give you all chunks that contain that exact phrase. Would you like to try this and see what you find?</p>
<div id="ae8bf308" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>khan2025_chunks <span class="op">=</span> DocumentProcessor().process(<span class="st">'khan2025'</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>[chunk <span class="cf">for</span> chunk <span class="kw">in</span> khan2025_chunks <span class="cf">if</span> <span class="st">'5,842 entries'</span> <span class="kw">in</span> chunk.text]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre>[<span class="ansi-magenta-fg ansi-bold">chunk_id</span>: 18200
 <span class="ansi-magenta-fg ansi-bold">doc_id</span>: khan2025
 <span class="ansi-magenta-fg ansi-bold">text</span>: sed language model
 tuned for general-purpose tasks.
 ‚Ä¢ Mistral-7B-Instruct-v0.3: An instruction-tuned]</pre>
</div>
</div>
</div>
<p>It does not return the result with 5,482 labeled entries. How to make sure it returns that?</p>
</section>
<section id="reply-30" class="level5">
<h5 class="anchored" data-anchor-id="reply-30">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>The issue is that you‚Äôre searching for the exact phrase ‚Äò5,842 entries‚Äô but the actual text might have different formatting - perhaps it says ‚Äò5,842 labeled entries‚Äô or has extra spaces or punctuation.</p>
<p>Try searching with just the number:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>[chunk <span class="cf">for</span> chunk <span class="kw">in</span> khan2025_chunks <span class="cf">if</span> <span class="st">'5,842'</span> <span class="kw">in</span> chunk.text]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will find any chunk containing that number, regardless of the surrounding words. Does that return the result you‚Äôre looking for?</p>
<div id="c49d944a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>[chunk <span class="cf">for</span> chunk <span class="kw">in</span> khan2025_chunks <span class="cf">if</span> <span class="st">'5,842'</span> <span class="kw">in</span> chunk.text]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre>[<span class="ansi-magenta-fg ansi-bold">chunk_id</span>: 18200
 <span class="ansi-magenta-fg ansi-bold">doc_id</span>: khan2025
 <span class="ansi-magenta-fg ansi-bold">text</span>: sed language model
 tuned for general-purpose tasks.
 ‚Ä¢ Mistral-7B-Instruct-v0.3: An instruction-tuned]</pre>
</div>
</div>
</div>
<div id="b4b6aa69" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># [chunk for chunk in all_chunks if 'JetMoE-8B' in chunk.text]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Why the retrieve function is returning the result as 2nd index instead of first?</p>
</section>
<section id="reply-31" class="level5">
<h5 class="anchored" data-anchor-id="reply-31">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good question! There are a few possible reasons why the most relevant chunk isn‚Äôt ranking first:</p>
<ol type="1">
<li><strong>Embedding similarity</strong> - The query embedding might be more similar to other chunks based on semantic meaning, not just keyword matching</li>
<li><strong>Hybrid search weighting</strong> - The RRF (Reciprocal Rank Fusion) is combining dense and sparse results, and the sparse (BM25) scoring might be favoring other chunks</li>
<li><strong>Context around the keywords</strong> - Other chunks might have more context that semantically matches your query</li>
</ol>
<p>To debug this, you could: - Check the scores of the top 3 results to see how close they are - Try adjusting the fusion weights or prefetch limits in your hybrid search - Compare dense-only vs sparse-only vs hybrid results</p>
<p>Would you like to investigate any of these possibilities?</p>
</section>
</section>
<section id="generator" class="level2">
<h2 class="anchored" data-anchor-id="generator">Generator</h2>
<div id="7e0f70e7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator:</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, llm_client, prompt_template):</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>        fc.store_attr()</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_context(<span class="va">self</span>, retrieved_chunks):</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>.join([<span class="ss">f"Doc ID: </span><span class="sc">{</span>chunk<span class="sc">.</span>payload[<span class="st">'doc_id'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">URL: </span><span class="sc">{</span>chunk<span class="sc">.</span>payload[<span class="st">'url'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Title: </span><span class="sc">{</span>chunk<span class="sc">.</span>payload[<span class="st">'title'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Text: </span><span class="sc">{</span>chunk<span class="sc">.</span>payload[<span class="st">'text'</span>]<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> chunk <span class="kw">in</span> retrieved_chunks])</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> format_prompt(<span class="va">self</span>, query, context):</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.prompt_template.build(context<span class="op">=</span>context, question<span class="op">=</span>query)</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, query, retrieved_chunks):</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> <span class="va">self</span>.create_context(retrieved_chunks)</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>        formatted_prompt <span class="op">=</span> <span class="va">self</span>.format_prompt(query, context)</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> <span class="va">self</span>.llm_client.chat.completions.create(formatted_prompt[<span class="st">'messages'</span>], response_format<span class="op">=</span>{<span class="st">"type"</span>: <span class="st">"json_object"</span>})</span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>        time.sleep(<span class="dv">1</span>)</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> fc.loads(response.choices[<span class="dv">0</span>].message.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5f3427b6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># llm = fw.LLM(model="accounts/fireworks/models/deepseek-v3p1",  deployment_type="auto")</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="co"># llm = fw.LLM(model="accounts/fireworks/models/deepseek-r1-0528",  deployment_type="auto")</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model = 'accounts/fireworks/models/kimi-k2-instruct-0905'</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="co"># model = 'accounts/fireworks/models/llama-v3p1-70b-instruct'</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> <span class="st">'accounts/fireworks/models/deepseek-v3p1'</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="co"># model = 'accounts/fireworks/models/kimi-k2-thinking'</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> fw.LLM(model<span class="op">=</span>model,  deployment_type<span class="op">=</span><span class="st">"auto"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>I want to use a gemini model with openai spec</p>
<section id="reply-32" class="level5">
<h5 class="anchored" data-anchor-id="reply-32">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You can use Gemini models with the OpenAI spec by creating an OpenAI client with Gemini‚Äôs base URL, similar to what you did for embeddings earlier.</p>
<p>For the <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#generator"><code>Generator</code></a> class, you‚Äôd pass in an OpenAI client configured for Gemini:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>gemini_client <span class="op">=</span> openai.OpenAI(</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    base_url<span class="op">=</span><span class="st">"https://generativelanguage.googleapis.com/v1beta/openai/"</span>,</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    api_key<span class="op">=</span>os.environ[<span class="st">'GEMINI_API_KEY'</span>]</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then for the model name, you‚Äôd use something like <code>"gemini-1.5-pro"</code> or <code>"gemini-1.5-flash"</code>.</p>
<p>Have you already set up your Gemini API key, or would you like help with that part?</p>
<p>How to pass the model name?</p>
</section>
<section id="reply-33" class="level5">
<h5 class="anchored" data-anchor-id="reply-33">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>When you create the chat completion in your <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#generator.generate"><code>Generator.generate</code></a> method, you pass the model name to the <code>create</code> call:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> <span class="va">self</span>.llm_client.chat.completions.create(</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"gemini-1.5-pro"</span>,  <span class="co"># or "gemini-1.5-flash"</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>formatted_prompt[<span class="st">'messages'</span>],</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    response_format<span class="op">=</span>{<span class="st">"type"</span>: <span class="st">"json_object"</span>}</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Note: Check if Gemini supports the <code>response_format</code> parameter - not all models/providers support structured JSON output this way.</p>
<p>Would you like to test this with your Gemini client?</p>
<p>How to enable the thinking model for kimi-k2-thinking model? And is there a prefill the model with {</p>
</section>
<section id="reply-34" class="level5">
<h5 class="anchored" data-anchor-id="reply-34">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>For thinking models like <code>kimi-k2-thinking</code>, you typically need to:</p>
<ol type="1">
<li><strong>Enable thinking mode</strong> - Check if there‚Äôs a parameter like <code>enable_thinking=True</code> or similar in the Fireworks API</li>
<li><strong>Prefill with <code>{</code></strong> - Yes, some models support prefilling the assistant‚Äôs response to force JSON output</li>
</ol>
<p>For prefilling, you‚Äôd add an assistant message with just <code>{</code> to force JSON formatting:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="st">"your prompt"</span>},</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: <span class="st">"{"</span>, <span class="st">"prefix"</span>: <span class="va">True</span>}</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The exact parameter name might vary. Have you checked the Fireworks documentation for <code>kimi-k2-thinking</code> specific parameters? Would you like me to help you look that up?</p>
<p>Yes please</p>
</section>
<section id="reply-35" class="level5">
<h5 class="anchored" data-anchor-id="reply-35">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<details class="tool-usage-details">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb73"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"id"</span><span class="fu">:</span> <span class="st">"toolu_01QueTq9m7VMedbMSWThKkaY"</span><span class="fu">,</span></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"call"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"function"</span><span class="fu">:</span> <span class="st">"read_url"</span><span class="fu">,</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"arguments"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"url"</span><span class="fu">:</span> <span class="st">"https://docs.fireworks.ai/guides/querying-text-models"</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">},</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"result"</span><span class="fu">:</span> <span class="st">"'Skip to main content</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[Fireworks AI Docs home page![light logo](https://mintcdn.com/fireworksai/sTHhFfY93wc80BaS/logo/logo.svg?fit=max&amp;auto=format&amp;n=sTHhFfY93wc80BaS&amp;q=85&amp;s=33df07dcf4adc8ba74a3d9c6e5153b3a)![dark logo](https://mintcdn.com/fireworksai/sTHhFfY93wc80BaS/logo/logo.svg?fit=max&amp;auto=format&amp;n=sTHhFfY93wc80BaS&amp;q=85&amp;s=33df07dcf4adc8ba74a3d9c6e5153b3a)](/)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[Documentation](/getting-started/introduction)[API &amp; SDK Reference](/api-reference/introduction)[CLI Reference](/tools-sdks/firectl/firectl)[Demos](https://demos.fireworks.ai)[Resources](/getting-started/concepts)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Community](https://discord.gg/fireworks-ai)</span><span class="ch">\\</span><span class="st">n  * [Status](https://status.fireworks.ai/)</span><span class="ch">\\</span><span class="st">n  * [Dashboard](https://app.fireworks.ai/login)</span><span class="ch">\\</span><span class="st">n  * [Dashboard](https://app.fireworks.ai/login)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSearch...</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nNavigation</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nModels &amp; Inference</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nText Models</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSearch...</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\u2318</span><span class="st">K</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##### Get Started</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Overview](/getting-started/introduction)</span><span class="ch">\\</span><span class="st">n  * [Serverless Quickstart](/getting-started/quickstart)</span><span class="ch">\\</span><span class="st">n  * [Deployments Quickstart](/getting-started/ondemand-quickstart)</span><span class="ch">\\</span><span class="st">n  * [Which model should I use?](/guides/recommended-models)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##### Deployments</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Deployments Overview](/guides/ondemand-deployments)</span><span class="ch">\\</span><span class="st">n  * [Autoscaling](/deployments/autoscaling)</span><span class="ch">\\</span><span class="st">n  * [Custom Models](/models/uploading-custom-models)</span><span class="ch">\\</span><span class="st">n  * [Quantization](/models/quantization)</span><span class="ch">\\</span><span class="st">n  * Advanced</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##### Models &amp; Inference</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Text Models](/guides/querying-text-models)</span><span class="ch">\\</span><span class="st">n  * [Vision Models](/guides/querying-vision-language-models)</span><span class="ch">\\</span><span class="st">n  * [Speech to Text Models](/guides/querying-asr-models)</span><span class="ch">\\</span><span class="st">n  * [Embeddings &amp; Reranking](/guides/querying-embeddings-models)</span><span class="ch">\\</span><span class="st">n  * [Tool Calling](/guides/function-calling)</span><span class="ch">\\</span><span class="st">n  * [Batch API](/guides/batch-inference)</span><span class="ch">\\</span><span class="st">n  * Advanced</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##### Fine Tuning</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Fine Tuning Overview](/fine-tuning/finetuning-intro)</span><span class="ch">\\</span><span class="st">n  * Supervised Fine Tuning</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Reinforcement Fine Tuning</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Deploying Fine Tuned Models](/fine-tuning/deploying-loras)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##### Administration</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Managing users](/accounts/users)</span><span class="ch">\\</span><span class="st">n  * [Custom SSO](/accounts/sso)</span><span class="ch">\\</span><span class="st">n  * [Service Accounts](/accounts/service-accounts)</span><span class="ch">\\</span><span class="st">n  * [Rate Limits &amp; Quotas](/guides/quotas_usage/rate-limits)</span><span class="ch">\\</span><span class="st">n  * [Exporting Billing Metrics](/accounts/exporting-billing-metrics)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##### Security &amp; Compliance</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Data Security &amp; Privacy](/guides/security_compliance/data_security)</span><span class="ch">\\</span><span class="st">n  * [Zero Data Retention](/guides/security_compliance/data_handling)</span><span class="ch">\\</span><span class="st">n  * [Audit &amp; Access Logs](/guides/security_compliance/audit_logs)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##### Integrations</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Cloud Integrations](/ecosystem/integrations)</span><span class="ch">\\</span><span class="st">n  * [Agent Frameworks](/ecosystem/integrations/agent-frameworks)</span><span class="ch">\\</span><span class="st">n  * [MLOps &amp; Observability](/ecosystem/integrations/mlops-observability)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOn this page</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Chat Completions API</span><span class="ch">\\</span><span class="st">n  * Alternative query methods</span><span class="ch">\\</span><span class="st">n  * Querying dedicated deployments</span><span class="ch">\\</span><span class="st">n  * Common patterns</span><span class="ch">\\</span><span class="st">n  * Multi-turn conversations</span><span class="ch">\\</span><span class="st">n  * System prompts</span><span class="ch">\\</span><span class="st">n  * Streaming responses</span><span class="ch">\\</span><span class="st">n  * Async requests</span><span class="ch">\\</span><span class="st">n  * Usage &amp; performance tracking</span><span class="ch">\\</span><span class="st">n  * Advanced capabilities</span><span class="ch">\\</span><span class="st">n  * Configuration &amp; debugging</span><span class="ch">\\</span><span class="st">n  * Understanding tokens</span><span class="ch">\\</span><span class="st">n  * OpenAI SDK Migration</span><span class="ch">\\</span><span class="st">n  * Next steps</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nModels &amp; Inference</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Text Models</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy page</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nQuery, track and manage inference for text models</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy page</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nNew to Fireworks? Start with the [Serverless Quickstart](/getting-started/quickstart) for a step-by-step guide to making your first API call.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFireworks provides fast, cost-effective access to leading open-source text models through OpenAI-compatible APIs. Query models via serverless inference or dedicated deployments using the chat completions API (recommended), completions API, or responses API. [Browse 100+ available models </span><span class="ch">\u2192</span><span class="st">](https://fireworks.ai/models)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nChat Completions API</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Python</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * JavaScript</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * curl</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nimport os</span><span class="ch">\\</span><span class="st">nfrom openai import OpenAI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nclient = OpenAI(</span><span class="ch">\\</span><span class="st">n    api_key=os.environ.get(</span><span class="ch">\"</span><span class="st">FIREWORKS_API_KEY</span><span class="ch">\"</span><span class="st">),</span><span class="ch">\\</span><span class="st">n    base_url=</span><span class="ch">\"</span><span class="st">https://api.fireworks.ai/inference/v1</span><span class="ch">\"\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\\</span><span class="st">n        </span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n        </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Explain quantum computing in simple terms</span><span class="ch">\"\\</span><span class="st">n    }]</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nprint(response.choices[0].message.content)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMost models automatically format your messages with the correct template. To verify the exact prompt used, enable the `echo` parameter.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAlternative query methods</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFireworks also supports [Completions API](/guides/completions-api) and [Responses API](/guides/response-api).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nQuerying dedicated deployments</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFor consistent performance, guaranteed capacity, or higher throughput, you can query [on-demand deployments](/guides/ondemand-deployments) instead of serverless models. Deployments use the same APIs with a deployment-specific model identifier:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n&lt;MODEL_NAME&gt;#&lt;DEPLOYMENT_NAME&gt;</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFor example:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1#accounts/&lt;ACCOUNT_ID&gt;/deployments/&lt;DEPLOYMENT_ID&gt;</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}]</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCommon patterns</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMulti-turn conversations</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMaintain conversation history by including all previous messages:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nmessages = [</span><span class="ch">\\</span><span class="st">n    {</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">system</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">You are a helpful assistant.</span><span class="ch">\"</span><span class="st">},</span><span class="ch">\\</span><span class="st">n    {</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">What</span><span class="ch">\\</span><span class="st">'s the capital of France?</span><span class="ch">\"</span><span class="st">},</span><span class="ch">\\</span><span class="st">n    {</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">assistant</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">The capital of France is Paris.</span><span class="ch">\"</span><span class="st">},</span><span class="ch">\\</span><span class="st">n    {</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">What</span><span class="ch">\\</span><span class="st">'s its population?</span><span class="ch">\"</span><span class="st">}</span><span class="ch">\\</span><span class="st">n]</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=messages</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nprint(response.choices[0].message.content)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nThe model uses the full conversation history to provide contextually relevant responses.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSystem prompts</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOverride the default system prompt by setting the first message with `role: </span><span class="ch">\"</span><span class="st">system</span><span class="ch">\"</span><span class="st">`:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nmessages = [</span><span class="ch">\\</span><span class="st">n    {</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">system</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">You are a helpful Python expert who provides concise code examples.</span><span class="ch">\"</span><span class="st">},</span><span class="ch">\\</span><span class="st">n    {</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">How do I read a CSV file?</span><span class="ch">\"</span><span class="st">}</span><span class="ch">\\</span><span class="st">n]</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=messages</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nTo completely omit the system prompt, set the first message</span><span class="ch">\u2019</span><span class="st">s `content` to an empty string.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nStreaming responses</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nStream tokens as they</span><span class="ch">\u2019</span><span class="st">re generated for real time, interactive UX. Covered in detail in the [Serverless Quickstart](/getting-started/quickstart#streaming-responses).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nstream = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Tell me a story</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    stream=True</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nfor chunk in stream:</span><span class="ch">\\</span><span class="st">n    if chunk.choices[0].delta.content:</span><span class="ch">\\</span><span class="st">n        print(chunk.choices[0].delta.content, end=</span><span class="ch">\"\"</span><span class="st">)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n**Aborting streams:** Close the connection to stop generation and avoid billing for ungenerated tokens:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nfor chunk in stream:</span><span class="ch">\\</span><span class="st">n    print(chunk.choices[0].delta.content, end=</span><span class="ch">\"\"</span><span class="st">)</span><span class="ch">\\</span><span class="st">n    if some_condition:</span><span class="ch">\\</span><span class="st">n        stream.close()</span><span class="ch">\\</span><span class="st">n        break</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsync requests</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nUse async clients to make multiple concurrent requests for better throughput:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Python</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * JavaScript</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nimport asyncio</span><span class="ch">\\</span><span class="st">nfrom openai import AsyncOpenAI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nclient = AsyncOpenAI(</span><span class="ch">\\</span><span class="st">n    api_key=os.environ.get(</span><span class="ch">\"</span><span class="st">FIREWORKS_API_KEY</span><span class="ch">\"</span><span class="st">),</span><span class="ch">\\</span><span class="st">n    base_url=</span><span class="ch">\"</span><span class="st">https://api.fireworks.ai/inference/v1</span><span class="ch">\"\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nasync def main():</span><span class="ch">\\</span><span class="st">n    response = await client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n        model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n        messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}]</span><span class="ch">\\</span><span class="st">n    )</span><span class="ch">\\</span><span class="st">n    print(response.choices[0].message.content)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nasyncio.run(main())</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nUsage &amp; performance tracking</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nEvery response includes token usage information and performance metrics for debugging and observability. For aggregate metrics over time, see the [usage dashboard](https://app.fireworks.ai/account/usage). **Token usage** (prompt, completion, total tokens) is included in the response body for all requests. **Performance metrics** (latency, time-to-first-token, etc.) are included in response headers for non-streaming requests. For streaming requests, use the `perf_metrics_in_response` parameter to include all metrics in the response body.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Non-streaming</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Streaming (usage only)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Streaming (with performance metrics)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}]</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Token usage (always included)</span><span class="ch">\\</span><span class="st">nprint(response.usage.prompt_tokens)      # Tokens in your prompt</span><span class="ch">\\</span><span class="st">nprint(response.usage.completion_tokens)  # Tokens generated</span><span class="ch">\\</span><span class="st">nprint(response.usage.total_tokens)       # Total tokens billed</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Performance metrics are in response headers:</span><span class="ch">\\</span><span class="st">n# fireworks-prompt-tokens, fireworks-server-time-to-first-token, etc.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nUsage information is automatically included in the final chunk for streaming responses (the chunk with `finish_reason` set). This is a Fireworks extension - OpenAI SDK doesn</span><span class="ch">\u2019</span><span class="st">t return usage for streaming by default.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFor all available metrics and details, see the [API reference documentation](/api-reference/post-chatcompletions#response-perf_metrics).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nIf you encounter errors during inference, see [Inference Error Codes](/guides/inference-error-codes) for common issues and resolutions.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAdvanced capabilities</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nExtend text models with additional features for structured outputs, tool integration, and performance optimization:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## [Tool callingConnect models to external tools and APIs with type-safe parameters](/guides/function-calling)## [Structured outputsEnforce JSON schemas for reliable data extraction](/structured-responses/structured-response-formatting)## [Responses APIMulti-step reasoning for complex problem-solving](/guides/response-api)## [Predicted outputsSpeed up edits by predicting unchanged sections](/guides/predicted-outputs)## [Prompt cachingCache common prompts to reduce latency and cost](/guides/prompt-caching)## [Batch inferenceProcess large volumes of requests asynchronously](/guides/batch-inference)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nConfiguration &amp; debugging</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSampling parameters</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nControl how the model generates text. Fireworks automatically uses recommended sampling parameters from each model</span><span class="ch">\u2019</span><span class="st">s HuggingFace `generation_config.json` when you don</span><span class="ch">\u2019</span><span class="st">t specify them explicitly, ensuring optimal performance out-of-the-box.We pull `temperature`, `top_k`, `top_p`, `min_p`, and `typical_p` from the model</span><span class="ch">\u2019</span><span class="st">s configuration when not explicitly provided.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nTemperature</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAdjust randomness (0 = deterministic, higher = more creative):</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Write a poem</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    temperature=0.7  # Override model default</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMax tokens</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nControl the maximum number of tokens in the generated completion:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nmax_tokens=100  # Generate at most 100 tokens</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n**Important notes:**</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Default value is 2048 tokens if not specified</span><span class="ch">\\</span><span class="st">n  * Most models support up to their full context window (e.g., 128K for DeepSeek R1)</span><span class="ch">\\</span><span class="st">n  * When the limit is reached, you</span><span class="ch">\u2019</span><span class="st">ll see `</span><span class="ch">\"</span><span class="st">finish_reason</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">length</span><span class="ch">\"</span><span class="st">` in the response</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSet `max_tokens` appropriately for your use case to avoid truncated responses. Check the model</span><span class="ch">\u2019</span><span class="st">s context window in the [Model Library](https://fireworks.ai/models).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nTop-p (nucleus sampling)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nConsider only the most probable tokens summing to `top_p` probability mass:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">ntop_p=0.9  # Consider top 90% probability mass</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nTop-k</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nConsider only the k most probable tokens:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">ntop_k=50  # Consider top 50 tokens</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMin-p</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nExclude tokens below a probability threshold:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nmin_p=0.05  # Exclude tokens with &lt;5% probability</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nTypical-p</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nUse typical sampling to select tokens with probability close to the entropy of the distribution:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">ntypical_p=0.95  # Consider tokens with typical probability</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nRepetition penalties</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nReduce repetitive text with `frequency_penalty`, `presence_penalty`, or `repetition_penalty`:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nfrequency_penalty=0.5,   # Penalize frequent tokens (OpenAI compatible)</span><span class="ch">\\</span><span class="st">npresence_penalty=0.5,    # Penalize any repeated token (OpenAI compatible)</span><span class="ch">\\</span><span class="st">nrepetition_penalty=1.1   # Exponential penalty from prompt + output</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSampling options header</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nThe `fireworks-sampling-options` header contains the actual default sampling parameters used for the model, including values from the model</span><span class="ch">\u2019</span><span class="st">s HuggingFace `generation_config.json`:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Python</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * JavaScript</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nimport os</span><span class="ch">\\</span><span class="st">nfrom openai import OpenAI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nclient = OpenAI(</span><span class="ch">\\</span><span class="st">n    api_key=os.environ.get(</span><span class="ch">\"</span><span class="st">FIREWORKS_API_KEY</span><span class="ch">\"</span><span class="st">),</span><span class="ch">\\</span><span class="st">n    base_url=</span><span class="ch">\"</span><span class="st">https://api.fireworks.ai/inference/v1</span><span class="ch">\"\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.with_raw_response.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}]</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Access headers from the raw response</span><span class="ch">\\</span><span class="st">nsampling_options = response.headers.get(</span><span class="ch">\\</span><span class="st">'fireworks-sampling-options</span><span class="ch">\\</span><span class="st">')</span><span class="ch">\\</span><span class="st">nprint(sampling_options)  # e.g., </span><span class="ch">\\</span><span class="st">'{</span><span class="ch">\"</span><span class="st">temperature</span><span class="ch">\"</span><span class="st">: 0.7, </span><span class="ch">\"</span><span class="st">top_p</span><span class="ch">\"</span><span class="st">: 0.9}</span><span class="ch">\\</span><span class="st">'</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">ncompletion = response.parse()  # get the parsed response object</span><span class="ch">\\</span><span class="st">nprint(completion.choices[0].message.content)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSee the [API reference](/api-reference/post-chatcompletions) for detailed parameter descriptions.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMultiple generations</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nGenerate multiple completions in one request:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Tell me a joke</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    n=3  # Generate 3 different jokes</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nfor choice in response.choices:</span><span class="ch">\\</span><span class="st">n    print(choice.message.content)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nToken probabilities (logprobs)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nInspect token probabilities for debugging or analysis:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    logprobs=True,</span><span class="ch">\\</span><span class="st">n    top_logprobs=5  # Show top 5 alternatives per token</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nfor content in response.choices[0].logprobs.content:</span><span class="ch">\\</span><span class="st">n    print(f</span><span class="ch">\"</span><span class="st">Token: {content.token}, Logprob: {content.logprob}</span><span class="ch">\"</span><span class="st">)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nPrompt inspection (echo &amp; raw_output)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nVerify how your prompt was formatted:**Echo:** Return the prompt along with the generation:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    echo=True</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n**Raw output:** See raw token IDs and prompt fragments:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nExperimental API - may change without notice.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    raw_output=True</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nprint(response.raw_output.prompt_token_ids)  # Token IDs</span><span class="ch">\\</span><span class="st">nprint(response.raw_output.completion)        # Raw completion</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nIgnore EOS token</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nForce generation to continue past the end-of-sequence token (useful for benchmarking):</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    ignore_eos=True,</span><span class="ch">\\</span><span class="st">n    max_tokens=100  # Will always generate exactly 100 tokens</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOutput quality may degrade when ignoring EOS. This API is experimental and should not be relied upon for production use cases.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nLogit bias</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nModify token probabilities to encourage or discourage specific tokens:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    logit_bias={</span><span class="ch">\\</span><span class="st">n        123: 10.0,   # Strongly encourage token ID 123</span><span class="ch">\\</span><span class="st">n        456: -50.0   # Strongly discourage token ID 456</span><span class="ch">\\</span><span class="st">n    }</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMirostat sampling</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nControl perplexity dynamically using the [Mirostat algorithm](https://arxiv.org/abs/2007.14966):</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nresponse = client.chat.completions.create(</span><span class="ch">\\</span><span class="st">n    model=</span><span class="ch">\"</span><span class="st">accounts/fireworks/models/deepseek-v3p1</span><span class="ch">\"</span><span class="st">,</span><span class="ch">\\</span><span class="st">n    messages=[{</span><span class="ch">\"</span><span class="st">role</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">user</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">content</span><span class="ch">\"</span><span class="st">: </span><span class="ch">\"</span><span class="st">Hello</span><span class="ch">\"</span><span class="st">}],</span><span class="ch">\\</span><span class="st">n    mirostat_target=5.0,  # Target perplexity</span><span class="ch">\\</span><span class="st">n    mirostat_lr=0.1       # Learning rate for adjustments</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nUnderstanding tokens</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nLanguage models process text in chunks called **tokens**. In English, a token can be as short as one character or as long as one word. Different model families use different **tokenizers** , so the same text may translate to different token counts depending on the model. **Why tokens matter:**</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Models have maximum context lengths measured in tokens</span><span class="ch">\\</span><span class="st">n  * Pricing is based on token usage (prompt + completion)</span><span class="ch">\\</span><span class="st">n  * Token count affects response time</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFor Llama models, use [this tokenizer tool](https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/) to estimate token counts. Actual usage is returned in the `usage` field of every API response.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOpenAI SDK Migration</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOpenAI SDK compatibility notes</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFireworks provides an OpenAI-compatible API, making migration straightforward. However, there are some minor differences to be aware of:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nBehavioral differences</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n**`stop` parameter:**</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * **Fireworks** : Returns text including the stop word</span><span class="ch">\\</span><span class="st">n  * **OpenAI** : Omits the stop word</span><span class="ch">\\</span><span class="st">n  * _You can easily truncate it client-side if needed_</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n**`max_tokens` with context limits:**</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * **Fireworks** : Automatically adjusts `max_tokens` lower if `prompt + max_tokens` exceeds the model</span><span class="ch">\u2019</span><span class="st">s context window</span><span class="ch">\\</span><span class="st">n  * **OpenAI** : Returns an invalid request error</span><span class="ch">\\</span><span class="st">n  * _Control this behavior with the`context_length_exceeded_behavior` parameter_</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n**Streaming usage stats:**</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * **Fireworks** : Returns `usage` field in the final chunk (where `finish_reason` is set) for both streaming and non-streaming</span><span class="ch">\\</span><span class="st">n  * **OpenAI** : Only returns usage for non-streaming responses</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nExample accessing streaming usage:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCopy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAsk AI</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nfor chunk in client.chat.completions.create(stream=True, ...):</span><span class="ch">\\</span><span class="st">n    if chunk.usage:  # Available in final chunk</span><span class="ch">\\</span><span class="st">n        print(f</span><span class="ch">\"</span><span class="st">Tokens: {chunk.usage.total_tokens}</span><span class="ch">\"</span><span class="st">)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nUnsupported parameters</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nThe following OpenAI parameters are not yet supported:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * `presence_penalty`</span><span class="ch">\\</span><span class="st">n  * `frequency_penalty`</span><span class="ch">\\</span><span class="st">n  * `best_of` (use `n` instead)</span><span class="ch">\\</span><span class="st">n  * `logit_bias`</span><span class="ch">\\</span><span class="st">n  * `functions` (deprecated - use [Tool Calling](/guides/function-calling) with the `tools` parameter instead)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nHave a use case requiring one of these? [Join our Discord](https://discord.gg/fireworks-ai) to discuss.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n##</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">u200b</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nNext steps</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## [Vision modelsProcess images alongside text](/guides/querying-vision-language-models)## [Audio modelsTranscribe and translate audio](/guides/querying-asr-models)## [EmbeddingsGenerate vector representations for search](/guides/querying-embeddings-models)## [On-demand deploymentsDeploy models on dedicated GPUs](/guides/ondemand-deployments)## [Fine-tuningCustomize models for your use case](/fine-tuning/finetuning-intro)## [Error codesTroubleshoot common inference errors](/guides/inference-error-codes)## [API ReferenceComplete API documentation](/api-reference/post-chatcompletions)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nWas this page helpful?</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nYesNo</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[Exporting MetricsPrevious](/deployments/exporting-metrics)[Vision ModelsNext](/guides/querying-vision-language-models)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\u2318</span><span class="st">I</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[Fireworks AI Docs home page![light logo](https://mintcdn.com/firew&lt;TRUNCATED&gt;"</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<p>I don‚Äôt see specific documentation for <code>kimi-k2-thinking</code> model parameters in that page. Let me search for more specific information about thinking models and prefilling:Let me search more specifically for prefilling and JSON formatting with thinking models:Based on my search, I don‚Äôt see specific documentation about prefilling or thinking mode parameters for the <code>kimi-k2-thinking</code> model on Fireworks. However, here‚Äôs what I found:</p>
<p><a href="https://docs.fireworks.ai/api-reference/post-chatcompletions" title="Create Chat Completion - Fireworks AI Docs">*</a> <a href="https://docs.fireworks.ai/api-reference/post-chatcompletions" title="Create Chat Completion - Fireworks AI Docs">*</a> For JSON formatting, Fireworks supports <code>response_format</code> with <code>{"type": "json_object"}</code> to guarantee valid JSON output. <a href="https://docs.fireworks.ai/api-reference/post-chatcompletions" title="Create Chat Completion - Fireworks AI Docs">*</a> You should also instruct the model to produce JSON via a system or user message, otherwise it may generate whitespace.</p>
<p>For prefilling with <code>{</code>, this is typically an Anthropic-specific feature that may not be available in the Fireworks API. The standard approach would be:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>response_format<span class="op">=</span>{<span class="st">"type"</span>: <span class="st">"json_object"</span>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>As for ‚Äúthinking mode‚Äù - <a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking" title="moonshotai/Kimi-K2-Thinking ¬∑ Hugging Face">*</a> Kimi K2 Thinking is built as a thinking agent that reasons step-by-step, so the thinking behavior should be built-in. You likely don‚Äôt need a special parameter to enable it.</p>
<p>Would you like to try using <code>response_format</code> for JSON output instead of prefilling?</p>
<p>Yes how to change the generator code?</p>
</section>
<section id="reply-36" class="level5">
<h5 class="anchored" data-anchor-id="reply-36">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You‚Äôll need to modify the <a href="https://anubhavmaity.github.io/wattbot/generator.html#generate"><code>generate</code></a> method in your <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#generator"><code>Generator</code></a> class to add the <code>response_format</code> parameter. Here‚Äôs the key change:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> <span class="va">self</span>.llm_client.chat.completions.create(</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">**</span>formatted_prompt[<span class="st">'messages'</span>],</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    response_format<span class="op">=</span>{<span class="st">"type"</span>: <span class="st">"json_object"</span>}</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>However, I notice your current code is calling <code>formatted_prompt['messages']</code> directly - can you show me what your <code>prompt_template.build()</code> method returns? That will help me give you the exact syntax you need.</p>
<div id="e23bf584" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>loaded_prompt <span class="op">=</span> braintrust.load_prompt(project<span class="op">=</span><span class="st">"wattbot"</span>, slug<span class="op">=</span><span class="st">"wattbot-prompt-v1"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5177b082" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>qa.iloc[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>id                                                                   q003
question                What is the name of the benchmark suite presen...
answer                                            The ML.ENERGY Benchmark
answer_value                                          ML.ENERGY Benchmark
answer_unit                                                      is_blank
ref_id                                                      ['chung2025']
ref_url                              ['https://arxiv.org/pdf/2505.06371']
supporting_materials    We present the ML.ENERGY Benchmark, a benchmar...
explanation                                                         Quote
Name: 0, dtype: object</code></pre>
</div>
</div>
<div id="0ec57f70" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>query_text <span class="op">=</span> <span class="st">'What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?'</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>retrieved_chunks <span class="op">=</span> Retriever(embedder, qs).retrieve(query_text, limit<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Generator(llm, loaded_prompt)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>generator.generate(query_text, retrieved_chunks)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>{'answer': 'ML.ENERGY Benchmark',
 'answer_value': 'ML.ENERGY Benchmark',
 'answer_unit': 'is_blank',
 'ref_id': ['chung2025'],
 'ref_url': ['https://arxiv.org/pdf/2505.06371'],
 'supporting_materials': 'We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments',
 'explanation': 'Quote'}</code></pre>
</div>
</div>
</section>
</section>
<section id="rag" class="level2">
<h2 class="anchored" data-anchor-id="rag">RAG</h2>
<div id="416872f7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RAG:</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, retriever, generator):</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>        fc.store_attr()</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> answer_question(<span class="va">self</span>, query, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>        retrieved_chunks <span class="op">=</span> <span class="va">self</span>.retriever.retrieve(query, limit<span class="op">=</span>limit)</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.generator.generate(query, retrieved_chunks), retrieved_chunks</span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_dataset(<span class="va">self</span>, df, limit<span class="op">=</span><span class="dv">5</span>, n_workers<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> process_row(idx_r):</span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>            idx, r <span class="op">=</span> idx_r</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>            answer, retrieved_chunks <span class="op">=</span> <span class="va">self</span>.answer_question(r[<span class="st">'question'</span>], limit<span class="op">=</span>limit)</span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(answer, <span class="bu">dict</span>): <span class="bu">print</span>(idx)</span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>                <span class="st">'id'</span>: r[<span class="st">'id'</span>],</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>                <span class="st">'question'</span>: r[<span class="st">'question'</span>],</span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>                <span class="st">'answer'</span>: answer[<span class="st">'answer'</span>],</span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>                <span class="st">'answer_value'</span>: answer[<span class="st">'answer_value'</span>],</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>                <span class="st">'answer_unit'</span>: answer[<span class="st">'answer_unit'</span>],</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>                <span class="st">'ref_id'</span>: answer[<span class="st">'ref_id'</span>],</span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>                <span class="st">'ref_url'</span>: answer[<span class="st">'ref_url'</span>],</span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>                <span class="st">'supporting_materials'</span>: answer[<span class="st">'supporting_materials'</span>],</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>                <span class="st">'explanation'</span>: answer[<span class="st">'explanation'</span>]</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> fc.parallel(process_row, df.iterrows(), threadpool<span class="op">=</span><span class="va">True</span>, n_workers<span class="op">=</span>n_workers)</span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.DataFrame(<span class="bu">list</span>(results))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="6a333fcb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>rag <span class="op">=</span> RAG(retriever, generator)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>pred_answer, retrieved_chunks <span class="op">=</span> rag.answer_question(query_text, limit<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span> pred_answer</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>{'answer': 'ML.ENERGY Benchmark',
 'answer_value': 'ML.ENERGY Benchmark',
 'answer_unit': 'is_blank',
 'ref_id': ['chung2025'],
 'ref_url': ['https://arxiv.org/pdf/2505.06371'],
 'supporting_materials': 'The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization',
 'explanation': 'Quote'}</code></pre>
</div>
</div>
</section>
<section id="evaluation" class="level2">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<div id="52c08a87" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_numeric(value):</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">float</span>(value)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> (<span class="pp">ValueError</span>, <span class="pp">TypeError</span>):</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">False</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5a6ec9c1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> score_answer_value(predicted, expected):</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> expected <span class="op">==</span> <span class="st">"is_blank"</span>:</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> predicted <span class="op">==</span> <span class="st">"is_blank"</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(expected, <span class="bu">str</span>) <span class="kw">and</span> expected.startswith(<span class="st">'['</span>): expected <span class="op">=</span> ast.literal_eval(expected)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_numeric(expected) <span class="kw">and</span> is_numeric(predicted):</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>        pred_num, exp_num <span class="op">=</span> <span class="bu">map</span>(<span class="bu">float</span>, (predicted, expected))</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> <span class="bu">abs</span>(pred_num <span class="op">-</span> exp_num) <span class="op">&lt;=</span> <span class="bu">abs</span>(exp_num <span class="op">*</span> <span class="fl">0.001</span>) <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> <span class="bu">str</span>(predicted).strip().lower() <span class="op">==</span> <span class="bu">str</span>(expected).strip().lower() <span class="cf">else</span> <span class="fl">0.0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="177d3716" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>score_answer_value([<span class="dv">80</span>,<span class="dv">90</span>], <span class="st">"[80,90]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>1.0</code></pre>
</div>
</div>
<p>I want to change the answer_value to evaluate the ranges properly</p>
<section id="reply-37" class="level5">
<h5 class="anchored" data-anchor-id="reply-37">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>To handle range evaluation properly, you need to detect when the expected value is a range (like <code>[80,90]</code>) and check if the predicted value falls within that range.</p>
<p>Here are a few approaches:</p>
<ol type="1">
<li>Parse ranges and check if predicted value is within bounds</li>
<li>Handle both list formats <code>[80,90]</code> and string formats <code>"[80,90]"</code></li>
<li>Consider whether predicted should be a single value in the range or also a range</li>
</ol>
<p>What format is your predicted value in - is it a single number, a list, or a string? And should it match the exact range or just fall within it?</p>
<p>The answer value will be a list but the dataframe can have string of list</p>
</section>
<section id="reply-38" class="level5">
<h5 class="anchored" data-anchor-id="reply-38">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Got it! So you need to:</p>
<ol type="1">
<li>Parse the string representation of a list (like <code>"[80,90]"</code>) into an actual list</li>
<li>Compare two lists for equality</li>
</ol>
<p>You can use <code>ast.literal_eval()</code> to safely parse the string into a list:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ast</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>parsed <span class="op">=</span> ast.literal_eval(<span class="st">"[80,90]"</span>)  <span class="co"># Returns [80, 90]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then compare the predicted list with the parsed expected list. Would you like to update the <a href="https://anubhavmaity.github.io/wattbot/evaluate.html#score_answer_value"><code>score_answer_value</code></a> function to handle this, or would you prefer to try it yourself first?</p>
<p>Please update it</p>
</section>
<section id="reply-39" class="level5">
<h5 class="anchored" data-anchor-id="reply-39">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Here‚Äôs the updated function to handle list comparisons:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> score_answer_value(predicted, expected):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> expected <span class="op">==</span> <span class="st">"is_blank"</span>:</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> predicted <span class="op">==</span> <span class="st">"is_blank"</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Handle list/range comparisons</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(expected, <span class="bu">str</span>) <span class="kw">and</span> expected.startswith(<span class="st">'['</span>):</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>            expected <span class="op">=</span> ast.literal_eval(expected)</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> (<span class="pp">ValueError</span>, <span class="pp">SyntaxError</span>):</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(expected, <span class="bu">list</span>) <span class="kw">and</span> <span class="bu">isinstance</span>(predicted, <span class="bu">list</span>):</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> predicted <span class="op">==</span> expected <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_numeric(expected) <span class="kw">and</span> is_numeric(predicted):</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>        pred_num, exp_num <span class="op">=</span> <span class="bu">map</span>(<span class="bu">float</span>, (predicted, expected))</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> <span class="bu">abs</span>(pred_num <span class="op">-</span> exp_num) <span class="op">&lt;=</span> <span class="bu">abs</span>(exp_num <span class="op">*</span> <span class="fl">0.001</span>) <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> <span class="bu">str</span>(predicted).strip().lower() <span class="op">==</span> <span class="bu">str</span>(expected).strip().lower() <span class="cf">else</span> <span class="fl">0.0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This adds list handling before the numeric check. Try it with your test case and let me know if it works!</p>
<div id="d2df2ed3" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>score_answer_value(pred_answer[<span class="st">'answer_value'</span>], qa.iloc[<span class="dv">0</span>][<span class="st">'answer_value'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>1.0</code></pre>
</div>
</div>
<div id="64e6b223" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> score_ref_id(predicted, expected):</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> expected <span class="op">==</span> <span class="st">"is_blank"</span>:</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> predicted <span class="op">==</span> <span class="st">"is_blank"</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(expected, <span class="bu">str</span>) <span class="kw">and</span> expected.startswith(<span class="st">'['</span>): expected <span class="op">=</span> ast.literal_eval(expected)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    pred_set <span class="op">=</span> <span class="bu">set</span>(predicted) <span class="cf">if</span> <span class="bu">isinstance</span>(predicted, <span class="bu">list</span>) <span class="cf">else</span> <span class="bu">set</span>([predicted])</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    exp_set <span class="op">=</span> <span class="bu">set</span>(expected) <span class="cf">if</span> <span class="bu">isinstance</span>(expected, <span class="bu">list</span>) <span class="cf">else</span> <span class="bu">set</span>([expected])</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    intersection <span class="op">=</span> <span class="bu">len</span>(pred_set.intersection(exp_set))</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    union <span class="op">=</span> <span class="bu">len</span>(pred_set.union(exp_set))</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> intersection <span class="op">/</span> union <span class="cf">if</span> union <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c0b887de" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> score_is_na(predicted_answer, expected_answer):</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    expected_is_na <span class="op">=</span> expected_answer[<span class="st">'answer_value'</span>] <span class="op">==</span> <span class="st">'is_blank'</span></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    predicted_is_na <span class="op">=</span> predicted_answer[<span class="st">'answer_value'</span>] <span class="op">==</span> <span class="st">'is_blank'</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> expected_is_na <span class="op">==</span> predicted_is_na <span class="cf">else</span> <span class="fl">0.0</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="416b802b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_wattbot_score(predicted_answer, expected_row):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'answer_value'</span> <span class="kw">not</span> <span class="kw">in</span> predicted_answer: <span class="bu">print</span>(expected_row[<span class="st">'id'</span>], predicted_answer)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    answer_score <span class="op">=</span> score_answer_value(predicted_answer[<span class="st">'answer_value'</span>], expected_row[<span class="st">'answer_value'</span>])</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    ref_score <span class="op">=</span> score_ref_id(predicted_answer[<span class="st">'ref_id'</span>], expected_row[<span class="st">'ref_id'</span>])</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    na_score <span class="op">=</span> score_is_na(predicted_answer, expected_row)</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> <span class="fl">0.75</span> <span class="op">*</span> answer_score <span class="op">+</span> <span class="fl">0.15</span> <span class="op">*</span> ref_score <span class="op">+</span> <span class="fl">0.10</span> <span class="op">*</span> na_score</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fc.NS(score<span class="op">=</span>score, answer_score<span class="op">=</span>answer_score, ref_score<span class="op">=</span>ref_score, na_score<span class="op">=</span>na_score)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="64e39783" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>calculate_wattbot_score(pred_answer, qa.iloc[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>namespace(score=1.0, answer_score=1.0, ref_score=1.0, na_score=1.0)</code></pre>
</div>
</div>
<div id="f754c8d7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Evaluator:</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, rag):</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>        fc.store_attr()</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.results <span class="op">=</span> []</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> evaluate_dataset(<span class="va">self</span>, df, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>        experiment <span class="op">=</span> braintrust.init(project<span class="op">=</span><span class="st">"wattbot"</span>, experiment<span class="op">=</span><span class="st">"evaluation"</span>)</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> evaluate_row(idx_r):</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>            idx, r <span class="op">=</span> idx_r</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>            predicted_answer, retrieved_chunks <span class="op">=</span> <span class="va">self</span>.rag.answer_question(r[<span class="st">'question'</span>], limit<span class="op">=</span>limit)</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>            wattbot_score <span class="op">=</span> calculate_wattbot_score(predicted_answer, r)</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>            context <span class="op">=</span> <span class="va">self</span>.rag.generator.create_context(retrieved_chunks)</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>            prompt <span class="op">=</span> <span class="va">self</span>.rag.generator.format_prompt(r[<span class="st">'question'</span>], context)</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>            experiment.log(<span class="bu">input</span><span class="op">=</span>r[<span class="st">'question'</span>], output<span class="op">=</span>predicted_answer, expected<span class="op">=</span>r, scores<span class="op">=</span>wattbot_score.__dict__, metadata <span class="op">=</span> {<span class="st">'context'</span>: context, <span class="st">'prompt'</span>: prompt})</span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> fc.NS(qid<span class="op">=</span>r[<span class="st">'id'</span>], q<span class="op">=</span>r[<span class="st">'question'</span>], predicted<span class="op">=</span>predicted_answer, expected<span class="op">=</span>r, <span class="op">**</span>wattbot_score.__dict__, context<span class="op">=</span>context)</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.results <span class="op">=</span> fc.parallel(evaluate_row, df.iterrows(), threadpool<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">sum</span>(r.score <span class="cf">for</span> r <span class="kw">in</span> <span class="va">self</span>.results) <span class="op">/</span> <span class="bu">len</span>(<span class="va">self</span>.results)</span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_worst_predictions(<span class="va">self</span>, n<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>        sorted_results <span class="op">=</span> <span class="bu">sorted</span>(<span class="va">self</span>.results, key<span class="op">=</span><span class="kw">lambda</span> r: r.score)</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sorted_results[:n]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>How can I have the context and prompt logged into braintrust respecting the newlines?</p>
</section>
<section id="reply-40" class="level5">
<h5 class="anchored" data-anchor-id="reply-40">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>The newlines in your context and prompt strings might not be preserved when logged to Braintrust. To ensure they‚Äôre displayed properly, you can try:</p>
<ol type="1">
<li><strong>Store as a list instead of a single string</strong> - Break the context into separate chunks</li>
<li><strong>Use explicit newline encoding</strong> - Though this usually isn‚Äôt necessary</li>
<li><strong>Check Braintrust‚Äôs UI settings</strong> - Some UIs have options to preserve formatting</li>
</ol>
<p>The simplest approach is to log the context as a structured object:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>metadata <span class="op">=</span> {</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'context_chunks'</span>: [chunk.payload <span class="cf">for</span> chunk <span class="kw">in</span> retrieved_chunks],</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'prompt'</span>: prompt</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Would you like to try this approach, or would you prefer to keep it as a single formatted string and investigate Braintrust‚Äôs display options?</p>
<p>In braintrust for some examples it is respecting the new line and for some it does not. Is there an issue on our code?</p>
</section>
<section id="reply-41" class="level5">
<h5 class="anchored" data-anchor-id="reply-41">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>That‚Äôs interesting - inconsistent newline handling suggests the issue might be with how the data is being formatted before logging.</p>
<p>A few things to check:</p>
<ol type="1">
<li>Are the newlines actually in the strings (<code>\n</code>) or are they literal line breaks?</li>
<li>Is the <code>context</code> or <code>prompt</code> being processed differently in some cases (e.g., through JSON serialization)?</li>
<li>Are you logging strings vs dict values differently?</li>
</ol>
<p>Could you show me an example of one that works and one that doesn‚Äôt? Or check if the <a href="https://anubhavmaity.github.io/wattbot/generator.html#create_context"><code>create_context</code></a> method is consistently returning strings with <code>\n</code> characters?</p>
<p>Yes please</p>
</section>
<section id="reply-42" class="level5">
<h5 class="anchored" data-anchor-id="reply-42">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>The core issue is that <code>self.rag.retrieved_chunks</code> is shared state that gets overwritten by concurrent threads. Here are two solutions:</p>
<p><strong>Option 1: Make RAG return chunks along with the answer</strong></p>
<p>Modify your <code>RAG.answer_question</code> method to return both the answer and the chunks:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> answer_question(<span class="va">self</span>, query, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>    retrieved_chunks <span class="op">=</span> <span class="va">self</span>.retriever.retrieve(query, limit<span class="op">=</span>limit)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> <span class="va">self</span>.generator.generate(query, retrieved_chunks)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> answer, retrieved_chunks</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then update your evaluator to use the returned chunks instead of the shared state.</p>
<p><strong>Option 2: Remove threading</strong></p>
<p>Change <code>threadpool=True</code> to <code>threadpool=False</code> in your <code>fc.parallel</code> call. This makes execution sequential, avoiding the race condition entirely.</p>
<p>Which approach would you prefer? Option 1 is better if you want to keep parallel execution for speed, while Option 2 is simpler if speed isn‚Äôt critical.</p>
<p>Do you see any other race condition problem with the code?</p>
</section>
<section id="reply-43" class="level5">
<h5 class="anchored" data-anchor-id="reply-43">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes! Looking at your code more carefully, there‚Äôs another race condition in the <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#generator"><code>Generator</code></a> class and the <code>braintrust.init</code> call:</p>
<ol type="1">
<li><p><strong>Braintrust experiment object</strong>: You‚Äôre creating <code>experiment = braintrust.init(...)</code> once, but then multiple threads are calling <code>experiment.log()</code> simultaneously. This could cause issues depending on whether Braintrust‚Äôs logging is thread-safe.</p></li>
<li><p><strong>LLM client state</strong>: Your <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#generator"><code>Generator</code></a> uses <code>self.llm_client</code>, and if multiple threads call <a href="https://anubhavmaity.github.io/wattbot/generator.html#generate"><code>generate()</code></a> at the same time, they‚Äôre all sharing the same client connection. This might work depending on the client implementation, but could cause problems.</p></li>
</ol>
<p>Do you want to explore solutions for these, or would you like to first check if there are other issues you‚Äôve noticed?</p>
<div id="6c4d97dc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>evals <span class="op">=</span> Evaluator(rag)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>evals.evaluate_dataset(df<span class="op">=</span>qa, limit<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Skipping git metadata. This is likely because the repository has not been published to a remote yet. Remote named 'origin' didn't exist</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>0.7838414634146342</code></pre>
</div>
</div>
<p>Why some of the outputs are empty? This is for the kimi k2 thinking model whereas the deepseek model gives proper output</p>
</section>
<section id="reply-44" class="level5">
<h5 class="anchored" data-anchor-id="reply-44">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>The empty outputs (<code>{}</code>) from the Kimi K2 Thinking model suggest it‚Äôs not returning valid JSON in some cases. This could be due to:</p>
<ol type="1">
<li><strong>Thinking tokens interfering with JSON</strong> - The model might be outputting reasoning/thinking text before the JSON, breaking the <code>json.loads()</code> parsing</li>
<li><strong>Response format not properly enforced</strong> - The <code>response_format={"type": "json_object"}</code> might not be working as expected with this model</li>
<li><strong>Model-specific behavior</strong> - Thinking models sometimes structure their output differently</li>
</ol>
<p>To debug this, you could:</p>
<ol type="1">
<li>Print the raw response content before parsing to see what the model is actually returning</li>
<li>Try adding error handling to catch and log failed parses</li>
<li>Check if the prompt needs to be more explicit about JSON-only output for thinking models</li>
</ol>
<p>Would you like to add some debugging to see what the raw responses look like?</p>
<div id="3095dae6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>tst_ans <span class="op">=</span> rag.predict_dataset(tst)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="113b8201" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>tst_ans.isnull().<span class="bu">sum</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>id                      0
question                0
answer                  0
answer_value            0
answer_unit             0
ref_id                  0
ref_url                 0
supporting_materials    0
explanation             0
dtype: int64</code></pre>
</div>
</div>
<div id="fcd81317" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>tst_ans.to_csv(data_path<span class="op">/</span><span class="st">'tst_ans_v5.csv'</span>, index<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>What are other ways to improve this model?</p>
</section>
<section id="reply-45" class="level5">
<h5 class="anchored" data-anchor-id="reply-45">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great question! Here are several ways you could improve your RAG model‚Äôs performance:</p>
<p><strong>Retrieval improvements:</strong> - Experiment with different chunk sizes and overlap (your current <code>chunk_size=1500, step=1400</code>) - Try different embedding models or ensemble multiple embeddings - Adjust the hybrid search fusion weights between dense and sparse - Increase the number of retrieved chunks and use reranking</p>
<p><strong>Generation improvements:</strong> - Experiment with different LLMs or prompt engineering - Add few-shot examples to your prompt template - Implement answer verification or self-consistency checks</p>
<p><strong>Evaluation-driven improvements:</strong> - Analyze your worst predictions (you already have <code>get_worst_predictions</code>) - Look for patterns in what types of questions fail</p>
<p>Which area interests you most, or would you like to start by examining your worst predictions to see where the model struggles?</p>
<div id="7ba179f1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>evals.get_worst_predictions()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>[namespace(qid='q280',
           q='Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.',
           predicted={'answer': 'Unanswerable from the provided context',
                      'answer_value': 'is_blank',
                      'answer_unit': 'is_blank',
                      'ref_id': ['is_blank'],
                      'ref_url': ['is_blank'],
                      'supporting_materials': 'is_blank',
                      'explanation': 'is_blank'},
           expected=id                                                                   q280
                    question                Given the total pre-training GPU hours and the...
                    answer                                                           ~13 days
                    answer_value                                                           13
                    answer_unit                                                          days
                    ref_id                                                       ['shen2024']
                    ref_url                              ['https://arxiv.org/pdf/2404.07413']
                    supporting_materials    "‚Ä¶30,000 H100 GPU hours‚Ä¶ We conduct training o...
                    explanation             Math: wall_clock_hours ‚âà 30,000 GPUh √∑ 96 GPUs...
                    Name: 34, dtype: object,
           score=0.0,
           answer_score=0.0,
           ref_score=0.0,
           na_score=0.0,
           context='Doc ID: strubell2019\nURL: https://arxiv.org/pdf/1906.02243\nTitle: Energy and Policy Considerations for Deep Learning in NLP\nText: day . W e train all models on\na single NVIDIA Titan X GPU, with the excep-\ntion of ELMo which was trained on 3 NVIDIA\nGTX 1080 Ti GPUs. While training, we repeat-\nedly query the NVIDIA System Management In-\nterface\n2 to sample the GPU power consumption\nand report the average over all samples. T o sample\nCPU power consumption, we use Intel‚Äôs Running\nA verage Power Limit interface.\n3\n2 nvidia-smi: https://bit.ly/30sGEbi\n3 RAPL power meter: https://bit.ly/2LObQhV\nConsumer Renew . Gas Coal Nuc.\nChina 22% 3% 65% 4%\nGermany 40% 7% 38% 13%\nUnited States 17% 35% 27% 19%\nAmazon-A WS 17% 24% 30% 26%\nGoogle 56% 14% 15% 10%\nMicrosoft 32% 23% 31% 10%\nT able 2: Percent energy sourced from: Renewable (e.g.\nhydro, solar, wind), natural gas, coal and nuclear for\nthe top 3 cloud compute providers (\nCook et al. , 2017),\ncompared to the United States, 4 China5 and Germany\n(Burger, 2019).\nW e estimate the total time expected for mod-\nels to train to completion using training times and\nhardware reported in the original papers. W e then\ncalculate the power consumption in kilowatt-hours\n(kWh) as follows. Let pc be the average power\ndraw (in watts) from all CPU sockets during train-\ning, let pr be the average power draw from all\nDRAM (main memory) sockets, let pg be the aver-\nage power draw of a GPU during training, and let\ng be the number of GPUs used to train. W e esti-\nmate total power consumption as combined GPU,\nCPU and DRAM consumption, then multiply this\nby Power Usage Effectiveness (PUE), whi\n\nDoc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText: 2, June 21‚Äì24, 2022, Seoul, Republic of Korea\nModel BERT BERT 6B Dense Dense Dense ViT ViT ViT ViT ViT\nfinetune pretrain Transf. 121 169 201 Tiny Small Base Large Huge\nGPU 4¬∑V100 8¬∑V100 256¬∑A100 1¬∑P40 1¬∑P40 1¬∑P40 1¬∑V100 1¬∑V100 1¬∑V100 4¬∑V100 4¬∑V100\nHours 6 36 192 0.3 0.3 0.4 19 19 21 90 216\nkWh 3.1 37.3 13,812.4 0.02 0.03 0.04 1.7 2.2 4.7 93.3 237.6\nTable 2. For the 11 models in our analysis: the type of GPU, the number of GPUs of that type, the number of hours, and the energy\nused in kWh. For example, our BERT language modeling (BERT LM) experiment used 8 V100 GPUs for 36 hours and used a total of\n37.3 kWh. We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would\ntake to train to completion, we estimate a full training run would consume approximately 103,593 kWh.\n4.1 NLP\nBERT Training. We monitored the energy consumption while training a BERT-small model [8] for approximately 36\nhours on 8 NVIDIA V100 GPUs. That training run consumed over 37 kWh of electricity.\nBERT Finetuning. We tracked the energy consumption while finetuning the BERT-small model on a standard natural\nlanguage inference task [48, MNLI] for approximately 6 hours on 4 NVIDIA V100 GPUs. Our finetuning run consumed\naround 3.2 kWh of electricity, i.e., less than one tenth that due to BERT-small pre-training.\n6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising\nover 6.1 billion parameters dur\n\nDoc ID: strubell2019\nURL: https://arxiv.org/pdf/1906.02243\nTitle: Energy and Policy Considerations for Deep Learning in NLP\nText: n, an increase of just 0.1\nBLEU at the cost of at least $150k in on-demand\ncompute time and non-trivial carbon emissions.\n4.2 Cost of development: Case study\nT o quantify the computational requirements of\nR&amp;D for a new model we study the logs of\nall training required to develop Linguistically-\nInformed Self-Attention (\nStrubell et al. , 2018), a\nmulti-task model that performs part-of-speech tag-\nging, labeled dependency parsing, predicate detec-\ntion and semantic role labeling. This model makes\nfor an interesting case study as a representative\nNLP pipeline and as a Best Long Paper at EMNLP .\nModel training associated with the project\nspanned a period of 172 days (approx. 6 months).\nDuring that time 123 small hyperparameter grid\nsearches were performed, resulting in 4789 jobs\nin total. Jobs varied in length ranging from a min-\nimum of 3 minutes, indicating a crash, to a maxi-\nmum of 9 days, with an average job length of 52\nhours. All training was done on a combination of\nNVIDIA Titan X (72%) and M40 (28%) GPUs.\n8\nThe sum GPU time required for the project\ntotaled 9998 days (27 years). This averages to\n8 W e approximate cloud compute cost using P100 pricing.\nEstimated cost (USD)\nModels Hours Cloud compute Electricity\n1 120 $52‚Äì$175 $5\n24 2880 $1238‚Äì$4205 $118\n4789 239,942 $103k‚Äì$350k $9870\nT able 4: Estimated cost in terms of cloud compute and\nelectricity for training: (1) a single model (2) a single\ntune and (3) all models trained during R&amp;D.\nabout 60 GPUs running constantly th\n\nDoc ID: luccioni2023\nURL: https://arxiv.org/pdf/2302.08476\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\nText: e to ML model are rising.\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\nwhereas simpler and shallower models such as convolutional neural networks te\n\nDoc ID: cottier2024\nURL: https://arxiv.org/pdf/2405.21015\nTitle: The Rising Costs of Training Frontier AI Models\nText: of chips √ó\n\x10\n1 ‚àí exp\n\x02\n‚àí Training time √ó r ln 10\n\x03\x11\nwhere training time is in years. However, we could estimate chip-hours more often and more reliably than the training\ntime or the number of chips separately. This is because chip-hours can also be estimated from training compute in\nFLOP divided by the FLOP/s achieved during training. We used a linear approximation to take advantage of these\nchip-hour estimates:\nAmortized training cost = Start value per chip √ó Training chip-hours\n(365 √ó 24) hours/year √ó r ln 10\nThis approximation is valid if(Training time)√ór ln 10is small, and this is the case for the training times in our data and\nour choice of r = 0.14. In an extreme case, a training time of 1 year results in 1 √ó 0.14 ln(10)‚àº= 32%deprecation\ncompared to 1 ‚àí exp(‚àí1 √ó 0.14 ln(10))‚àº= 28%depreciation. This is not a large difference relative to other sources\nof uncertainty.\nDue to NVIDIA covering defects and component failures under warranty, we concluded that hardware failures are not a\nsignificant source of depreciation relative to hardware progress. As one data point, an average of 1 to 2 failures per\nweek occurred when training the BLOOM model on a cluster of 384 NVIDIA A100 GPUs [25]. Even if these were all\ncatastrophic failures, the expected hardware lifetime would be 3.7 years. We expect that NVIDIA replaces or repairs\ndefective GPUs on a faster timescale, which makes the cost of failure small compared to hardware price depreciation.\nA.4 Energy cost estimation\nTo model th\n\nDoc ID: luccioni2023\nURL: https://arxiv.org/pdf/2302.08476\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\nText:  [38] and it remains a fair approximation of the\nactual energy consumption of many hardware models. We provide more information about TDP and the hardware used\nfor training the models in our sample in Section A.2 of the Appendix.\nTraining Time. Training time was computed as the total number of hardware hours, which is different from the\n"wall time" of ML model training, since most models were trained on multiple units at once. For instance, if training a\n1For instance, methane is 28 times more potent than CO 2 based on its 100-year global warming potential, so energy generation emitting 1 gram of\nmethane per kWh will emit 28 grams of CO2eq per kWh.\nManuscript pending review Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 5\nmodel used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours ; a model using 8 GPUs for 48 hours will\ntherefore have an equivalent training time.\n4 DATA ANALYSIS\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the\nmain sources of energy used for training (¬ß 4.1), the order of magnitude of CO2 emissions produced (¬ß 4.2), the evolution\nof these emissions over time (¬ß 4.3) and the relationship between carbon emissions and model performance (¬ß 4.4) 2.\n4.1 What are the main sources of energy used for training ML models?\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity\nof that \n\nDoc ID: cottier2024\nURL: https://arxiv.org/pdf/2405.21015\nTitle: The Rising Costs of Training Frontier AI Models\nText:  of about $5,000.\n14 A.3 Amortization model\nAs explained in section 2.2, we estimated the value of the training hardware at the beginning of training as:\nStart value per chip = Acquisition cost per chip\nexp\n\x10\x02\nTraining start date ‚àí Hardware availability date\n\x03\n¬∑ r ln 10\n\x11\nwhere r is a depreciation rate in orders of magnitude per year, and the difference in dates is in years. The hardware\navailability date depended on the type of hardware. If the hardware was a Google TPU, we used the hardware\nannouncement date. For GPUs, we used a 90-day buffer between the GPU first going on the market and the GPU\nactually being shipped to the buyer. Our results are robust to variations in this buffer time‚Äîsee Appendix B.4.\nFor the training start date, there were a few known cases‚Äîfor example, GPT-4 finished training in August 2022 [12].\nOtherwise, we subtracted the training time from the publication date, and then subtracted a further 60 days to account\nfor time spent evaluating the model and writing the paper. Again, our results are robust to variations in this buffer. If the\ntraining time was unknown, we used the median of known values in our dataset, which was approximately 33 days.\nThe precise way to amortize the training cost through exponential depreciation is:\nAmortized training cost = Start value per chip √ó Number of chips √ó Depreciation during training\n= Start value per chip √ó Number of chips √ó\n\x10\n1 ‚àí exp\n\x02\n‚àí Training time √ó r ln 10\n\x03\x11\nwhere training time is in years. However, we cou\n\nDoc ID: wu2021a\nURL: https://arxiv.org/pdf/2111.00364\nTitle: Sustainable AI: Environmental Implications, Challenges and Opportunities\nText:  large collection of diverse ML\nideas are explored simultaneously at-scale. Thus, during this\nphase, we observe unique system resource requirements from\nthe large pool of training experiments. Within Facebook‚Äôs ML\nresearch cluster, 50% (p50) of ML training experiments take up\nto 1.5 GPU days while 99% (p99) of the experiments complete\nwithin 24 GPU days. There are a number of large-scale, trillion\nparameter models which require over 500 GPUs days.\nOnce a ML solution is determined as promising, it moves into\nTraining where the ML solution is evaluated using extensive\nproduction data ‚Äî data that is more recent, is larger in quantity,\nand contains richer features . The process often requires\nadditional hyper-parameter tuning. Depending on the ML task\nrequirement, the models can be trained/re-trained at different\nfrequencies. For example, models supporting Facebook‚ÄôsSearch\nservice were trained at an hourly cadence whereas the Language\nTranslation models were trained weekly [24]. A p50 production\nmodel training workÔ¨Çow takes 2.96 GPU days while a training\nworkÔ¨Çow at p99 can take up to 125 GPU days.\nFinally, for Inference, the best-performing model is de-\nployed, producing trillions of daily predictions to serve billions\nof users worldwide. The total compute cycles for inference\npredictions are expected to exceed the corresponding training\ncycles for the deployed model.\nB. Machine Learning System Life Cycle\nLife Cycle Analysis (LCA) is a common methodology to\nassess the carbon emis\n\nDoc ID: patterson2021\nURL: https://arxiv.org/pdf/2104.10350\nTitle: Carbon Emissions and Large Neural Network Training\nText: takes   ~14.8   days   for   10,000   \nGPUs   at   24.6   TeraFLOPS/sec   to   compute   3.14E+23   FLOPS.   For   the   CO 2 e   calculation,   it   doesn‚Äôt   \nactually   matter   whether   it   takes   2   weeks   on   10,000   GPUs   or   20   weeks   on   1,000   GPUs,   but   we   need   \none   number   for   Table   4,   so   we   used   NVIDIA‚Äôs   suggestion   of   10,000   GPUs.   \n‚óè Total   Computation   (Table   1,   row   13;   Table   4,   row   16):    We   calculate   from   measured   performance,   \nnumber   of   chips,   and   days   to   train   (except   for   GPT-3,   as   OpenAI   published   the   total   FLOPS).   \n‚óè %   of   Google   2019   Energy   Consumption.   (Table   4,   row   17):    For   all   models   (even   those   not   actually   run   \nin   Google   datacenters   or   not   run   in   2019),   we   calculate   the   percentage   of   Google‚Äôs   total   energy   \nconsumption   of   12.2   Terawatt-hours   in   2019   [Goo20].     \n‚óè Ratio   of   round   trips   (Table   4,   row   22) .   To   give   perspective   on   the   CO 2 e   cost   of   training   a   model   is   \ncompared   to   other   activities,   we   show   the   CO 2 e   of   passenger   jets.    Google   Flights    calculated   the   \naverage   CO 2    emission   for   all   the   direct   flights   between   San   Francisco   (SFO)   and   New   York   (JFK)   in   its   \ndatabase   as   90.2t,   so   the   average   round   trip   is   180.4t.   (This   is   for   the\n\nDoc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText: the energy consumption of training a large language model comprising\nover 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.\nThis model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate\nthe total energy consumption to train this model to completion would be approximately (60/8)‚àó13.8 = 103.5 MWh, or\n103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!\n4.2 Computer Vision\nDenseNets. We trained three sizes of DenseNets [19] on MNIST [25]. The jobs lasted between 20 and 25 minutes and\nconsumed between 20 and 38Wh (or 0.02 to 0.04 kWh) of electricity, which is negligible compared to the other models.\nVision Transformers. We evaluated the energy consumption during the training of five sizes of Vision Transformers [9]\non ImageNet [7]. For the smallest ViT experiment (ViT tiny), training lasted around 19 hours on a single V100 and\nconsumed approximately 1.7 kWh. For the largest one (ViT huge), training lasted more than 9 days on a 4 V100s and\nconsumed approximately 237 kWh. The full list of models can be found in Table 2.\n5 EMISSIONS BY REGION AND TIME OF DAY\nUsing the methodology presented above, we provide some of the first measurements of the differences of actual\ndatacenters from a major cloud provider. Importantly, what we have is a time series of marginal emissions: for example,\nif a job were to run from 1 pm to 5 pm in the US West region wit'),
 namespace(qid='q170',
           q='How many days of CO‚ÇÇ emissions from an average American life are equivalent to training BERT base?',
           predicted={'answer': '6 years, 1 month',
                      'answer_value': '6.083',
                      'answer_unit': 'years',
                      'ref_id': ['morrison2025'],
                      'ref_url': ['https://arxiv.org/pdf/2503.05804'],
                      'supporting_materials': 'Table 2: We list the estimated power usage, carbon emissions, and water consumption from training our dense transformers... Llama 2 7B 81 31 6 yrs, 1 mo - -',
                      'explanation': 'Table'},
           expected=id                                                                   q170
                    question                How many days of CO‚ÇÇ emissions from an average...
                    answer                                                               14.4
                    answer_value                                                         14.4
                    answer_unit                                                          days
                    ref_id                                                   ['strubell2019']
                    ref_url                              ['https://arxiv.org/pdf/1906.02243']
                    supporting_materials                                  Table 1 and Table 3
                    explanation                               1438 lbs / 99.8 lbs/day = 14.4 
                    Name: 18, dtype: object,
           score=0.1,
           answer_score=0.0,
           ref_score=0.0,
           na_score=1.0,
           context='Doc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText:  average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating\nelectricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.\n8 Measuring the Carbon Intensity of AI in Cloud Instances FAccT ‚Äô22, June 21‚Äì24, 2022, Seoul, Republic of Korea\nFig. 2. Emissions for our 11 experiments described in ¬ß4. For each model we show a vertical blue bar, where the top of the bar is\nthe max, the bottom is the min, and the black line represents the average emissions (across regions and time of year). First and\nfourth quartiles are represented by the light blue at the top and bottom of each vertical blue bar. The largest training runs (e.g., 6\nbillion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only\ntrained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest\nexperiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the\nUnited States Environmental Protection Agency [46].\nThe largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as\ndescribed in ¬ß4, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of\nthis size can emit more CO2 than all emissions from the average\n\nDoc ID: luccioni2024\nURL: https://arxiv.org/pdf/2311.16863\nTitle: Power Hungry Processing: Watts Driving the Cost of AI Deployment?\nText: s, architecture\nand carbon emissions of their products, we can make a comparison based on the experiments carried out in the\npresent study. For instance, the average emissions of a BERT-based model fine-tuned for extractive question answering\n(bert-large-uncased-whole-word-masking-finetuned-squad), a task akin to extractive web search, is 0.70gùê∂ùëÇ2ùëíùëû\nper 1,000 queries, which is less than 3 times that of the multi-purpose models (2.36g for Flan-T5 base and 2.34g for\nBLOOMz-560M). The difference is much more drastic if comparing BERT-based models for tasks such as text classification\nwith the larger multi-purpose models: for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of\nùê∂ùëÇ2ùëíùëûper 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B. For comparison, the first PaLM\nmodel, released in 2022, has 540 billion parameters [7], whereas GPT-3 has 175 billion parameters [5] 8. While we see\nthe benefit of deploying generative zero-shot models given their ability to carry out multiple tasks, we do not see\nconvincing evidence for the necessity of their deployment in contexts where tasks are well-defined, for instance web\nsearch and navigation, given these models‚Äô energy requirements.\nFinally, the intent of our study is to set the stage for better understanding of the energy requirements and carbon\nemissions of the final, often overlooked, step in the ML model life cycle: model deployment. The comparison between\ntraining, finetuning and inference energ\n\nDoc ID: luccioni2023\nURL: https://arxiv.org/pdf/2302.08476\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\nText: omparing the carbon emissions of two or more models and approaches. The\nfirst paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large\nTransformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the\nlifetime emissions of five US cars. [48]. This perspective has since been explored further via analyses of the carbon\nfootprint of different neural network architectures [31, 37, 38] and the relative efficiency of different methods [35, 56].\nThese empirical studies are very recent (post-2019), remain relatively sparse and biased towards certain research\nareas (i.e. Natural Language Processing), and there are many aspects of the emissions of model training that remain\nunexplored. In sum, there is a need for a more broad and multi-faceted analysis in order to better understand the scale\nand variation of carbon emissions in our community.\nTools and approaches for measuring carbon emissions. Developing standardized approaches for estimating the carbon\nemissions of model training has also been the focus of much work [5, 20, 26, 27, 30, 45, 51]. As a result, there are several\ntools that exist for this purpose, such as Code Carbon and the Experiment Impact Tracker, which can be used during the\nmodel training process, or the ML CO2 Calculator, which can be used after training, all of which provide an estimate\nof the amount of carbon emitted. However, a recent study on different ca\n\nDoc ID: luccioni2023\nURL: https://arxiv.org/pdf/2302.08476\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\nText: e to ML model are rising.\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\nwhereas simpler and shallower models such as convolutional neural networks te\n\nDoc ID: luccioni2025c\nURL: https://arxiv.org/pdf/2506.15572\nTitle: Misinformation by Omission: The Need for More Environmental Transparency in AI\nText:  31\nIn the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions\nintensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons)\nCO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.\nThe research article was written for a specialized audience of AI and NLP researchers, who would have the background\nknowledge to understand the appropriate scoping for the estimate. However, an author‚Äôs tweet publicizing the paper and\nfeaturing a table containing the ‚Äúfive cars‚Äù estimate was widely shared on social media, leading to the publication being picked\nup by numerous media outlets (including MIT Technology Review32 and Forbes33). The ‚Äúfive cars‚Äù number has since been\nmisinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of\narchitectures, training approaches and electricity sources used for powering AI model training; the original article reports AI\ntraining workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity),\nand AI model training more broadly often requires even less energy and corresponding emissions.\nFurther, the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much\nless frequently than the average AI model training w\n\nDoc ID: wu2021a\nURL: https://arxiv.org/pdf/2111.00364\nTitle: Sustainable AI: Environmental Implications, Challenges and Opportunities\nText:  carbon footprint of large-scale ML tasks (Figure 4). Taking into\naccount carbon-free energy, such as solar, the operational energy consumption\ncan be signiÔ¨Åcantly reduced, leaving the manufacturing carbon cost as the\ndominating source of AI‚Äôs carbon footprint.\nBoth Training and Inference can contribute signiÔ¨Åcantly to the\noverall carbon footprint of machine learning tasks at Facebook.\nThe exact breakdown between the two phases varies across\nML use cases.\nThe overall operational carbon footprint is categorized into\nofÔ¨Çine training, online training, and inference. OfÔ¨Çine training\nencompasses both experimentation and training models with\nhistorical data. Online training is particularly relevant to\nrecommendation models where parameters are continuously\nupdated based on recent data. The inference footprint represents\nthe emission from serving production trafÔ¨Åc. The online training\nand inference emissions are considered over the period of\nofÔ¨Çine training. For recommendation use cases, we Ô¨Ånd the\ncarbon footprint is split evenly between training and inference.\nOn the other hand, the carbon footprint of LM is dominated\nby the inference phase, using much higher inference resources\n(65%) as compared to training (35%).\nBoth operational and embodied carbon emissions can con-\ntribute signiÔ¨Åcantly to the overall footprint of ML tasks .\nOperational Carbon Footprint: Across the life cycle of\nthe Facebook models shown in Figure 4, the average carbon\nfootprint is 1.8 √ó higher than that of th\n\nDoc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText: er line) at different times throughout the year.\nWhat do emissions look like across the 11 experiments described in ¬ß4? In Figure 2 we show results for all 11\nexperiments, which cover two BERT experiments (finetuning and language modeling), partial training of a 6.1 billion\nparameter Transformer, 3 sizes of DenseNets, and five sizes of Vision Transformers. Each experiment is represented by\na vertical blue bar showing the range of emissions that would be emitted for that experiment across different regions.\nThe top of the blue bar is the emissions from running that experiment in the region with the most emissions, the bottom\nis the emissions from running that experiment in the region with the least emissions, the black line represents the\naverage, and the light blue regions are the top and bottom quartiles.\nIn Figure 2 we also include estimates of equivalent sources of emissions per the United States Environmental\nProtection Agency [46]. One phone charge is estimated to emit 8.22 √ó10‚àí6 metric tons (using US national weighted\naverage CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 √ó10‚àí4 metric\ntons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed\nis estimated to emit 8.887 √ó10‚àí3 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons,\none average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from g\n\nDoc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText: com/Page.aspx?id=1012\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n6 Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n‚Ä† denotes unreleased models that were trained for various internal experi
========== TRUNCATED ==========
) kWh per GPU hour, b) CO 2 grams per GPU hour, and c) CO 2 grams per kWh. Here we\ncompare against [34] and [33] which report information about training especially large models. Their estimates also\n9 FAccT ‚Äô22, June 21‚Äì24, 2022, Seoul, Republic of Korea Dodge et al.\ninclude additional sources of CO2, like PUE (Power Usage Effectiveness) of their datacenters, so we expect their kWh per\nGPU hour and CO2 per GPU hour to be higher than our estimates (which only count the GPU electricity consumption).\nAcross our experiments, we find kWh per GPU hour to range from 0.07 to 0.28\n\nDoc ID: strubell2019\nURL: https://arxiv.org/pdf/1906.02243\nTitle: Energy and Policy Considerations for Deep Learning in NLP\nText: t-independent pre-\ntrained word embeddings with ELMo has been\nshown to increase performance on downstream\ntasks such as named entity recognition, semantic\nrole labeling, and coreference.\nPeters et al. (2018)\nreport that ELMo was trained on 3 NVIDIA GTX\n1080 GPUs for 2 weeks (336 hours).\nBERT .The BERT model (\nDevlin et al. , 2019) pro-\nvides a Transformer-based architecture for build-\ning contextual representations similar to ELMo,\nbut trained with a different language modeling ob-\njective. BERT substantially improves accuracy on\ntasks requiring sentence-level representations such\nas question answering and natural language infer-\nence.\nDevlin et al. (2019) report that the BERT\nbase model (110M parameters) was trained on 16\nTPU chips for 4 days (96 hours). NVIDIA reports\nthat they can train a BERT model in 3.3 days (79.2\nhours) using 4 DGX-2H servers, totaling 64 T esla\nV100 GPUs (\nForster et al. , 2019).\nGPT -2. This model is the latest edition of\nOpenAI‚Äôs GPT general-purpose token encoder,\nalso based on Transformer-style self-attention and\ntrained with a language modeling objective (\nRad-\nford et al. , 2019). By training a very large model\non massive data, Radford et al. (2019) show high\nzero-shot performance on question answering and\nlanguage modeling benchmarks. The large model\ndescribed in\nRadford et al. (2019) has 1542M pa-\nrameters and is reported to require 1 week (168\nhours) of training on 32 TPUv3 chips.\n6\n3 Related work\nThere is some precedent for work characterizin\n\nDoc ID: luccioni2023\nURL: https://arxiv.org/pdf/2302.08476\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\nText:  [38] and it remains a fair approximation of the\nactual energy consumption of many hardware models. We provide more information about TDP and the hardware used\nfor training the models in our sample in Section A.2 of the Appendix.\nTraining Time. Training time was computed as the total number of hardware hours, which is different from the\n"wall time" of ML model training, since most models were trained on multiple units at once. For instance, if training a\n1For instance, methane is 28 times more potent than CO 2 based on its 100-year global warming potential, so energy generation emitting 1 gram of\nmethane per kWh will emit 28 grams of CO2eq per kWh.\nManuscript pending review Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning 5\nmodel used 16 GPUs for 24 hours, this equals a training time of 384 GPU hours ; a model using 8 GPUs for 48 hours will\ntherefore have an equivalent training time.\n4 DATA ANALYSIS\nIn the sections below, we present several aspects regarding the carbon footprint of training ML models, examining the\nmain sources of energy used for training (¬ß 4.1), the order of magnitude of CO2 emissions produced (¬ß 4.2), the evolution\nof these emissions over time (¬ß 4.3) and the relationship between carbon emissions and model performance (¬ß 4.4) 2.\n4.1 What are the main sources of energy used for training ML models?\nThe primary energy source used for powering an electricity grid is the single biggest influence on the carbon intensity\nof that \n\nDoc ID: rubei2025\nURL: https://arxiv.org/pdf/2501.05899\nTitle: Prompt engineering and its implications on the energy consumption of Large Language Models\nText: e training when\nthe carbon emission is below a certain threshold. The results\nshows that the proposed approach succeed in reducing the\ncarbon emission even though the region may impact the ob-\ntained results. Liu and Yin [37] investigate how to reduce and\nmeasure the consumption of pre-trained models by combining\nfine-tuning and efficient tokenizers. In particular, BERT, Distil-\nBERT, and T5 models are compared using SQuAD benchmark\n[38] in terms of accuracy and carbon emissions. The experi-\nmental results reveal that both the T5 and BERT models emit-\nted considerably more CO2 compared to DistilBERT and the\nT4 GPU contributes in reducing the overall carbon emissions.\nSamsi et al. [13] compare the inference performance in terms\nof watts of different Llama models, i.e., evaluating smaller\nmodels (7B, 13B) against the largest available version (65B) at\nthe time of writing. In addition, the authors consider different\nGPUs, i.e., V100 and A100. The study reveals that 8 V100\nGPUs each with 32 GB of RAM or 4 A100 GPUs each with\n80GB of memory are required for any meaningful inferences\nwith the 65B LLaMA model, thus making small models a\nsuitable choice for energy-efficient applications. Cursaro et al. [39] conduct a controlled experiment in which code generated\nby CodeLlama is compared with the human one considering\ndifferent languages, i.e., C++, Java, and Python, tested on a\ndedicated platform. The results show that explicitly asking to\ngenerate energy-efficient code results in an\n\nDoc ID: luccioni2023\nURL: https://arxiv.org/pdf/2302.08476\nTitle: Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning\nText: e to ML model are rising.\nIn Section 4, we have discussed that the main sources of variance in the amount of emissions associated to training\nmachine learning models is due to the carbon intensity of the primary energy source and the training time, with the\npower consumption of the hardware having a smaller influence. In terms of training time, the models in our sample\nrange from just about 15 minutes (total GPU/TPU time) up to more than 400,000 hours, with a median of 72 hours,\npointing again to large variance in our sample. While the maximum of of 400,000 GPU hours (equivalent to about 170\ndays with 100 GPUs) in our sample seems very large, note that the total training time of GPT-3 was estimated to be\nover 3.5 million hours (14.8 days with 10,000 GPUs) [38]. Obviously, such long training times result in large amounts of\ncarbon emissions, even with lower carbon intensity energy sources. By way of illustration, the model with the longest\ntraining time in our sample would have reduced by about 30 times the carbon emissions had it used the grid with the\nlowest carbon intensity in our sample, but it would have still resulted in over 1 ton of CO2eq. Also, generally speaking,\nwe can see that the models at the higher end of the emissions spectrum tend to be Transformer-based model with more\nlayers (as well as using techniques such as Neural Architecture Search to find optimal combinations of parameters),\nwhereas simpler and shallower models such as convolutional neural networks te\n\nDoc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText: the energy consumption of training a large language model comprising\nover 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.\nThis model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate\nthe total energy consumption to train this model to completion would be approximately (60/8)‚àó13.8 = 103.5 MWh, or\n103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!\n4.2 Computer Vision\nDenseNets. We trained three sizes of DenseNets [19] on MNIST [25]. The jobs lasted between 20 and 25 minutes and\nconsumed between 20 and 38Wh (or 0.02 to 0.04 kWh) of electricity, which is negligible compared to the other models.\nVision Transformers. We evaluated the energy consumption during the training of five sizes of Vision Transformers [9]\non ImageNet [7]. For the smallest ViT experiment (ViT tiny), training lasted around 19 hours on a single V100 and\nconsumed approximately 1.7 kWh. For the largest one (ViT huge), training lasted more than 9 days on a 4 V100s and\nconsumed approximately 237 kWh. The full list of models can be found in Table 2.\n5 EMISSIONS BY REGION AND TIME OF DAY\nUsing the methodology presented above, we provide some of the first measurements of the differences of actual\ndatacenters from a major cloud provider. Importantly, what we have is a time series of marginal emissions: for example,\nif a job were to run from 1 pm to 5 pm in the US West region wit\n\nDoc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText:  to\nsingle-instance emissions calculations. We leave it open for future research to address how to appropriately allocate\nCO2 emissions from such data center-wide processes to individual reserved cloud instances.\n4 ELECTRICITY CONSUMPTION FOR AI WORKLOADS\nAs outlined in ¬ß3.1, calculating software carbon intensity begins with recording the electricity consumption, which\ncan then be mapped to emissions based on the emissions of the grid being used. In this section, we present data on\nelectricity consumption for experiments training 11 different models, covering natural language processing (NLP) and\ncomputer vision applications, ranging from less than an hour on a single GPU up to more than 8 days on 256 GPUs. We\noutline both the experiments themselves and their electricity consumption, and in the following section we use the\nelectricity consumption and carbon intensity tool described in the previous section to calculate their software carbon\nintensity.\n2We note that our conclusions drawn from experiments and analyses on time-shifting and location-shifting are still applicable with tools that measure\nmore electricity than just the GPU.\n3https://www.google.com/about/datacenters/efficiency/\n4One of the largest single source of CO2 emissions, contributing to 7%-8% of global emissions, is the production of cement [20].\n6 Measuring the Carbon Intensity of AI in Cloud Instances FAccT ‚Äô22, June 21‚Äì24, 2022, Seoul, Republic of Korea\nModel BERT BERT 6B Dense Dense Dense ViT ViT ViT ViT V\n\nDoc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText: e carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\nassumed 100% GPU power draw throughout training.\n3 Published as a conference paper at ICLR 2025\nwhere the cost of a scientific resultR (e.g. a claim that a particular training setup reachesX accuracy\non benchmark Y ) is proportional to the product of the cost of processing a single example E, the\nsize of the training dataset D, and the number of hyperparameter experiments H. In previous work,\nE ¬∑ D, the cost of training on the training dataset, is what is most commonly reported, and H, the\ntotal number of experiments, is most often excluded.\nIn our analysis, we calculate the total power consumption during model training, development, and\ninference, and use this to estimate the total carbon emissions and water consumption during each\nstage. We follow previous work (Luccioni et al., 2023; Dubey et al., 2024; Gemma Team et al.,\n2024) to calculate CO2 emissions (CO2e) from power consumption:\nCO2e = P ¬∑ PUE ¬∑ CI (2)\nwhere the total carbon emissions is equal to the power usage P, multiplied by the power usage\neffectiveness (PUE)6 of the data center, multiplied by the carbon intensity CI of the local power\ngrid. We run all experiments in our two GPU clusters, Jupiter and Augusta, which are located in\nTexas and Iowa, respectively (see OLMo et al. (2025) for more information). Our 13B model was\ntrained on Augusta, and all other experiments analyzed in this paper were trained on Jupiter.\nOur data center provider\n\nDoc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText:  coefficients of 1.29 L / kWh and 1.2\nrespectively, and carbon intensity of 0.332 kg CO2e / kWh. Note the difference in units for energy consumption\nand carbon emissions, namely MWh ‚Üí kWh, tons ‚Üí grams CO2eq, and kL ‚Üí L. The measurements reported\nin this table account for the GPU processes associated with active inference, but not CPU or RAM associated\nwith e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings.\nAlso of note is the relatively small variability in carbon emissions and water consumption across different model\nsizes in cases where batches are not saturated, despite faster inference in smaller models when fully saturated;\ngreater peak efficiency does not guarantee efficient deployment if inference is not optimized. We do not report\n‚Äùbreak-even‚Äù points for Qwen 2.5 because its training costs are not public.\nModel\nRequest\nfreq.\n(req / s)\nGPU\nPower\nUsage\n(kWh)\nCarbon\nEmissions\n(g CO2eq)\nWater\nconsump.\n(L)\nSeconds\nper 100 req.\n# Inf. for\nCO2 equiv.\nw/ training\nLlama 3.2 1B ‚àû 0.003 1.0 0.004 1.38 258 bil.\n8 0.036 12.0 0.054 12.64 21.5 bil.\n1 0.160 53.1 0.238 100.58 4.83 bil.\nQwen 2.5 7B ‚àû 0.009 3.0 0.013 1.79 ‚Äî\n8 0.053 17.6 0.079 12.77 ‚Äî\n1 0.308 102.3 0.459 100.58 ‚Äî\nLlama 3.1 8B ‚àû 0.011 3.7 0.016 2.13 276 bil.\n8 0.051 16.9 0.076 12.79 59.5 bil.\n1 0.333 110.6 0.496 100.64 9.12 bil.\nLlama 2 13B ‚àû 0.034 11.3 0.051 6.53 13.3 bil.\n8 0.060 19.9 0.089 13.09 7.52 bil.\n1 0.401 133.1 0.597 100.73 1.13 bil.\nOLMo 1 1B (3T) ‚àû 0.004'),
 namespace(qid='q272',
           q='How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?',
           predicted={'answer': 'The training of a 6.1B-parameter language model to completion is estimated to consume 103.5 MWh of energy, which is equivalent to approximately 20.7 years of electricity use by an average U.S. household.',
                      'answer_value': 20.7,
                      'answer_unit': 'years',
                      'ref_id': ['dodge2022'],
                      'ref_url': ['https://arxiv.org/pdf/2206.05229'],
                      'supporting_materials': 'Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8)*13.8 = 103.5 MWh',
                      'explanation': 'Math: (60/8)*13.8 = 103.5 MWh. The equivalent U.S. household-years is calculated by comparing to the average U.S. household electricity consumption of 10.6 MWh/year, as referenced in the context. 103.5 MWh / 5 MWh/year = 20.7 years.'},
           expected=id                                                                   q272
                    question                How many U.S. household-years of electricity c...
                    answer                                                                1.3
                    answer_value                                                          1.3
                    answer_unit                                               household-years
                    ref_id                                       ['dodge2022','strubell2019']
                    ref_url                 ['https://arxiv.org/pdf/2206.05229','https://a...
                    supporting_materials    Dodge 2022: "The 6.1B parameter model consumed...
                    explanation                 13.8 MWh √∑ 10.7 MWh/yr ‚âà 1.3 household-years.
                    Name: 32, dtype: object,
           score=0.175,
           answer_score=0.0,
           ref_score=0.5,
           na_score=1.0,
           context='Doc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText: Published as a conference paper at ICLR 2025\nHOLISTICALLY EVALUATING THE ENVIRONMENTAL\nIMPACT OF CREATING LANGUAGE MODELS\nJacob Morrison1 Clara Na2 Jared Fernandez2\nTim Dettmers1,2 Emma Strubell1,2 Jesse Dodge1\n1Allen Institute for AI 2Carnegie Mellon University\njacobm@allenai.org\nABSTRACT\nAs the performance of artificial intelligence systems has dramatically increased,\nso too has the environmental impact of creating these systems. While many model\ndevelopers release estimates of the power consumption and carbon emissions from\nthe final training runs for their latest models, there is comparatively little trans-\nparency into the impact of model development, hardware manufacturing, and total\nwater usage throughout. In this work, we estimate the real-world environmental\nimpact of developing a series of language models, ranging from 20 million to 13\nbillion active parameters, trained on up to 5.6 trillion tokens each. When account-\ning for hardware manufacturing, model development, and our final training runs,\nwe find that our series of models released 493 metric tons of carbon emissions,\nequivalent to powering about 98 homes in the United States for one year, and\nconsumed 2.769 million liters of water , equivalent to about 24.5 years of water\nusage by a person in the United States, even though our data center is extremely\nwater-efficient. We measure and report the environmental impact of our model\ndevelopment; to the best of our knowledge we are the first to do so for LLMs, and\n\n\nDoc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText: com/Page.aspx?id=1012\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n6 Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n‚Ä† denotes unreleased models that were trained for various internal experiments.\nPower\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv. to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv. to...\n(water usage,\n1 person, U.S.)\nGemma 2B &amp; 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M‚Ä† 0.8 0.3 3 weeks 1 3 days\nOLMo 60M‚Ä† 1.2 0.4 1 month 1.6 5 days\nOLMo 150M‚Ä† 2.4 1 2 mo, 1 wk 3.6 12\n\nDoc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText: pact of our model\ndevelopment; to the best of our knowledge we are the first to do so for LLMs, and\nwe find that model development, the impact of which is generally not disclosed\nby most model developers, amounted to ‚àº50% of that of training. By looking at\ndetailed time series data for power consumption, we also find that power usage\nthroughout training is not consistent, fluctuating between ‚àº15% and ‚àº85% of\nour hardware‚Äôs maximum power draw, with negative implications for grid-scale\nplanning as demand continues to grow. We close with a discussion on the con-\ntinued difficulty of estimating the environmental impact of AI systems, and key\ntakeaways for model developers and the public at large.\n1 I NTRODUCTION\nIn recent years, the field of artificial intelligence has progressed at an unprecedented pace, driven\nin large part by the development and deployment of large language and multimodal models. How-\never, the development of these models comes with significant environmental costs (Schwartz et al.,\n2020; Strubell et al., 2020; Wu et al., 2022). Training these models requires massive computational\nresources, which, in turn, require large amounts of energy. Powering training both emits carbon\n(by burning fossil fuels) and consumes water (by evaporating or polluting it in power plants, data\ncenters, and hardware manufacturing processes; Li et al. (2023)). There is a growing demand for\nenergy to power AI workloads, with projections estimating that datacenters may consume upwards\no\n\nDoc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText: where we\nrank each model by both its total water consumption\nand its CO2 emissions. Our small models (&lt;1B param-\neters) were trained on 1.7 trillion tokens, OLMo 1B was\ntrained on 3 trillion, OLMo 2 7B was trained on 4 tril-\nlion, OLMoE was trained on 5 trillion, and OLMo 2\n13B was trained on 5.6 trillion. We see that the total\nenvironmental impact for larger training runs is quite\nhigh, and increases quickly with model and dataset size.\nIn this paper, we estimate the energy use and\nenvironmental impacts caused by training the\nOLMo series of transformer language models\n(Groeneveld et al., 2024; OLMo et al., 2025),\nranging in size from 20 million to 13 billion\nactive parameters, trained on 1.7 to 5.6 trillion\ntokens. To do this, we calculate Scope 2 CO 2\nemissions in accordance with the Greenhouse\nGas Protocol‚Äôs definitions,3 and Scope 1 and 2\nwater consumption following Li et al. (2023);\nin addition, we calculate ‚Äúupstream‚Äù embod-\nied carbon and water consumption, and provide\n‚Äúdownstream‚Äù estimates from use of our mod-\nels (which are part, but not all, of Scope 3).\nImportantly, we calculate (i) electricity con-\nsumption, (ii) carbon emissions, and (iii) wa-\nter consumption at three points in the machine\nlearning pipeline: early model development\n(e.g., hyperparameter tuning and experiments\nbefore the final training run), training of the\nmain model, and inference. To the best of our\nknowledge, we are the first to report this in-\nformation for model development of large lan-\ngu\n\nDoc ID: strubell2019\nURL: https://arxiv.org/pdf/1906.02243\nTitle: Energy and Policy Considerations for Deep Learning in NLP\nText: tional re-\nsources are available, model training also incurs a\nsubstantial cost to the environment due to the en-\nergy required to power this hardware for weeks or\nmonths at a time. Though some of this energy may\ncome from renewable or carbon credit-offset re-\nsources, the high energy demands of these models\nare still a concern since (1) energy is not currently\nderived from carbon-neural sources in many loca-\ntions, and (2) when renewable energy is available,\nit is still limited to the equipment we have to pro-\nduce and store it, and energy spent training a neu-\nral network might better be allocated to heating a\nfamily‚Äôs home. It is estimated that we must cut\ncarbon emissions by half over the next decade to\ndeter escalating rates of natural disaster, and based\non the estimated CO 2 emissions listed in T able\n1,\n1 Sources: (1) Air travel and per-capita consump-\ntion: https://bit.ly/2Hw0xWc; (2) car lifetime:\nhttps://bit.ly/2Qbr0w1. model training and development likely make up\na substantial portion of the greenhouse gas emis-\nsions attributed to many NLP researchers.\nT o heighten the awareness of the NLP commu-\nnity to this issue and promote mindful practice and\npolicy , we characterize the dollar cost and carbon\nemissions that result from training the neural net-\nworks at the core of many state-of-the-art NLP\nmodels. W e do this by estimating the kilowatts\nof energy required to train a variety of popular\noff-the-shelf NLP models, which can be converted\nto approximate carbon e\n\nDoc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText: the energy consumption of training a large language model comprising\nover 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.\nThis model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate\nthe total energy consumption to train this model to completion would be approximately (60/8)‚àó13.8 = 103.5 MWh, or\n103,500 kWh ‚Äî almost 2800 times more than training the BERT-small model!\n4.2 Computer Vision\nDenseNets. We trained three sizes of DenseNets [19] on MNIST [25]. The jobs lasted between 20 and 25 minutes and\nconsumed between 20 and 38Wh (or 0.02 to 0.04 kWh) of electricity, which is negligible compared to the other models.\nVision Transformers. We evaluated the energy consumption during the training of five sizes of Vision Transformers [9]\non ImageNet [7]. For the smallest ViT experiment (ViT tiny), training lasted around 19 hours on a single V100 and\nconsumed approximately 1.7 kWh. For the largest one (ViT huge), training lasted more than 9 days on a 4 V100s and\nconsumed approximately 237 kWh. The full list of models can be found in Table 2.\n5 EMISSIONS BY REGION AND TIME OF DAY\nUsing the methodology presented above, we provide some of the first measurements of the differences of actual\ndatacenters from a major cloud provider. Importantly, what we have is a time series of marginal emissions: for example,\nif a job were to run from 1 pm to 5 pm in the US West region wit\n\nDoc ID: morrison2025\nURL: https://arxiv.org/pdf/2503.05804\nTitle: Holistically Evaluating the Environmental Impact of Creating Language Models\nText:  our\nknowledge, we are the first to report this in-\nformation for model development of large lan-\nguage models, and we find the environmental\nimpact of developing even our relatively small\nmodels (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or\nthe amount of water consumed by one average person in the United States in about 7.5 years. We\nencourage the reader to consider larger models released by other organizations to have equivalently\nlarger environmental impacts.\nOur methodology draws upon best practices from recent publications, aiming to provide the most\nthorough reporting yet of the environmental impact of LLMs. For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training. We focus our efforts on a wide range of model sizes,\noptimized for widespread deployment (Dubey et al., 2024; Mehta et al., 2024; Gemma Team et al.,\n2024), and estimate what the environmental impact would be if our models were deployed in a va-\nriety of different scenarios. We find that in some scenarios, our models would need to run inference\non a few billion instances to match the electricity consumed, carbon emitted, and water consumed\nof the entire training process, a figure that can be reached by production systems in weeks to months\nbased on current u\n\nDoc ID: jegham2025\nURL: https://arxiv.org/pdf/2505.09598\nTitle: How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference\nText:  Li, Adam Michaleas, Michael Jones,\nWilliam Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to\nwatts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High\nPerformance Extreme Computing Conference (HPEC), pages 1‚Äì9. IEEE, 2023.\n[21] The Green Grid. PUE‚Ñ¢: A Comprehensive Examination of the Metric. February 2012. White\nPaper 49.\n[22] International Organization for Standardization (ISO) and International Electrotechnical Com-\nmission (IEC). Information technology ‚Äì Data centres ‚Äì Key performance indicators ‚Äì Part\n2: Power usage effectiveness (PUE), April 2016. URL https://www.iso.org/standard/\n63211.html.\n[23] U.S. Environmental Protection Agency (EPA). Emissions &amp; Generation Resource Integrated\nDatabase (eGRID). https://www.epa.gov/egrid, 2025.\n[24] International Energy Agency (IEA). Emissions Factors. 2025.\n[25] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for\nmodern deep learning research. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pages 13693‚Äì13696, 2020.\n[26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[27] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal B\n\nDoc ID: dodge2022\nURL: https://arxiv.org/pdf/2206.05229\nTitle: Measuring the Carbon Intensity of AI in Cloud Instances\nText: 2, June 21‚Äì24, 2022, Seoul, Republic of Korea\nModel BERT BERT 6B Dense Dense Dense ViT ViT ViT ViT ViT\nfinetune pretrain Transf. 121 169 201 Tiny Small Base Large Huge\nGPU 4¬∑V100 8¬∑V100 256¬∑A100 1¬∑P40 1¬∑P40 1¬∑P40 1¬∑V100 1¬∑V100 1¬∑V100 4¬∑V100 4¬∑V100\nHours 6 36 192 0.3 0.3 0.4 19 19 21 90 216\nkWh 3.1 37.3 13,812.4 0.02 0.03 0.04 1.7 2.2 4.7 93.3 237.6\nTable 2. For the 11 models in our analysis: the type of GPU, the number of GPUs of that type, the number of hours, and the energy\nused in kWh. For example, our BERT language modeling (BERT LM) experiment used 8 V100 GPUs for 36 hours and used a total of\n37.3 kWh. We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would\ntake to train to completion, we estimate a full training run would consume approximately 103,593 kWh.\n4.1 NLP\nBERT Training. We monitored the energy consumption while training a BERT-small model [8] for approximately 36\nhours on 8 NVIDIA V100 GPUs. That training run consumed over 37 kWh of electricity.\nBERT Finetuning. We tracked the energy consumption while finetuning the BERT-small model on a standard natural\nlanguage inference task [48, MNLI] for approximately 6 hours on 4 NVIDIA V100 GPUs. Our finetuning run consumed\naround 3.2 kWh of electricity, i.e., less than one tenth that due to BERT-small pre-training.\n6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising\nover 6.1 billion parameters dur\n\nDoc ID: zschache2025\nURL: https://arxiv.org/pdf/2508.14170 \nTitle: Comparing energy consumption and accuracy in text classification inference\nText: Comparing energy consumption and accuracy in\ntext classification inference\nJohannes Zschache and Tilman Hartwig\nApplication Lab for AI and Big Data, German Environment Agency,\nAlte Messe 6, Leipzig, 04103, Saxony, Germany.\n*Corresponding author(s). E-mail(s): tilman.hartwig@uba.de;\nContributing authors: johannes.zschache@uba.de;\nAbstract\nThe increasing deployment of large language models (LLMs) in natural language\nprocessing (NLP) tasks raises concerns about energy efficiency and sustainabil-\nity. While prior research has largely focused on energy consumption during\nmodel training, the inference phase has received comparatively less attention.\nThis study systematically evaluates the trade-offs between model accuracy and\nenergy consumption in text classification inference across various model archi-\ntectures and hardware configurations. Our empirical analysis shows that the\nbest-performing model in terms of accuracy can also be energy-efficient, while\nlarger LLMs tend to consume significantly more energy with lower classifica-\ntion accuracy. We observe substantial variability in inference energy consumption\n(&lt;mWh to &gt;kWh), influenced by model type, model size, and hardware spec-\nifications. Additionally, we find a strong correlation between inference energy\nconsumption and model runtime, indicating that execution time can serve as\na practical proxy for energy usage in settings where direct measurement is not\nfeasible. These findings have implications for sustainable AI develop')]
========== MIDDLE OF OUTPUT TRUNCATED ==========</code></pre>
</div>
</div>
<p>What do you think where the RAG is going wrong?</p>
</section>
<section id="reply-46" class="level5">
<h5 class="anchored" data-anchor-id="reply-46">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†</p>
<p>Looking at your worst predictions, I can see a few key patterns where the RAG is failing:</p>
<p><strong>1. Missing the correct source document entirely</strong> - For q280 about JetMoE-8B, none of the retrieved chunks are from <code>shen2024</code> (the expected reference) - For q170 and q262, the chunks are from related papers (dodge2022, luccioni2023) but not from <code>strubell2019</code> which has the specific answer</p>
<p><strong>2. Right document, wrong chunk</strong> - Even when the document is retrieved (like q078), the specific numerical detail might not be in the chunk, or it‚Äôs buried in a way that‚Äôs hard for the LLM to extract</p>
<p><strong>3. Semantic vs.&nbsp;keyword mismatch</strong> - Questions asking for very specific numbers/calculations might not embed similarly to the text containing those numbers</p>
<p><strong>4. Answer format issues</strong> - For q078, the model found the right info but reported it inverted (10-50 responses per bottle instead of 0.02-0.1 bottles per response)</p>
<p>Would you like to dig deeper into any of these patterns, or try some experiments to improve retrieval for these specific failure cases?</p>
<p>Sure</p>
</section>
<section id="reply-47" class="level5">
<h5 class="anchored" data-anchor-id="reply-47">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let‚Äôs start by investigating the retrieval issue more closely.</p>
<p>For the failed questions, would you like to:</p>
<ol type="1">
<li><strong>Search for specific text</strong> from the expected answers in your document chunks to see if they‚Äôre actually there</li>
<li><strong>Compare embeddings</strong> between the query and the correct chunk to understand why it ranked lower</li>
<li><strong>Try different retrieval parameters</strong> (like increasing the limit or adjusting hybrid search weights)</li>
</ol>
<p>Which approach would you like to explore first?</p>
<p>Will chaning the embedding model improve the score?</p>
</section>
<section id="reply-48" class="level5">
<h5 class="anchored" data-anchor-id="reply-48">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>It could, but it depends on what‚Äôs causing the failures. Let me break this down:</p>
<p><strong>When a different embedding model might help:</strong> - If your current model (nomic-embed-text) isn‚Äôt capturing domain-specific terminology well - If it‚Äôs not good at distinguishing between similar technical concepts - If there‚Äôs a mismatch between how queries and documents are embedded</p>
<p><strong>When it might NOT help much:</strong> - If the problem is that the correct chunks simply don‚Äôt contain the specific facts (chunking issue) - If the information is there but the LLM isn‚Äôt extracting it correctly (generation issue) - If your hybrid search weights need tuning</p>
<p><strong>To test this hypothesis</strong>, you could: 1. Check if the correct chunks are even in your top-10 or top-20 results 2. Try a domain-specific embedding model (like one trained on scientific papers) 3. Compare results with your current model</p>
<p>Do you want to first check whether the right chunks are being retrieved at all (just ranked lower), or would you prefer to try a different embedding model right away?</p>
<p>I think the worst predictions are already there, what do you think?</p>
</section>
<section id="reply-49" class="level5">
<h5 class="anchored" data-anchor-id="reply-49">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You‚Äôre absolutely right! You already have the worst predictions with <code>evals.get_worst_predictions()</code>.</p>
<p>Looking at those results, the key diagnostic question is: <strong>Are the correct chunks being retrieved at all, just ranked poorly?</strong></p>
<p>You could check this by: - Looking at whether the expected <code>ref_id</code> appears anywhere in the top 10-20 retrieved chunks - Searching for the specific answer text in your chunks to see if it exists</p>
<p>For example, for q280 about JetMoE-8B, you could search all chunks for ‚ÄúJetMoE‚Äù or ‚Äú30,000‚Äù to see if that information is even in your corpus.</p>
<p>Would you like to write a quick check to see if the expected document IDs appear in the retrieved chunks for your worst predictions?</p>
<p>I want to improve the figure and table intepretation from the pdf</p>
</section>
<section id="reply-50" class="level5">
<h5 class="anchored" data-anchor-id="reply-50">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good idea! Improving figure and table interpretation could help with those questions that reference specific data points.</p>
<p>Here are a few approaches you could take:</p>
<ol type="1">
<li><strong>Extract tables separately</strong> - Use a specialized tool to parse tables from PDFs and store them with better structure</li>
<li><strong>Add visual context</strong> - Include table/figure captions and surrounding text when chunking</li>
<li><strong>Use multimodal embeddings</strong> - Embed images of tables/figures along with text</li>
<li><strong>Structured extraction</strong> - Convert tables to structured formats (CSV, JSON) before embedding</li>
</ol>
<p>Which approach interests you most, or would you like to explore what‚Äôs currently happening with tables in your chunks?</p>
<p>Also the ref_id has square bracket even though the answer is is_blank for example [‚Äúis_blank‚Äù]</p>
</section>
<section id="reply-51" class="level5">
<h5 class="anchored" data-anchor-id="reply-51">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good catch! That‚Äôs a data consistency issue. The <code>ref_id</code> should be <code>"is_blank"</code> (a string) when the answer is blank, not <code>["is_blank"]</code> (a list containing the string).</p>
<p>This could be happening in: 1. Your prompt template - if it‚Äôs instructing the model to always return a list 2. Your data preprocessing - if you‚Äôre wrapping everything in lists 3. The LLM‚Äôs output - if it‚Äôs being too literal about the JSON schema</p>
<p>Would you like to check your prompt template to see how <code>ref_id</code> is defined, or would you prefer to add post-processing to normalize this?</p>
<p>Lets address the table and image extraction. I think we are missing a great deal of information there. I want to generate the embedding and the text from the table/image also have the image/table part of the payload if possible for further debugging</p>
</section>
<section id="reply-52" class="level5">
<h5 class="anchored" data-anchor-id="reply-52">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great plan! Extracting tables and images separately will likely improve your results significantly.</p>
<p>Here‚Äôs the approach:</p>
<ol type="1">
<li><strong>Extract tables/images from PDFs</strong> - Use a library like <code>pdfplumber</code> for tables or a multimodal model for images</li>
<li><strong>Convert to text</strong> - Tables ‚Üí markdown/CSV format, Images ‚Üí captions/descriptions</li>
<li><strong>Store in chunks</strong> - Include the extracted text plus metadata (page number, type: ‚Äútable‚Äù/‚Äúimage‚Äù)</li>
<li><strong>Optionally store raw data</strong> - Keep the image bytes or table structure in payload for debugging</li>
</ol>
<p>For your use case, would you like to: - Start with just tables (simpler, likely covers most of your numerical data)? - Or handle both tables and images from the beginning?</p>
<p>Lets start with table</p>
</section>
<section id="reply-53" class="level5">
<h5 class="anchored" data-anchor-id="reply-53">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! For table extraction from PDFs, you have a few good options:</p>
<ol type="1">
<li><strong>pdfplumber</strong> - Good for structured tables, extracts to list of dicts</li>
<li><strong>camelot-py</strong> - Specialized for tables, handles complex layouts</li>
<li><strong>tabula-py</strong> - Java-based, exports to pandas DataFrames</li>
<li><strong>Multimodal LLM</strong> - Use vision models to ‚Äúread‚Äù tables from PDF images</li>
</ol>
<p>For your wattbot data with research papers, I‚Äôd recommend starting with <strong>pdfplumber</strong> since it‚Äôs already available and works well with academic PDFs.</p>
<p>Would you like to try extracting tables from one of your PDFs to see what we get, or would you prefer me to show you a small code example first?</p>
<p>I also want to see why my rd.read_pdf does not do a good job in extracting table</p>
</section>
<section id="reply-54" class="level5">
<h5 class="anchored" data-anchor-id="reply-54">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good idea to investigate! <code>contextkit.read_pdf</code> likely uses a basic text extraction method that doesn‚Äôt handle table structure well - it probably just extracts text in reading order, losing the column/row relationships.</p>
<p>To see what‚Äôs happening, you could:</p>
<ol type="1">
<li>Pick a PDF with a known table (like one of your papers with data tables)</li>
<li>Extract it with <code>rd.read_pdf()</code> and look at the output</li>
<li>Compare with what a table-specific tool extracts</li>
</ol>
<p>Want to try this with one of your documents to see the difference?</p>
<p>I have a doc id strubell2019</p>
</section>
<section id="reply-55" class="level5">
<h5 class="anchored" data-anchor-id="reply-55">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Let‚Äôs look at what <code>rd.read_pdf</code> extracts from that document. Try this:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>content <span class="op">=</span> rd.read_pdf(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then search for a section that should contain a table (like ‚ÄúTable 1‚Äù or similar) and print a chunk around it to see how the table data looks.</p>
<p>Would you like to do this, or should I help you with the code?</p>
<div id="9539a5df" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>content <span class="op">=</span> rd.read_pdf(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="65cd9b7f" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>content</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>'arXiv:1906.02243v1  [cs.CL]  5 Jun 2019\nEnergy and Policy Considerations for Deep Learning in NLP\nEmma Strubell Ananya Ganesh Andrew McCallum\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{strubell, aganesh, mccallum}@cs.umass.edu\nAbstract\nRecent progress in hardware and methodol-\nogy for training neural networks has ushered\nin a new generation of large networks trained\non abundant data. These models have ob-\ntained notable gains in accuracy across many\nNLP tasks. However, these accuracy improve-\nments depend on the availability of exception-\nally large computational resources that neces-\nsitate similarly substantial energy consump-\ntion. As a result these models are costly to\ntrain and develop, both Ô¨Ånancially, due to the\ncost of hardware and electricity or cloud com-\npute time, and environmentally, due to the car-\nbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring\nthis issue to the attention of NLP researchers\nby quantifying the approximate Ô¨Ånancial and\nenvironmental costs of training a variety of re-\ncently successful neural network models for\nNLP . Based on these Ô¨Åndings, we propose ac-\ntionable recommendations to reduce costs and\nimprove equity in NLP research and practice.\n1 Introduction\nAdvances in techniques and hardware for train-\ning deep neural networks have recently en-\nabled impressive accuracy improvements across\nmany fundamental NLP tasks (\nBahdanau et al. ,\n2015; Luong et al. , 2015; Dozat and Man-\nning, 2017; V aswani et al. , 2017), with the\nmost computationally-hungry models obtaining\nthe highest scores (\nPeters et al. , 2018; Devlin et al. ,\n2019; Radford et al. , 2019; So et al. , 2019). As\na result, training a state-of-the-art model now re-\nquires substantial computational resources which\ndemand considerable energy , along with the as-\nsociated Ô¨Ånancial and environmental costs. Re-\nsearch and development of new models multiplies\nthese costs by thousands of times by requiring re-\ntraining to experiment with model architectures\nand hyperparameters. Whereas a decade ago most\nConsumption CO 2e (lbs)\nAir travel, 1 passenger, NY ‚Üî SF 1984\nHuman life, avg, 1 year 11,023\nAmerican life, avg, 1 year 36,156\nCar, avg incl. fuel, 1 lifetime 126,000\nT raining one model (GPU)\nNLP pipeline (parsing, SRL) 39\nw/ tuning &amp; experimentation 78,468\nTransformer (big) 192\nw/ neural architecture search 626,155\nT able 1: Estimated CO 2 emissions from training com-\nmon NLP models, compared to familiar consumption. 1\nNLP models could be trained and developed on\na commodity laptop or server, many now require\nmultiple instances of specialized hardware such as\nGPUs or TPUs, therefore limiting access to these\nhighly accurate models on the basis of Ô¨Ånances.\nEven when these expensive computational re-\nsources are available, model training also incurs a\nsubstantial cost to the environment due to the en-\nergy required to power this hardware for weeks or\nmonths at a time. Though some of this energy may\ncome from renewable or carbon credit-offset re-\nsources, the high energy demands of these models\nare still a concern since (1) energy is not currently\nderived from carbon-neural sources in many loca-\ntions, and (2) when renewable energy is available,\nit is still limited to the equipment we have to pro-\nduce and store it, and energy spent training a neu-\nral network might better be allocated to heating a\nfamily‚Äôs home. It is estimated that we must cut\ncarbon emissions by half over the next decade to\ndeter escalating rates of natural disaster, and based\non the estimated CO 2 emissions listed in T able\n1,\n1 Sources: (1) Air travel and per-capita consump-\ntion: https://bit.ly/2Hw0xWc; (2) car lifetime:\nhttps://bit.ly/2Qbr0w1. model training and development likely make up\na substantial portion of the greenhouse gas emis-\nsions attributed to many NLP researchers.\nT o heighten the awareness of the NLP commu-\nnity to this issue and promote mindful practice and\npolicy , we characterize the dollar cost and carbon\nemissions that result from training the neural net-\nworks at the core of many state-of-the-art NLP\nmodels. W e do this by estimating the kilowatts\nof energy required to train a variety of popular\noff-the-shelf NLP models, which can be converted\nto approximate carbon emissions and electricity\ncosts. T o estimate the even greater resources re-\nquired to transfer an existing model to a new task\nor develop new models, we perform a case study\nof the full computational resources required for the\ndevelopment and tuning of a recent state-of-the-art\nNLP pipeline (\nStrubell et al. , 2018). W e conclude\nwith recommendations to the community based on\nour Ô¨Åndings, namely: (1) Time to retrain and sen-\nsitivity to hyperparameters should be reported for\nNLP machine learning models; (2) academic re-\nsearchers need equitable access to computational\nresources; and (3) researchers should prioritize de-\nveloping efÔ¨Åcient models and hardware.\n2 Methods\nT o quantify the computational and environmen-\ntal cost of training deep neural network mod-\nels for NLP , we perform an analysis of the en-\nergy required to train a variety of popular off-\nthe-shelf NLP models, as well as a case study of\nthe complete sum of resources required to develop\nLISA (\nStrubell et al. , 2018), a state-of-the-art NLP\nmodel from EMNLP 2018, including all tuning\nand experimentation.\nW e measure energy use as follows. W e train the\nmodels described in ¬ß\n2.1 using the default settings\nprovided, and sample GPU and CPU power con-\nsumption during training. Each model was trained\nfor a maximum of 1 day . W e train all models on\na single NVIDIA Titan X GPU, with the excep-\ntion of ELMo which was trained on 3 NVIDIA\nGTX 1080 Ti GPUs. While training, we repeat-\nedly query the NVIDIA System Management In-\nterface\n2 to sample the GPU power consumption\nand report the average over all samples. T o sample\nCPU power consumption, we use Intel‚Äôs Running\nA verage Power Limit interface.\n3\n2 nvidia-smi: https://bit.ly/30sGEbi\n3 RAPL power meter: https://bit.ly/2LObQhV\nConsumer Renew . Gas Coal Nuc.\nChina 22% 3% 65% 4%\nGermany 40% 7% 38% 13%\nUnited States 17% 35% 27% 19%\nAmazon-A WS 17% 24% 30% 26%\nGoogle 56% 14% 15% 10%\nMicrosoft 32% 23% 31% 10%\nT able 2: Percent energy sourced from: Renewable (e.g.\nhydro, solar, wind), natural gas, coal and nuclear for\nthe top 3 cloud compute providers (\nCook et al. , 2017),\ncompared to the United States, 4 China5 and Germany\n(Burger, 2019).\nW e estimate the total time expected for mod-\nels to train to completion using training times and\nhardware reported in the original papers. W e then\ncalculate the power consumption in kilowatt-hours\n(kWh) as follows. Let pc be the average power\ndraw (in watts) from all CPU sockets during train-\ning, let pr be the average power draw from all\nDRAM (main memory) sockets, let pg be the aver-\nage power draw of a GPU during training, and let\ng be the number of GPUs used to train. W e esti-\nmate total power consumption as combined GPU,\nCPU and DRAM consumption, then multiply this\nby Power Usage Effectiveness (PUE), which ac-\ncounts for the additional energy required to sup-\nport the compute infrastructure (mainly cooling).\nW e use a PUE coefÔ¨Åcient of 1.58, the 2018 global\naverage for data centers (\nAscierto, 2018). Then the\ntotal power pt required at a given instance during\ntraining is given by:\npt = 1.58t(pc + pr + gpg)\n1000 (1)\nThe U.S. Environmental Protection Agency (EP A)\nprovides average CO 2 produced (in pounds per\nkilowatt-hour) for power consumed in the U.S.\n(\nEP A, 2018), which we use to convert power to\nestimated CO 2 emissions:\nCO2e = 0.954pt (2)\nThis conversion takes into account the relative pro-\nportions of different energy sources (primarily nat-\nural gas, coal, nuclear and renewable) consumed\nto produce energy in the United States. T able\n2\nlists the relative energy sources for China, Ger-\nmany and the United States compared to the top\n5 U.S. Dept. of Energy: https://bit.ly/2JTbGnI\n5 China Electricity Council; trans. China Energy Portal:\nhttps://bit.ly/2QHE5O3 three cloud service providers. The U.S. break-\ndown of energy is comparable to that of the most\npopular cloud compute service, Amazon W eb Ser-\nvices, so we believe this conversion to provide a\nreasonable estimate of CO 2 emissions per kilowatt\nhour of compute energy used.\n2.1 Models\nW e analyze four models, the computational re-\nquirements of which we describe below . All mod-\nels have code freely available online, which we\nused out-of-the-box. For more details on the mod-\nels themselves, please refer to the original papers.\nT ransformer. The Transformer model (\nV aswani\net al. , 2017) is an encoder-decoder architecture\nprimarily recognized for efÔ¨Åcient and accurate ma-\nchine translation. The encoder and decoder each\nconsist of 6 stacked layers of multi-head self-\nattention.\nV aswani et al. (2017) report that the\nTransformer base model (65M parameters) was\ntrained on 8 NVIDIA P100 GPUs for 12 hours,\nand the Transformer big model (213M parame-\nters) was trained for 3.5 days (84 hours; 300k\nsteps). This model is also the basis for recent\nwork on neural architecture search ( NAS) for ma-\nchine translation and language modeling (\nSo et al. ,\n2019), and the NLP pipeline that we study in more\ndetail in ¬ß4.2 (Strubell et al. , 2018). So et al.\n(2019) report that their full architecture search ran\nfor a total of 979M training steps, and that their\nbase model requires 10 hours to train for 300k\nsteps on one TPUv2 core. This equates to 32,623\nhours of TPU or 274,120 hours on 8 P100 GPUs.\nELMo. The ELMo model (\nPeters et al. , 2018)\nis based on stacked LSTMs and provides rich\nword representations in context by pre-training on\na large amount of data using a language model-\ning objective. Replacing context-independent pre-\ntrained word embeddings with ELMo has been\nshown to increase performance on downstream\ntasks such as named entity recognition, semantic\nrole labeling, and coreference.\nPeters et al. (2018)\nreport that ELMo was trained on 3 NVIDIA GTX\n1080 GPUs for 2 weeks (336 hours).\nBERT .The BERT model (\nDevlin et al. , 2019) pro-\nvides a Transformer-based architecture for build-\ning contextual representations similar to ELMo,\nbut trained with a different language modeling ob-\njective. BERT substantially improves accuracy on\ntasks requiring sentence-level representations such\nas question answering and natural language infer-\nence.\nDevlin et al. (2019) report that the BERT\nbase model (110M parameters) was trained on 16\nTPU chips for 4 days (96 hours). NVIDIA reports\nthat they can train a BERT model in 3.3 days (79.2\nhours) using 4 DGX-2H servers, totaling 64 T esla\nV100 GPUs (\nForster et al. , 2019).\nGPT -2. This model is the latest edition of\nOpenAI‚Äôs GPT general-purpose token encoder,\nalso based on Transformer-style self-attention and\ntrained with a language modeling objective (\nRad-\nford et al. , 2019). By training a very large model\non massive data, Radford et al. (2019) show high\nzero-shot performance on question answering and\nlanguage modeling benchmarks. The large model\ndescribed in\nRadford et al. (2019) has 1542M pa-\nrameters and is reported to require 1 week (168\nhours) of training on 32 TPUv3 chips.\n6\n3 Related work\nThere is some precedent for work characterizing\nthe computational requirements of training and in-\nference in modern neural network architectures in\nthe computer vision community .\nLi et al. (2016)\npresent a detailed study of the energy use required\nfor training and inference in popular convolutional\nmodels for image classiÔ¨Åcation in computer vi-\nsion, including Ô¨Åne-grained analysis comparing\ndifferent neural network layer types.\nCanziani\net al. (2016) assess image classiÔ¨Åcation model ac-\ncuracy as a function of model size and gigaÔ¨Çops\nrequired during inference. They also measure av-\nerage power draw required during inference on\nGPUs as a function of batch size. Neither work an-\nalyzes the recurrent and self-attention models that\nhave become commonplace in NLP , nor do they\nextrapolate power to estimates of carbon and dol-\nlar cost of training.\nAnalysis of hyperparameter tuning has been\nperformed in the context of improved algorithms\nfor hyperparameter search (\nBergstra et al. , 2011;\nBergstra and Bengio , 2012; Snoek et al. , 2012). T o\nour knowledge there exists to date no analysis of\nthe computation required for R&amp;D and hyperpa-\nrameter tuning of neural network models in NLP .\n6 Via the authors on Reddit .\n7 GPU lower bound computed using pre-emptible\nP100/V100 U.S. resources priced at $0.43‚Äì$0.74/hr, upper\nbound uses on-demand U.S. resources priced at $1.46‚Äì\n$2.48/hr. W e similarly use pre-emptible ($1.46/hr‚Äì$2.40/ hr)\nand on-demand ($4.50/hr‚Äì$8/hr) pricing as lower and upper\nbounds for TPU v2/3; cheaper bulk contracts are available. Model Hardware Power (W) Hours kWh ¬∑PUE CO 2e Cloud compute cost\nTransformerbase P100x8 1415.78 12 27 26 $41‚Äì$140\nTransformerbig P100x8 1515.43 84 201 192 $289‚Äì$981\nELMo P100x3 517.66 336 275 262 $433‚Äì$1472\nBERTbase V100x64 12,041.51 79 1507 1438 $3751‚Äì$12,571\nBERTbase TPUv2x16 ‚Äî 96 ‚Äî ‚Äî $2074‚Äì$6912\nNAS P100x8 1515.43 274,120 656,347 626,155 $942,973‚Äì$3,20 1,722\nNAS TPUv2x1 ‚Äî 32,623 ‚Äî ‚Äî $44,055‚Äì$146,848\nGPT -2 TPUv3x32 ‚Äî 168 ‚Äî ‚Äî $12,902‚Äì$43,008\nT able 3: Estimated cost of training a model in terms of CO 2 emissions (lbs) and cloud compute cost (USD).\n7 Power\nand carbon footprint are omitted for TPUs due to lack of publi c information on power draw for this hardware.\n4 Experimental results\n4.1 Cost of training\nT able\n3 lists CO 2 emissions and estimated cost of\ntraining the models described in ¬ß2.1. Of note is\nthat TPUs are more cost-efÔ¨Åcient than GPUs on\nworkloads that make sense for that hardware (e.g.\nBERT). W e also see that models emit substan-\ntial carbon emissions; training BERT on GPU is\nroughly equivalent to a trans-American Ô¨Çight.\nSo\net al. (2019) report that NAS achieves a new state-\nof-the-art BLEU score of 29.7 for English to Ger-\nman machine translation, an increase of just 0.1\nBLEU at the cost of at least $150k in on-demand\ncompute time and non-trivial carbon emissions.\n4.2 Cost of development: Case study\nT o quantify the computational requirements of\nR&amp;D for a new model we study the logs of\nall training required to develop Linguistically-\nInformed Self-Attention (\nStrubell et al. , 2018), a\nmulti-task model that performs part-of-speech tag-\nging, labeled dependency parsing, predicate detec-\ntion and semantic role labeling. This model makes\nfor an interesting case study as a representative\nNLP pipeline and as a Best Long Paper at EMNLP .\nModel training associated with the project\nspanned a period of 172 days (approx. 6 months).\nDuring that time 123 small hyperparameter grid\nsearches were performed, resulting in 4789 jobs\nin total. Jobs varied in length ranging from a min-\nimum of 3 minutes, indicating a crash, to a maxi-\nmum of 9 days, with an average job length of 52\nhours. All training was done on a combination of\nNVIDIA Titan X (72%) and M40 (28%) GPUs.\n8\nThe sum GPU time required for the project\ntotaled 9998 days (27 years). This averages to\n8 W e approximate cloud compute cost using P100 pricing.\nEstimated cost (USD)\nModels Hours Cloud compute Electricity\n1 120 $52‚Äì$175 $5\n24 2880 $1238‚Äì$4205 $118\n4789 239,942 $103k‚Äì$350k $9870\nT able 4: Estimated cost in terms of cloud compute and\nelectricity for training: (1) a single model (2) a single\ntune and (3) all models trained during R&amp;D.\nabout 60 GPUs running constantly throughout the\n6 month duration of the project. T able\n4 lists upper\nand lower bounds of the estimated cost in terms\nof Google Cloud compute and raw electricity re-\nquired to develop and deploy this model.\n9 W e see\nthat while training a single model is relatively in-\nexpensive, the cost of tuning a model for a new\ndataset, which we estimate here to require 24 jobs,\nor performing the full R&amp;D required to develop\nthis model, quickly becomes extremely expensive.\n5 Conclusions\nAuthors should report training time and\nsensitivity to hyperparameters.\nOur experiments suggest that it would be beneÔ¨Å-\ncial to directly compare different models to per-\nform a cost-beneÔ¨Åt (accuracy) analysis. T o ad-\ndress this, when proposing a model that is meant\nto be re-trained for downstream use, such as re-\ntraining on a new domain or Ô¨Åne-tuning on a new\ntask, authors should report training time and com-\nputational resources required, as well as model\nsensitivity to hyperparameters. This will enable\ndirect comparison across models, allowing subse-\nquent consumers of these models to accurately as-\nsess whether the required computational resources\n9 Based on average U.S cost of electricity of $0.12/kWh. are compatible with their setting. More explicit\ncharacterization of tuning time could also reveal\ninconsistencies in time spent tuning baseline mod-\nels compared to proposed contributions. Realiz-\ning this will require: (1) a standard, hardware-\nindependent measurement of training time, such\nas gigaÔ¨Çops required to convergence, and (2) a\nstandard measurement of model sensitivity to data\nand hyperparameters, such as variance with re-\nspect to hyperparameters searched.\nAcademic researchers need equitable access to\ncomputation resources.\nRecent advances in available compute come at a\nhigh price not attainable to all who desire access.\nMost of the models studied in this paper were de-\nveloped outside academia; recent improvements in\nstate-of-the-art accuracy are possible thanks to in-\ndustry access to large-scale compute.\nLimiting this style of research to industry labs\nhurts the NLP research community in many ways.\nFirst, it stiÔ¨Çes creativity . Researchers with good\nideas but without access to large-scale compute\nwill simply not be able to execute their ideas,\ninstead constrained to focus on different prob-\nlems. Second, it prohibits certain types of re-\nsearch on the basis of access to Ô¨Ånancial resources.\nThis even more deeply promotes the already prob-\nlematic ‚Äúrich get richer‚Äù cycle of research fund-\ning, where groups that are already successful and\nthus well-funded tend to receive more funding\ndue to their existing accomplishments. Third, the\nprohibitive start-up cost of building in-house re-\nsources forces resource-poor groups to rely on\ncloud compute services such as A WS, Google\nCloud and Microsoft Azure.\nWhile these services provide valuable, Ô¨Çexi-\nble, and often relatively environmentally friendly\ncompute resources, it is more cost effective for\nacademic researchers, who often work for non-\nproÔ¨Åt educational institutions and whose research\nis funded by government entities, to pool resources\nto build shared compute centers at the level of\nfunding agencies, such as the U.S. National Sci-\nence Foundation. For example, an off-the-shelf\nGPU server containing 8 NVIDIA 1080 Ti GPUs\nand supporting hardware can be purchased for\napproximately $20,000 USD. At that cost, the\nhardware required to develop the model in our\ncase study (approximately 58 GPUs for 172 days)\nwould cost $145,000 USD plus electricity , about\nhalf the estimated cost to use on-demand cloud\nGPUs. Unlike money spent on cloud compute,\nhowever, that invested in centralized resources\nwould continue to pay off as resources are shared\nacross many projects. A government-funded aca-\ndemic compute cloud would provide equitable ac-\ncess to all researchers.\nResearchers should prioritize computationally\nefÔ¨Åcient hardware and algorithms.\nW e recommend a concerted effort by industry and\nacademia to promote research of more computa-\ntionally efÔ¨Åcient algorithms, as well as hardware\nthat requires less energy . An effort can also be\nmade in terms of software. There is already a\nprecedent for NLP software packages prioritizing\nefÔ¨Åcient models. An additional avenue through\nwhich NLP and machine learning software de-\nvelopers could aid in reducing the energy asso-\nciated with model tuning is by providing easy-\nto-use APIs implementing more efÔ¨Åcient alterna-\ntives to brute-force grid search for hyperparameter\ntuning, e.g. random or Bayesian hyperparameter\nsearch techniques (\nBergstra et al. , 2011; Bergstra\nand Bengio , 2012; Snoek et al. , 2012). While\nsoftware packages implementing these techniques\ndo exist,\n10 they are rarely employed in practice\nfor tuning NLP models. This is likely because\ntheir interoperability with popular deep learning\nframeworks such as PyT orch and T ensorFlow is\nnot optimized, i.e. there are not simple exam-\nples of how to tune T ensorFlow Estimators using\nBayesian search. Integrating these tools into the\nworkÔ¨Çows with which NLP researchers and practi-\ntioners are already familiar could have notable im-\npact on the cost of developing and tuning in NLP .\nAcknowledgements\nW e are grateful to Sherief Farouk and the anony-\nmous reviewers for helpful feedback on earlier\ndrafts. This work was supported in part by the\nCenters for Data Science and Intelligent Infor-\nmation Retrieval, the Chan Zuckerberg Initiative\nunder the ScientiÔ¨Åc Knowledge Base Construc-\ntion project, the IBM Cognitive Horizons Network\nagreement no. W1668553, and National Science\nFoundation grant no. IIS-1514053. Any opinions,\nÔ¨Åndings and conclusions or recommendations ex-\npressed in this material are those of the authors and\ndo not necessarily reÔ¨Çect those of the sponsor.\n10 For example, the Hyperopt Python library . References\nRhonda Ascierto. 2018.\nUptime Institute Global Data\nCenter Survey . T echnical report, Uptime Institute.\nDzmitry Bahdanau, Kyunghyun Cho, and Y oshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. In 3rd Inter-\nnational Conference for Learning Representations\n(ICLR), San Diego, California, USA.\nJames Bergstra and Y oshua Bengio. 2012. Random\nsearch for hyper-parameter optimization. Journal of\nMachine Learning Research, 13(Feb):281‚Äì305.\nJames S Bergstra, R¬¥ emi Bardenet, Y oshua Bengio, and\nBal¬¥ azs K ¬¥ egl. 2011. Algorithms for hyper-parameter\noptimization. In Advances in neural information\nprocessing systems, pages 2546‚Äì2554.\nBruno Burger. 2019.\nNet Public Electricity Generation\nin Germany in 2018 . T echnical report, Fraunhofer\nInstitute for Solar Energy Systems ISE.\nAlfredo Canziani, Adam Paszke, and Eugenio Culur-\nciello. 2016. An analysis of deep neural network\nmodels for practical applications .\nGary Cook, Jude Lee, T amina Tsai, Ada Kongn, John\nDeans, Brian Johnson, Elizabeth Jardim, and Brian\nJohnson. 2017.\nClicking Clean: Who is winning\nthe race to build a green internet? T echnical report,\nGreenpeace.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafÔ¨Åne attention for neural dependency pars-\ning. In ICLR.\nEP A. 2018.\nEmissions &amp; Generation Resource Inte-\ngrated Database (eGRID) . T echnical report, U.S.\nEnvironmental Protection Agency.\nChristopher Forster, Thor Johnsen, Swetha Man-\ndava, Sharath Turuvekere Sreenivas, Deyu Fu, Julie\nBernauer, Allison Gray, Sharan Chetlur, and Raul\nPuri. 2019.\nBER T Meets GPUs . T echnical report,\nNVIDIA AI.\nDa Li, Xinbo Chen, Michela Becchi, and Ziliang Zong.\n2016. Evaluating the energy efÔ¨Åciency of deep con-\nvolutional neural networks on cpus and gpus. 2016\nIEEE International Conferences on Big Data and\nCloud Computing (BDCloud), Social Computing\nand Networking (SocialCom), Sustainable Comput-\ning and Communications (SustainCom) (BDCloud-\nSocialCom-SustainCom), pages 477‚Äì484.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015.\nEffective approaches to attention-based\nneural machine translation . In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing , pages 1412‚Äì1421. Associa-\ntion for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In NAACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019.\nLanguage\nmodels are unsupervised multitask learners .\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\n2012. Practical bayesian optimization of machine\nlearning algorithms. In Advances in neural informa-\ntion processing systems, pages 2951‚Äì2959.\nDavid R. So, Chen Liang, and Quoc V . Le. 2019.\nThe evolved transformer . In Proceedings of the\n36th International Conference on Machine Learning\n(ICML).\nEmma Strubell, Patrick V erga, Daniel Andor,\nDavid W eiss, and Andrew McCallum. 2018.\nLinguistically-Informed Self-Attention for Se-\nmantic Role Labeling. In Conference on Empir-\nical Methods in Natural Language Processing\n(EMNLP), Brussels, Belgium.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In 31st Conference on Neural Information\nProcessing Systems (NIPS).'</code></pre>
</div>
</div>
<p>It is able to extract information from the table</p>
</section>
<section id="reply-56" class="level5">
<h5 class="anchored" data-anchor-id="reply-56">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>You‚Äôre right! Looking at the output, <code>rd.read_pdf</code> does extract the table content from strubell2019. For example, I can see Table 1 with the CO2 emissions data and Table 2 with the energy sources.</p>
<p>However, the structure is lost - it‚Äôs just linear text. So questions like ‚ÄúHow many days of CO‚ÇÇ emissions from an average American life are equivalent to training BERT base?‚Äù require the model to parse:</p>
<pre><code>Consumption CO 2e (lbs)
...
American life, avg, 1 year 36,156
...
BERT base ... 1438</code></pre>
<p>And then calculate: 1438 / (36,156/365) = 14.4 days</p>
<p>The challenge is that this information is spread across different parts of the text, and the table structure that makes it easy for humans to read is gone.</p>
<p>Would you still like to try extracting tables separately to preserve their structure better, or do you think the current extraction is sufficient?</p>
<p>It will better to try extracting tables separately to preserve the structure better</p>
</section>
<section id="reply-57" class="level5">
<h5 class="anchored" data-anchor-id="reply-57">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good choice! Extracting tables with their structure preserved will definitely help with those numerical questions.</p>
<p>For table extraction, you have a few options:</p>
<ol type="1">
<li><strong>pdfplumber</strong> - Extract tables as lists of dictionaries, good for structured tables</li>
<li><strong>camelot-py</strong> - Specialized for complex table layouts</li>
<li><strong>PyMuPDF (fitz)</strong> - Can extract tables and also get page images</li>
</ol>
<p>Which one would you like to try first? I‚Äôd suggest starting with <strong>pdfplumber</strong> since it‚Äôs straightforward and handles most academic paper tables well.</p>
<p>Extracting the table will not make it duplicate of the existing table information that is extracted rd.read_pdf How can we avoid that?</p>
</section>
<section id="reply-58" class="level5">
<h5 class="anchored" data-anchor-id="reply-58">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great point! You don‚Äôt want to duplicate the same information. Here are a few approaches:</p>
<ol type="1">
<li><strong>Replace table text with structured version</strong> - When you find a table, remove that text from the main content and replace it with a markdown/structured version</li>
<li><strong>Store tables as separate chunks</strong> - Extract tables separately and mark them with <code>type: "table"</code> in metadata, then skip those page regions when doing regular text extraction</li>
<li><strong>Hybrid approach</strong> - Keep both but use metadata to deduplicate during retrieval (e.g., filter by chunk type)</li>
</ol>
<p>The cleanest approach is probably #2 - extract tables separately with special metadata, then use a table-aware PDF reader that skips table regions.</p>
<p>Which approach sounds best for your use case?</p>
<p>Lets try to use pdf plumber and see if it is able to differentiate then we can use pdf plumber instead of rd.read_pdf</p>
</section>
<section id="reply-59" class="level5">
<h5 class="anchored" data-anchor-id="reply-59">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good plan! Let‚Äôs test pdfplumber on the strubell2019 PDF to see what it extracts.</p>
<p>First, you‚Äôll need to install it if you don‚Äôt have it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pdfplumber</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then try extracting from one page to see what tables it finds:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pdfplumber</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pdfplumber.<span class="bu">open</span>(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>) <span class="im">as</span> pdf:</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>    first_page <span class="op">=</span> pdf.pages[<span class="dv">0</span>]</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>    tables <span class="op">=</span> first_page.extract_tables()</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(tables)<span class="sc">}</span><span class="ss"> tables on page 1"</span>)</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tables:</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(tables[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will show you if pdfplumber can identify and extract tables separately. Want to try this?</p>
<div id="803da863" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pdfplumber</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting pdfplumber
  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)
Collecting pdfminer.six==20251107 (from pdfplumber)
  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)
Requirement already satisfied: Pillow&gt;=9.1 in /usr/local/lib/python3.12/site-packages (from pdfplumber) (12.0.0)
Collecting pypdfium2&gt;=4.18.0 (from pdfplumber)
  Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)
Requirement already satisfied: charset-normalizer&gt;=2.0.0 in /usr/local/lib/python3.12/site-packages (from pdfminer.six==20251107-&gt;pdfplumber) (3.4.4)
Requirement already satisfied: cryptography&gt;=36.0.0 in /usr/local/lib/python3.12/site-packages (from pdfminer.six==20251107-&gt;pdfplumber) (45.0.7)
Requirement already satisfied: cffi&gt;=1.14 in /usr/local/lib/python3.12/site-packages (from cryptography&gt;=36.0.0-&gt;pdfminer.six==20251107-&gt;pdfplumber) (2.0.0)
Requirement already satisfied: pycparser in /usr/local/lib/python3.12/site-packages (from cffi&gt;=1.14-&gt;cryptography&gt;=36.0.0-&gt;pdfminer.six==20251107-&gt;pdfplumber) (2.23)
Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)
Downloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/5.6 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.6/5.6 MB 79.8 MB/s  0:00:00
Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/3.0 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.0/3.0 MB 62.2 MB/s  0:00:00
Installing collected packages: pypdfium2, pdfminer.six, pdfplumber
  WARNING: The script pypdfium2 is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/3 [pdfminer.six]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/3 [pdfminer.six]  WARNING: The script pdfplumber is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 [pdfplumber]
Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.0.0</code></pre>
</div>
</div>
<div id="77ea2835" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pdfplumber</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pdfplumber.<span class="bu">open</span>(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>) <span class="im">as</span> pdf:</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a>    first_page <span class="op">=</span> pdf.pages[<span class="dv">0</span>]</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>    tables <span class="op">=</span> first_page.extract_tables()</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Found </span><span class="sc">{</span><span class="bu">len</span>(tables)<span class="sc">}</span><span class="ss"> tables on page 1"</span>)</span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tables:</span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(tables[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 0 tables on page 1</code></pre>
</div>
</div>
<p>It could not find any table</p>
</section>
<section id="reply-60" class="level5">
<h5 class="anchored" data-anchor-id="reply-60">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>The table might be on a different page. Try checking a few more pages to see where the tables are:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pdfplumber.<span class="bu">open</span>(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>) <span class="im">as</span> pdf:</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, page <span class="kw">in</span> <span class="bu">enumerate</span>(pdf.pages[:<span class="dv">5</span>]):  <span class="co"># Check first 5 pages</span></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>        tables <span class="op">=</span> page.extract_tables()</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> tables:</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Page </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: Found </span><span class="sc">{</span><span class="bu">len</span>(tables)<span class="sc">}</span><span class="ss"> table(s)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will show you which pages have tables that pdfplumber can detect. Want to try this?</p>
<div id="b7993ba9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pdfplumber.<span class="bu">open</span>(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>) <span class="im">as</span> pdf:</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, page <span class="kw">in</span> <span class="bu">enumerate</span>(pdf.pages[:<span class="dv">5</span>]):  <span class="co"># Check first 5 pages</span></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>        tables <span class="op">=</span> page.extract_tables()</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> tables:</span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Page </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: Found </span><span class="sc">{</span><span class="bu">len</span>(tables)<span class="sc">}</span><span class="ss"> table(s)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>It was on the first page</p>
</section>
<section id="reply-61" class="level5">
<h5 class="anchored" data-anchor-id="reply-61">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Interesting - so pdfplumber isn‚Äôt detecting the table on page 1 even though we can see it in the text. This could be because:</p>
<ol type="1">
<li>The table isn‚Äôt structured with proper borders/lines in the PDF</li>
<li>It‚Äôs formatted as plain text rather than a table object</li>
<li>pdfplumber‚Äôs default settings are too strict</li>
</ol>
<p>You could try adjusting pdfplumber‚Äôs table detection settings, or alternatively, try a different tool like <strong>camelot-py</strong> which is better at detecting borderless tables.</p>
<p>Would you like to try camelot, or adjust pdfplumber‚Äôs settings?</p>
<div id="6dd451ac" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>??rd.read_pdf</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display cell-output-markdown">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> read_pdf(file_path: <span class="bu">str</span> <span class="co"># path of PDF file to read</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>            ) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Reads the text of a PDF with PdfReader"</span></span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a>        reader <span class="op">=</span> PdfReader(<span class="bu">file</span>)</span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">' '</span>.join(page.extract_text() <span class="cf">for</span> page <span class="kw">in</span> reader.pages)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>File:</strong> <code>/usr/local/lib/python3.12/site-packages/contextkit/read.py</code></p>
</div>
</div>
<p>There is a python library marker https://github.com/datalab-to/marker which also extracts tables and images. I want to use it to extract chunks</p>
</section>
<section id="reply-62" class="level5">
<h5 class="anchored" data-anchor-id="reply-62">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great find! Marker is a modern library specifically designed for converting PDFs to markdown while preserving tables, images, and structure. It uses ML models for better extraction.</p>
<p>To use it, you‚Äôd need to:</p>
<ol type="1">
<li>Install marker: <code>!pip install marker-pdf</code></li>
<li>Use it to convert your PDFs to markdown format</li>
<li>Parse the markdown to extract text, tables, and images separately</li>
<li>Create chunks with appropriate metadata</li>
</ol>
<p>Would you like to try installing it and testing on one of your PDFs to see how well it extracts the tables?</p>
<p>It has its own in built functionality to create chunks, lets use that</p>
</section>
<section id="reply-63" class="level5">
<h5 class="anchored" data-anchor-id="reply-63">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Marker has built-in chunking functionality which should handle tables and images intelligently.</p>
<p>You‚Äôll want to explore Marker‚Äôs chunking API to see: 1. What metadata it provides for each chunk (like whether it‚Äôs a table, image, or text) 2. How it handles table formatting 3. What configuration options are available</p>
<p>Would you like to install marker and test it on one PDF first to see what the chunks look like?</p>
<p>Can you please read the repo?</p>
</section>
<section id="reply-64" class="level5">
<h5 class="anchored" data-anchor-id="reply-64">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<details class="tool-usage-details">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb124"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"id"</span><span class="fu">:</span> <span class="st">"toolu_012Avekt16UrcHMDNJjT9saE"</span><span class="fu">,</span></span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"call"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"function"</span><span class="fu">:</span> <span class="st">"read_url"</span><span class="fu">,</span></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"arguments"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>      <span class="dt">"url"</span><span class="fu">:</span> <span class="st">"https://github.com/datalab-to/marker"</span></span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span></span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">},</span></span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"result"</span><span class="fu">:</span> <span class="st">"'Skip to content</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Navigation Menu</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nToggle navigation</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ ](/)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fdatalab-to%2Fmarker)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAppearance settings</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Platform </span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ GitHub Copilot  Write better code with AI  ](https://github.com/features/copilot)</span><span class="ch">\\</span><span class="st">n    * [ GitHub Spark  New  Build and deploy intelligent apps  ](https://github.com/features/spark)</span><span class="ch">\\</span><span class="st">n    * [ GitHub Models  New  Manage and compare prompts  ](https://github.com/features/models)</span><span class="ch">\\</span><span class="st">n    * [ GitHub Advanced Security  Find and fix vulnerabilities  ](https://github.com/security/advanced-security)</span><span class="ch">\\</span><span class="st">n    * [ Actions  Automate any workflow  ](https://github.com/features/actions)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ Codespaces  Instant dev environments  ](https://github.com/features/codespaces)</span><span class="ch">\\</span><span class="st">n    * [ Issues  Plan and track work  ](https://github.com/features/issues)</span><span class="ch">\\</span><span class="st">n    * [ Code Review  Manage code changes  ](https://github.com/features/code-review)</span><span class="ch">\\</span><span class="st">n    * [ Discussions  Collaborate outside of code  ](https://github.com/features/discussions)</span><span class="ch">\\</span><span class="st">n    * [ Code Search  Find more, search less  ](https://github.com/features/code-search)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nExplore</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ Why GitHub ](https://github.com/why-github)</span><span class="ch">\\</span><span class="st">n    * [ Documentation  ](https://docs.github.com)</span><span class="ch">\\</span><span class="st">n    * [ GitHub Skills  ](https://skills.github.com)</span><span class="ch">\\</span><span class="st">n    * [ Blog  ](https://github.blog)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nIntegrations</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ GitHub Marketplace ](https://github.com/marketplace)</span><span class="ch">\\</span><span class="st">n    * [ MCP Registry ](https://github.com/mcp)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ View all features  ](https://github.com/features)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Solutions </span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nBy company size</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ Enterprises ](https://github.com/enterprise)</span><span class="ch">\\</span><span class="st">n    * [ Small and medium teams ](https://github.com/team)</span><span class="ch">\\</span><span class="st">n    * [ Startups ](https://github.com/enterprise/startups)</span><span class="ch">\\</span><span class="st">n    * [ Nonprofits ](/solutions/industry/nonprofits)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nBy use case</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ App Modernization ](/solutions/use-case/app-modernization)</span><span class="ch">\\</span><span class="st">n    * [ DevSecOps ](/solutions/use-case/devsecops)</span><span class="ch">\\</span><span class="st">n    * [ DevOps ](/solutions/use-case/devops)</span><span class="ch">\\</span><span class="st">n    * [ CI/CD ](/solutions/use-case/ci-cd)</span><span class="ch">\\</span><span class="st">n    * [ View all use cases ](/solutions/use-case)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nBy industry</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ Healthcare ](/solutions/industry/healthcare)</span><span class="ch">\\</span><span class="st">n    * [ Financial services ](/solutions/industry/financial-services)</span><span class="ch">\\</span><span class="st">n    * [ Manufacturing ](/solutions/industry/manufacturing)</span><span class="ch">\\</span><span class="st">n    * [ Government ](/solutions/industry/government)</span><span class="ch">\\</span><span class="st">n    * [ View all industries ](/solutions/industry)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ View all solutions  ](/solutions)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Resources </span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nTopics</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ AI ](/resources/articles?topic=ai)</span><span class="ch">\\</span><span class="st">n    * [ DevOps ](/resources/articles?topic=devops)</span><span class="ch">\\</span><span class="st">n    * [ Security ](/resources/articles?topic=security)</span><span class="ch">\\</span><span class="st">n    * [ Software Development ](/resources/articles?topic=software-development)</span><span class="ch">\\</span><span class="st">n    * [ View all ](/resources/articles)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nExplore</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ Learning Pathways  ](https://resources.github.com/learn/pathways)</span><span class="ch">\\</span><span class="st">n    * [ Events &amp; Webinars ](https://github.com/resources/events)</span><span class="ch">\\</span><span class="st">n    * [ Ebooks &amp; Whitepapers ](https://github.com/resources/whitepapers)</span><span class="ch">\\</span><span class="st">n    * [ Customer Stories ](https://github.com/customer-stories)</span><span class="ch">\\</span><span class="st">n    * [ Partners ](https://github.com/partners)</span><span class="ch">\\</span><span class="st">n    * [ Executive Insights ](https://github.com/solutions/executive-insights)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Open Source </span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ GitHub Sponsors  Fund open source developers  ](/sponsors)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ The ReadME Project  GitHub community articles  ](https://github.com/readme)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nRepositories</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ Topics ](https://github.com/topics)</span><span class="ch">\\</span><span class="st">n    * [ Trending ](https://github.com/trending)</span><span class="ch">\\</span><span class="st">n    * [ Collections ](https://github.com/collections)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Enterprise </span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ Enterprise platform  AI-powered developer platform  ](/enterprise)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAvailable add-ons</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n    * [ GitHub Advanced Security  Enterprise-grade security features  ](https://github.com/security/advanced-security)</span><span class="ch">\\</span><span class="st">n    * [ Copilot for business  Enterprise-grade AI features  ](/features/copilot/copilot-business)</span><span class="ch">\\</span><span class="st">n    * [ Premium Support  Enterprise-grade 24/7 support  ](/premium-support)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [Pricing](https://github.com/pricing)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSearch or jump to...</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Search code, repositories, users, issues, pull requests...</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSearch</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nClear</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n#  Provide feedback</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nWe read every piece of feedback, and take your input very seriously.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nInclude my email address so I can be contacted</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCancel  Submit feedback</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n#  Saved searches</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Use saved searches to filter your results more quickly</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nName</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nQuery</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nTo see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCancel  Create saved search</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ Sign in ](/login?return_to=https%3A%2F%2Fgithub.com%2Fdatalab-to%2Fmarker)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ Sign up ](/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=datalab-to%2Fmarker)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAppearance settings</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nResetting focus</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nYou signed in with another tab or window. [Reload]() to refresh your session. You signed out in another tab or window. [Reload]() to refresh your session. You switched accounts on another tab or window. [Reload]() to refresh your session. Dismiss alert</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n{{ message }}</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ datalab-to ](/datalab-to) / **[marker](/datalab-to/marker) ** Public</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [ Notifications ](/login?return_to=%2Fdatalab-to%2Fmarker) You must be signed in to change notification settings</span><span class="ch">\\</span><span class="st">n  * [ Fork 2k ](/login?return_to=%2Fdatalab-to%2Fmarker)</span><span class="ch">\\</span><span class="st">n  * [ Star  29.9k ](/login?return_to=%2Fdatalab-to%2Fmarker)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nConvert PDF to markdown + JSON quickly with high accuracy</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[www.datalab.to](https://www.datalab.to </span><span class="ch">\"</span><span class="st">https://www.datalab.to</span><span class="ch">\"</span><span class="st">)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n### License</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ GPL-3.0 license ](/datalab-to/marker/blob/master/LICENSE)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ 29.9k stars ](/datalab-to/marker/stargazers) [ 2k forks ](/datalab-to/marker/forks) [ Branches ](/datalab-to/marker/branches) [ Tags ](/datalab-to/marker/tags) [ Activity ](/datalab-to/marker/activity)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ Star  ](/login?return_to=%2Fdatalab-to%2Fmarker)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[ Notifications ](/login?return_to=%2Fdatalab-to%2Fmarker) You must be signed in to change notification settings</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [ Code ](/datalab-to/marker)</span><span class="ch">\\</span><span class="st">n  * [ Issues 298 ](/datalab-to/marker/issues)</span><span class="ch">\\</span><span class="st">n  * [ Pull requests 46 ](/datalab-to/marker/pulls)</span><span class="ch">\\</span><span class="st">n  * [ Actions ](/datalab-to/marker/actions)</span><span class="ch">\\</span><span class="st">n  * [ Projects 0 ](/datalab-to/marker/projects)</span><span class="ch">\\</span><span class="st">n  * [ Security Uh oh!  There was an error while loading. [Please reload this page](). ](/datalab-to/marker/security)</span><span class="ch">\\</span><span class="st">n  * [ Insights ](/datalab-to/marker/pulse)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAdditional navigation options</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * [ Code  ](/datalab-to/marker)</span><span class="ch">\\</span><span class="st">n  * [ Issues  ](/datalab-to/marker/issues)</span><span class="ch">\\</span><span class="st">n  * [ Pull requests  ](/datalab-to/marker/pulls)</span><span class="ch">\\</span><span class="st">n  * [ Actions  ](/datalab-to/marker/actions)</span><span class="ch">\\</span><span class="st">n  * [ Projects  ](/datalab-to/marker/projects)</span><span class="ch">\\</span><span class="st">n  * [ Security  ](/datalab-to/marker/security)</span><span class="ch">\\</span><span class="st">n  * [ Insights  ](/datalab-to/marker/pulse)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# datalab-to/marker</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nmaster</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[Branches](/datalab-to/marker/branches)[Tags](/datalab-to/marker/tags)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[](/datalab-to/marker/branches)[](/datalab-to/marker/tags)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nGo to file</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nCode</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOpen more actions menu</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Folders and files</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nName| Name| Last commit message| Last commit date  </span><span class="ch">\\</span><span class="st">n---|---|---|---  </span><span class="ch">\\</span><span class="st">n  </span><span class="ch">\\</span><span class="st">n## Latest commit</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## History</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[1,339 Commits](/datalab-to/marker/commits/master/)[](/datalab-to/marker/commits/master/)  </span><span class="ch">\\</span><span class="st">n[.github](/datalab-to/marker/tree/master/.github </span><span class="ch">\"</span><span class="st">.github</span><span class="ch">\"</span><span class="st">)| [.github](/datalab-to/marker/tree/master/.github </span><span class="ch">\"</span><span class="st">.github</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[benchmarks](/datalab-to/marker/tree/master/benchmarks </span><span class="ch">\"</span><span class="st">benchmarks</span><span class="ch">\"</span><span class="st">)| [benchmarks](/datalab-to/marker/tree/master/benchmarks </span><span class="ch">\"</span><span class="st">benchmarks</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[data](/datalab-to/marker/tree/master/data </span><span class="ch">\"</span><span class="st">data</span><span class="ch">\"</span><span class="st">)| [data](/datalab-to/marker/tree/master/data </span><span class="ch">\"</span><span class="st">data</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[examples](/datalab-to/marker/tree/master/examples </span><span class="ch">\"</span><span class="st">examples</span><span class="ch">\"</span><span class="st">)| [examples](/datalab-to/marker/tree/master/examples </span><span class="ch">\"</span><span class="st">examples</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[marker](/datalab-to/marker/tree/master/marker </span><span class="ch">\"</span><span class="st">marker</span><span class="ch">\"</span><span class="st">)| [marker](/datalab-to/marker/tree/master/marker </span><span class="ch">\"</span><span class="st">marker</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[signatures/version1](/datalab-to/marker/tree/master/signatures/version1 </span><span class="ch">\"</span><span class="st">This path skips through empty directories</span><span class="ch">\"</span><span class="st">)| [signatures/version1](/datalab-to/marker/tree/master/signatures/version1 </span><span class="ch">\"</span><span class="st">This path skips through empty directories</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[static/fonts](/datalab-to/marker/tree/master/static/fonts </span><span class="ch">\"</span><span class="st">This path skips through empty directories</span><span class="ch">\"</span><span class="st">)| [static/fonts](/datalab-to/marker/tree/master/static/fonts </span><span class="ch">\"</span><span class="st">This path skips through empty directories</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[tests](/datalab-to/marker/tree/master/tests </span><span class="ch">\"</span><span class="st">tests</span><span class="ch">\"</span><span class="st">)| [tests](/datalab-to/marker/tree/master/tests </span><span class="ch">\"</span><span class="st">tests</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[.gitignore](/datalab-to/marker/blob/master/.gitignore </span><span class="ch">\"</span><span class="st">.gitignore</span><span class="ch">\"</span><span class="st">)| [.gitignore](/datalab-to/marker/blob/master/.gitignore </span><span class="ch">\"</span><span class="st">.gitignore</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[.pre-commit-config.yaml](/datalab-to/marker/blob/master/.pre-commit-config.yaml </span><span class="ch">\"</span><span class="st">.pre-commit-config.yaml</span><span class="ch">\"</span><span class="st">)| [.pre-commit-config.yaml](/datalab-to/marker/blob/master/.pre-commit-config.yaml </span><span class="ch">\"</span><span class="st">.pre-commit-config.yaml</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[CLA.md](/datalab-to/marker/blob/master/CLA.md </span><span class="ch">\"</span><span class="st">CLA.md</span><span class="ch">\"</span><span class="st">)| [CLA.md](/datalab-to/marker/blob/master/CLA.md </span><span class="ch">\"</span><span class="st">CLA.md</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[LICENSE](/datalab-to/marker/blob/master/LICENSE </span><span class="ch">\"</span><span class="st">LICENSE</span><span class="ch">\"</span><span class="st">)| [LICENSE](/datalab-to/marker/blob/master/LICENSE </span><span class="ch">\"</span><span class="st">LICENSE</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[MODEL_LICENSE](/datalab-to/marker/blob/master/MODEL_LICENSE </span><span class="ch">\"</span><span class="st">MODEL_LICENSE</span><span class="ch">\"</span><span class="st">)| [MODEL_LICENSE](/datalab-to/marker/blob/master/MODEL_LICENSE </span><span class="ch">\"</span><span class="st">MODEL_LICENSE</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[README.md](/datalab-to/marker/blob/master/README.md </span><span class="ch">\"</span><span class="st">README.md</span><span class="ch">\"</span><span class="st">)| [README.md](/datalab-to/marker/blob/master/README.md </span><span class="ch">\"</span><span class="st">README.md</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[chunk_convert.py](/datalab-to/marker/blob/master/chunk_convert.py </span><span class="ch">\"</span><span class="st">chunk_convert.py</span><span class="ch">\"</span><span class="st">)| [chunk_convert.py](/datalab-to/marker/blob/master/chunk_convert.py </span><span class="ch">\"</span><span class="st">chunk_convert.py</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[convert.py](/datalab-to/marker/blob/master/convert.py </span><span class="ch">\"</span><span class="st">convert.py</span><span class="ch">\"</span><span class="st">)| [convert.py](/datalab-to/marker/blob/master/convert.py </span><span class="ch">\"</span><span class="st">convert.py</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[convert_single.py](/datalab-to/marker/blob/master/convert_single.py </span><span class="ch">\"</span><span class="st">convert_single.py</span><span class="ch">\"</span><span class="st">)| [convert_single.py](/datalab-to/marker/blob/master/convert_single.py </span><span class="ch">\"</span><span class="st">convert_single.py</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[extraction_app.py](/datalab-to/marker/blob/master/extraction_app.py </span><span class="ch">\"</span><span class="st">extraction_app.py</span><span class="ch">\"</span><span class="st">)| [extraction_app.py](/datalab-to/marker/blob/master/extraction_app.py </span><span class="ch">\"</span><span class="st">extraction_app.py</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[marker_app.py](/datalab-to/marker/blob/master/marker_app.py </span><span class="ch">\"</span><span class="st">marker_app.py</span><span class="ch">\"</span><span class="st">)| [marker_app.py](/datalab-to/marker/blob/master/marker_app.py </span><span class="ch">\"</span><span class="st">marker_app.py</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[marker_server.py](/datalab-to/marker/blob/master/marker_server.py </span><span class="ch">\"</span><span class="st">marker_server.py</span><span class="ch">\"</span><span class="st">)| [marker_server.py](/datalab-to/marker/blob/master/marker_server.py </span><span class="ch">\"</span><span class="st">marker_server.py</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[poetry.lock](/datalab-to/marker/blob/master/poetry.lock </span><span class="ch">\"</span><span class="st">poetry.lock</span><span class="ch">\"</span><span class="st">)| [poetry.lock](/datalab-to/marker/blob/master/poetry.lock </span><span class="ch">\"</span><span class="st">poetry.lock</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[pyproject.toml](/datalab-to/marker/blob/master/pyproject.toml </span><span class="ch">\"</span><span class="st">pyproject.toml</span><span class="ch">\"</span><span class="st">)| [pyproject.toml](/datalab-to/marker/blob/master/pyproject.toml </span><span class="ch">\"</span><span class="st">pyproject.toml</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">n[pytest.ini](/datalab-to/marker/blob/master/pytest.ini </span><span class="ch">\"</span><span class="st">pytest.ini</span><span class="ch">\"</span><span class="st">)| [pytest.ini](/datalab-to/marker/blob/master/pytest.ini </span><span class="ch">\"</span><span class="st">pytest.ini</span><span class="ch">\"</span><span class="st">)|  |   </span><span class="ch">\\</span><span class="st">nView all files  </span><span class="ch">\\</span><span class="st">n  </span><span class="ch">\\</span><span class="st">n## Repository files navigation</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * README</span><span class="ch">\\</span><span class="st">n  * GPL-3.0 license</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Marker</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMarker converts documents to markdown, JSON, chunks, and HTML quickly and accurately.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Converts PDF, image, PPTX, DOCX, XLSX, HTML, EPUB files in all languages</span><span class="ch">\\</span><span class="st">n  * Formats tables, forms, equations, inline math, links, references, and code blocks</span><span class="ch">\\</span><span class="st">n  * Extracts and saves images</span><span class="ch">\\</span><span class="st">n  * Removes headers/footers/other artifacts</span><span class="ch">\\</span><span class="st">n  * Extensible with your own formatting and logic</span><span class="ch">\\</span><span class="st">n  * Does structured extraction, given a JSON schema (beta)</span><span class="ch">\\</span><span class="st">n  * Optionally boost accuracy with LLMs (and your own prompt)</span><span class="ch">\\</span><span class="st">n  * Works on GPU, CPU, or MPS</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFor our managed API or on-prem document intelligence solution, check out [our platform here](https://datalab.to?utm_source=gh-marker).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Performance</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[![](/datalab-to/marker/raw/master/data/images/overall.png)](/datalab-to/marker/blob/master/data/images/overall.png)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nMarker benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nThe above results are running single PDF pages serially. Marker is significantly faster when running in batch mode, with a projected throughput of 25 pages/second on an H100.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSee below for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Hybrid Mode</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFor the highest accuracy, pass the `--use_llm` flag to use an LLM alongside marker. This will do things like merge tables across pages, handle inline math, format tables properly, and extract values from forms. It can use any gemini or ollama model. By default, it uses `gemini-2.0-flash`. See below for details.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nHere is a table benchmark comparing marker, gemini flash alone, and marker with use_llm:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[![](/datalab-to/marker/raw/master/data/images/table.png)](/datalab-to/marker/blob/master/data/images/table.png)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nAs you can see, the use_llm mode offers higher accuracy than marker or gemini alone.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Examples</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nPDF | File type | Markdown | JSON  </span><span class="ch">\\</span><span class="st">n---|---|---|---  </span><span class="ch">\\</span><span class="st">n[Think Python](https://greenteapress.com/thinkpython/thinkpython.pdf) | Textbook | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/thinkpython/thinkpython.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/thinkpython.json)  </span><span class="ch">\\</span><span class="st">n[Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/switch_transformers/switch_trans.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/switch_trans.json)  </span><span class="ch">\\</span><span class="st">n[Multi-column CNN](https://arxiv.org/pdf/1804.07821.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/multicolcnn/multicolcnn.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/multicolcnn.json)  </span><span class="ch">\\</span><span class="st">n  </span><span class="ch">\\</span><span class="st">n# Commercial usage</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOur model weights use a modified AI Pubs Open Rail-M license (free for research, personal use, and startups under $2M funding/revenue) and our code is GPL. For broader commercial licensing or to remove GPL requirements, visit our pricing page [here](https://www.datalab.to/pricing?utm_source=gh-marker).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Hosted API &amp; On-prem</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nThere</span><span class="ch">\\</span><span class="st">'s a [hosted API](https://www.datalab.to?utm_source=gh-marker) and [painless on-prem solution](https://www.datalab.to/blog/self-serve-on-prem-licensing) for marker - it</span><span class="ch">\\</span><span class="st">'s free to sign up, and we</span><span class="ch">\\</span><span class="st">'ll throw in credits for you to test it out.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nThe API:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Supports PDF, image, PPT, PPTX, DOC, DOCX, XLS, XLSX, HTML, EPUB files</span><span class="ch">\\</span><span class="st">n  * Is 1/4th the price of leading cloud-based competitors</span><span class="ch">\\</span><span class="st">n  * Fast - ~15s for a 250 page PDF</span><span class="ch">\\</span><span class="st">n  * Supports LLM mode</span><span class="ch">\\</span><span class="st">n  * High uptime (99.99%)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Community</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n[Discord](https://discord.gg//KuZwXNGnfH) is where we discuss future development.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Installation</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nYou</span><span class="ch">\\</span><span class="st">'ll need python 3.10+ and [PyTorch](https://pytorch.org/get-started/locally/).</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nInstall with:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">npip install marker-pdf</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nIf you want to use marker on documents other than PDFs, you will need to install additional dependencies with:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">npip install marker-pdf[full]</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n# Usage</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nFirst, some configuration:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * Your torch device will be automatically detected, but you can override this. For example, `TORCH_DEVICE=cuda`.</span><span class="ch">\\</span><span class="st">n  * Some PDFs, even digital ones, have bad text in them. Set `--force_ocr` to force OCR on all lines, or the `strip_existing_ocr` to keep all digital text, and strip out any existing OCR text.</span><span class="ch">\\</span><span class="st">n  * If you care about inline math, set `force_ocr` to convert inline math to LaTeX.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Interactive App</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nI</span><span class="ch">\\</span><span class="st">'ve included a streamlit app that lets you interactively try marker with some basic options. Run it with:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">npip install streamlit streamlit-ace</span><span class="ch">\\</span><span class="st">nmarker_gui</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Convert a single file</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nmarker_single /path/to/file.pdf</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nYou can pass in PDFs or images.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nOptions:</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * `--page_range TEXT`: Specify which pages to process. Accepts comma-separated page numbers and ranges. Example: `--page_range </span><span class="ch">\"</span><span class="st">0,5-10,20</span><span class="ch">\"</span><span class="st">` will process pages 0, 5 through 10, and page 20.</span><span class="ch">\\</span><span class="st">n  * `--output_format [markdown|json|html|chunks]`: Specify the format for the output results.</span><span class="ch">\\</span><span class="st">n  * `--output_dir PATH`: Directory where output files will be saved. Defaults to the value specified in settings.OUTPUT_DIR.</span><span class="ch">\\</span><span class="st">n  * `--paginate_output`: Paginates the output, using `</span><span class="ch">\\\\</span><span class="st">n</span><span class="ch">\\\\</span><span class="st">n{PAGE_NUMBER}` followed by `-` * 48, then `</span><span class="ch">\\\\</span><span class="st">n</span><span class="ch">\\\\</span><span class="st">n`</span><span class="ch">\\</span><span class="st">n  * `--use_llm`: Uses an LLM to improve accuracy. You will need to configure the LLM backend - see below.</span><span class="ch">\\</span><span class="st">n  * `--force_ocr`: Force OCR processing on the entire document, even for pages that might contain extractable text. This will also format inline math properly.</span><span class="ch">\\</span><span class="st">n  * `--block_correction_prompt`: if LLM mode is active, an optional prompt that will be used to correct the output of marker. This is useful for custom formatting or logic that you want to apply to the output.</span><span class="ch">\\</span><span class="st">n  * `--strip_existing_ocr`: Remove all existing OCR text in the document and re-OCR with surya.</span><span class="ch">\\</span><span class="st">n  * `--redo_inline_math`: If you want the absolute highest quality inline math conversion, use this along with `--use_llm`.</span><span class="ch">\\</span><span class="st">n  * `--disable_image_extraction`: Don</span><span class="ch">\\</span><span class="st">'t extract images from the PDF. If you also specify `--use_llm`, then images will be replaced with a description.</span><span class="ch">\\</span><span class="st">n  * `--debug`: Enable debug mode for additional logging and diagnostic information.</span><span class="ch">\\</span><span class="st">n  * `--processors TEXT`: Override the default processors by providing their full module paths, separated by commas. Example: `--processors </span><span class="ch">\"</span><span class="st">module1.processor1,module2.processor2</span><span class="ch">\"</span><span class="st">`</span><span class="ch">\\</span><span class="st">n  * `--config_json PATH`: Path to a JSON configuration file containing additional settings.</span><span class="ch">\\</span><span class="st">n  * `config --help`: List all available builders, processors, and converters, and their associated configuration. These values can be used to build a JSON configuration file for additional tweaking of marker defaults.</span><span class="ch">\\</span><span class="st">n  * `--converter_cls`: One of `marker.converters.pdf.PdfConverter` (default) or `marker.converters.table.TableConverter`. The `PdfConverter` will convert the whole PDF, the `TableConverter` will only extract and convert tables.</span><span class="ch">\\</span><span class="st">n  * `--llm_service`: Which llm service to use if `--use_llm` is passed. This defaults to `marker.services.gemini.GoogleGeminiService`.</span><span class="ch">\\</span><span class="st">n  * `--help`: see all of the flags that can be passed into marker. (it supports many more options then are listed above)</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nThe list of supported languages for surya OCR is [here](https://github.com/VikParuchuri/surya/blob/master/surya/recognition/languages.py). If you don</span><span class="ch">\\</span><span class="st">'t need OCR, marker can work with any language.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Convert multiple files</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nmarker /path/to/input/folder</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * `marker` supports all the same options from `marker_single` above.</span><span class="ch">\\</span><span class="st">n  * `--workers` is the number of conversion workers to run simultaneously. This is automatically set by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Marker will use 5GB of VRAM per worker at the peak, and 3.5GB average.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Convert multiple files on multiple GPUs</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nNUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n  * `NUM_DEVICES` is the number of GPUs to use. Should be `2` or greater.</span><span class="ch">\\</span><span class="st">n  * `NUM_WORKERS` is the number of parallel processes to run on each GPU.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n## Use from python</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nSee the `PdfConverter` class at `marker/converters/pdf.py` function for additional arguments that can be passed.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">nfrom marker.converters.pdf import PdfConverter</span><span class="ch">\\</span><span class="st">nfrom marker.models import create_model_dict</span><span class="ch">\\</span><span class="st">nfrom marker.output import text_from_rendered</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nconverter = PdfConverter(</span><span class="ch">\\</span><span class="st">n    artifact_dict=create_model_dict(),</span><span class="ch">\\</span><span class="st">n)</span><span class="ch">\\</span><span class="st">nrendered = converter(</span><span class="ch">\"</span><span class="st">FILEPATH</span><span class="ch">\"</span><span class="st">)</span><span class="ch">\\</span><span class="st">ntext, _, images = text_from_rendered(rendered)</span><span class="ch">\\</span><span class="st">n```</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n`rendered` will be a pydantic basemodel with different properties depending on the output type requested. With markdown output (default), you</span><span class="ch">\\</span><span class="st">'ll have the properties `markdown`, `metadata`, and `images`. For json output, you</span><span class="ch">\\</span><span class="st">'ll have `children`, `block_type`, and `metadata`.</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n### Custom configuration</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">nYou can pass configuration using the `ConfigParser`. To see all availa&lt;TRUNCATED&gt;"</span></span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<p>Perfect! I‚Äôve read the Marker repo. Key findings for your use case:</p>
<p><strong>Marker has built-in chunking support!</strong> There‚Äôs a <code>chunk_convert.py</code> file and you can output in ‚Äúchunks‚Äù format using <code>--output_format chunks</code>.</p>
<p>The chunks format: - Flattens everything into a single list (not a tree) - Only top-level blocks from each page - Includes full HTML of each block - Designed specifically for RAG applications</p>
<p><strong>For tables specifically:</strong> - Marker extracts and formats tables automatically - Can use <code>--use_llm</code> flag to improve table accuracy significantly (0.816 ‚Üí 0.907 score) - Has a dedicated <code>TableConverter</code> for table-only extraction</p>
<p>Would you like to try using Marker‚Äôs chunking functionality to replace your current document processing?</p>
<p>I want to first try, lets try it on the strubell2019 document</p>
</section>
<section id="reply-65" class="level5">
<h5 class="anchored" data-anchor-id="reply-65">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! Let‚Äôs start by installing marker and testing it on that document.</p>
<p>First, install marker:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install marker<span class="op">-</span>pdf</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then try converting the strubell2019.pdf with chunking output:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>marker_single wattbot_data<span class="op">/</span>strubell2019.pdf <span class="op">--</span>output_format chunks <span class="op">--</span>output_dir marker_output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will create chunks and save them to the <code>marker_output</code> directory. Want to give it a try and see what the output looks like?</p>
<div id="78e4ef02" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install marker<span class="op">-</span>pdf</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting marker-pdf
  Downloading marker_pdf-1.10.1-py3-none-any.whl.metadata (30 kB)
Collecting Pillow&lt;11.0.0,&gt;=10.1.0 (from marker-pdf)
  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)
Collecting anthropic&lt;0.47.0,&gt;=0.46.0 (from marker-pdf)
  Downloading anthropic-0.46.0-py3-none-any.whl.metadata (23 kB)
Requirement already satisfied: click&lt;9.0.0,&gt;=8.2.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (8.3.1)
Requirement already satisfied: filetype&lt;2.0.0,&gt;=1.2.0 in ./.local/lib/python3.12/site-packages (from marker-pdf) (1.2.0)
Collecting ftfy&lt;7.0.0,&gt;=6.1.1 (from marker-pdf)
  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)
Requirement already satisfied: google-genai&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (1.51.0)
Collecting markdown2&lt;3.0.0,&gt;=2.5.2 (from marker-pdf)
  Downloading markdown2-2.5.4-py3-none-any.whl.metadata (2.1 kB)
Collecting markdownify&lt;2.0.0,&gt;=1.1.0 (from marker-pdf)
  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)
Collecting openai&lt;2.0.0,&gt;=1.65.2 (from marker-pdf)
  Using cached openai-1.109.1-py3-none-any.whl.metadata (29 kB)
Collecting pdftext&lt;0.7.0,&gt;=0.6.3 (from marker-pdf)
  Downloading pdftext-0.6.3-py3-none-any.whl.metadata (8.5 kB)
Collecting pre-commit&lt;5.0.0,&gt;=4.2.0 (from marker-pdf)
  Downloading pre_commit-4.4.0-py2.py3-none-any.whl.metadata (1.2 kB)
Requirement already satisfied: pydantic&lt;3.0.0,&gt;=2.4.2 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (2.12.4)
Requirement already satisfied: pydantic-settings&lt;3.0.0,&gt;=2.0.3 in ./.local/lib/python3.12/site-packages (from marker-pdf) (2.10.1)
Requirement already satisfied: python-dotenv&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (1.2.1)
Collecting rapidfuzz&lt;4.0.0,&gt;=3.8.1 (from marker-pdf)
  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)
Collecting regex&lt;2025.0.0,&gt;=2024.4.28 (from marker-pdf)
  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
Requirement already satisfied: scikit-learn&lt;2.0.0,&gt;=1.6.1 in ./.local/lib/python3.12/site-packages (from marker-pdf) (1.7.2)
Collecting surya-ocr&lt;0.18.0,&gt;=0.17.0 (from marker-pdf)
  Downloading surya_ocr-0.17.0-py3-none-any.whl.metadata (34 kB)
Requirement already satisfied: torch&lt;3.0.0,&gt;=2.7.0 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (2.9.1+cpu)
Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /usr/local/lib/python3.12/site-packages (from marker-pdf) (4.67.1)
Requirement already satisfied: transformers&lt;5.0.0,&gt;=4.45.2 in ./.local/lib/python3.12/site-packages (from marker-pdf) (4.56.2)
Requirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (4.11.0)
Requirement already satisfied: distro&lt;2,&gt;=1.7.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (1.9.0)
Requirement already satisfied: httpx&lt;1,&gt;=0.23.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (0.28.1)
Requirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (0.12.0)
Requirement already satisfied: sniffio in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (1.3.1)
Requirement already satisfied: typing-extensions&lt;5,&gt;=4.10 in /usr/local/lib/python3.12/site-packages (from anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (4.15.0)
Requirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.12/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (3.11)
Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/site-packages (from ftfy&lt;7.0.0,&gt;=6.1.1-&gt;marker-pdf) (0.2.14)
Requirement already satisfied: google-auth&lt;3.0.0,&gt;=2.14.1 in /usr/local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (2.43.0)
Requirement already satisfied: requests&lt;3.0.0,&gt;=2.28.1 in /usr/local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (2.32.5)
Requirement already satisfied: tenacity&lt;9.2.0,&gt;=8.2.3 in ./.local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (9.1.2)
Requirement already satisfied: websockets&lt;15.1.0,&gt;=13.0.0 in /usr/local/lib/python3.12/site-packages (from google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (15.0.1)
Requirement already satisfied: cachetools&lt;7.0,&gt;=2.0.0 in /usr/local/lib/python3.12/site-packages (from google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (6.2.2)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.12/site-packages (from google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (0.4.2)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.12/site-packages (from google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (4.9.1)
Requirement already satisfied: certifi in /usr/local/lib/python3.12/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (2025.11.12)
Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (1.0.9)
Requirement already satisfied: h11&gt;=0.16 in /usr/local/lib/python3.12/site-packages (from httpcore==1.*-&gt;httpx&lt;1,&gt;=0.23.0-&gt;anthropic&lt;0.47.0,&gt;=0.46.0-&gt;marker-pdf) (0.16.0)
Requirement already satisfied: beautifulsoup4&lt;5,&gt;=4.9 in /usr/local/lib/python3.12/site-packages (from markdownify&lt;2.0.0,&gt;=1.1.0-&gt;marker-pdf) (4.14.2)
Requirement already satisfied: six&lt;2,&gt;=1.15 in /usr/local/lib/python3.12/site-packages (from markdownify&lt;2.0.0,&gt;=1.1.0-&gt;marker-pdf) (1.17.0)
Requirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.12/site-packages (from beautifulsoup4&lt;5,&gt;=4.9-&gt;markdownify&lt;2.0.0,&gt;=1.1.0-&gt;marker-pdf) (2.8)
Collecting pypdfium2==4.30.0 (from pdftext&lt;0.7.0,&gt;=0.6.3-&gt;marker-pdf)
  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)
Collecting cfgv&gt;=2.0.0 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)
  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)
Collecting identify&gt;=1.0.0 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)
  Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)
Collecting nodeenv&gt;=0.11.1 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)
  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.12/site-packages (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf) (6.0.3)
Collecting virtualenv&gt;=20.10.0 (from pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)
  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)
Requirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic&lt;3.0.0,&gt;=2.4.2-&gt;marker-pdf) (0.7.0)
Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/site-packages (from pydantic&lt;3.0.0,&gt;=2.4.2-&gt;marker-pdf) (2.41.5)
Requirement already satisfied: typing-inspection&gt;=0.4.2 in /usr/local/lib/python3.12/site-packages (from pydantic&lt;3.0.0,&gt;=2.4.2-&gt;marker-pdf) (0.4.2)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.28.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (3.4.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests&lt;3.0.0,&gt;=2.28.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (2.5.0)
Requirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.12/site-packages (from rsa&lt;5,&gt;=3.1.4-&gt;google-auth&lt;3.0.0,&gt;=2.14.1-&gt;google-genai&lt;2.0.0,&gt;=1.0.0-&gt;marker-pdf) (0.6.1)
Requirement already satisfied: numpy&gt;=1.22.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (2.3.5)
Requirement already satisfied: scipy&gt;=1.8.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (1.16.3)
Requirement already satisfied: joblib&gt;=1.2.0 in ./.local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (1.5.2)
Requirement already satisfied: threadpoolctl&gt;=3.1.0 in ./.local/lib/python3.12/site-packages (from scikit-learn&lt;2.0.0,&gt;=1.6.1-&gt;marker-pdf) (3.6.0)
Collecting einops&lt;0.9.0,&gt;=0.8.1 (from surya-ocr&lt;0.18.0,&gt;=0.17.0-&gt;marker-pdf)
  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)
Collecting opencv-python-headless==4.11.0.86 (from surya-ocr&lt;0.18.0,&gt;=0.17.0-&gt;marker-pdf)
  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
Requirement already satisfied: platformdirs&lt;5.0.0,&gt;=4.3.6 in /usr/local/lib/python3.12/site-packages (from surya-ocr&lt;0.18.0,&gt;=0.17.0-&gt;marker-pdf) (4.5.0)
Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.20.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (80.9.0)
Requirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (1.14.0)
Requirement already satisfied: networkx&gt;=2.5.1 in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.5)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.1.6)
Requirement already satisfied: fsspec&gt;=0.8.5 in ./.local/lib/python3.12/site-packages (from torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (2025.3.0)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.34.0 in ./.local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (0.36.0)
Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (25.0)
Requirement already satisfied: tokenizers&lt;=0.23.0,&gt;=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (0.22.1)
Requirement already satisfied: safetensors&gt;=0.4.3 in ./.local/lib/python3.12/site-packages (from transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (0.6.2)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers&lt;5.0.0,&gt;=4.45.2-&gt;marker-pdf) (1.2.0)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy&gt;=1.13.3-&gt;torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (1.3.0)
Collecting distlib&lt;1,&gt;=0.3.7 (from virtualenv&gt;=20.10.0-&gt;pre-commit&lt;5.0.0,&gt;=4.2.0-&gt;marker-pdf)
  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2-&gt;torch&lt;3.0.0,&gt;=2.7.0-&gt;marker-pdf) (3.0.3)
Downloading marker_pdf-1.10.1-py3-none-any.whl (188 kB)
Downloading anthropic-0.46.0-py3-none-any.whl (223 kB)
Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)
Downloading markdown2-2.5.4-py3-none-any.whl (49 kB)
Downloading markdownify-1.2.2-py3-none-any.whl (15 kB)
Using cached openai-1.109.1-py3-none-any.whl (948 kB)
Downloading pdftext-0.6.3-py3-none-any.whl (23 kB)
Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/2.8 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.8/2.8 MB 86.5 MB/s  0:00:00
Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/4.5 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫ 4.5/4.5 MB 280.7 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4.5/4.5 MB 16.0 MB/s  0:00:00
Downloading pre_commit-4.4.0-py2.py3-none-any.whl (226 kB)
Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/3.2 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏ 3.1/3.2 MB 233.7 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.2/3.2 MB 12.3 MB/s  0:00:00
Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/796.9 kB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 796.9/796.9 kB 36.4 MB/s  0:00:00
Downloading surya_ocr-0.17.0-py3-none-any.whl (183 kB)
Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/50.0 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 38.3/50.0 MB 191.7 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏ 49.8/50.0 MB 176.3 MB/s eta 0:00:01   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50.0/50.0 MB 98.1 MB/s  0:00:00
Downloading einops-0.8.1-py3-none-any.whl (64 kB)
Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)
Downloading identify-2.6.15-py2.py3-none-any.whl (99 kB)
Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)
Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/6.0 MB ? eta -:--:--   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.0/6.0 MB 49.7 MB/s  0:00:00
Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)
Installing collected packages: distlib, virtualenv, regex, rapidfuzz, pypdfium2, Pillow, opencv-python-headless, nodeenv, markdown2, identify, ftfy, einops, cfgv, pre-commit, markdownify, openai, anthropic, pdftext, surya-ocr, marker-pdf
   ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  1/20 [virtualenv]  WARNING: The script virtualenv is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]  Attempting uninstall: pypdfium2
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]    Found existing installation: pypdfium2 5.0.0
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]    Uninstalling pypdfium2-5.0.0:
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]      Successfully uninstalled pypdfium2-5.0.0
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  3/20 [rapidfuzz]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  4/20 [pypdfium2]  WARNING: The script pypdfium2 is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  4/20 [pypdfium2]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  5/20 [Pillow]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  5/20 [Pillow]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  6/20 [opencv-python-headless]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  7/20 [nodeenv]  WARNING: The script nodeenv is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  7/20 [nodeenv]  WARNING: The script markdown2 is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The script identify-cli is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  WARNING: The script ftfy is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10/20 [ftfy]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10/20 [ftfy]  WARNING: The script pre-commit is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]  WARNING: The script markdownify is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
  Attempting uninstall: openai
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]    Found existing installation: openai 2.7.1
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]    Uninstalling openai-2.7.1:
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]      Successfully uninstalled openai-2.7.1
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 14/20 [markdownify]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]  WARNING: The script openai is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15/20 [openai]   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 16/20 [anthropic]  WARNING: The script pdftext is installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ‚îÅ‚îÅ 18/20 [surya-ocr]  WARNING: The scripts surya_detect, surya_gui, surya_latex_ocr, surya_layout, surya_ocr, surya_table and texify_gui are installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∫‚îÅ 19/20 [marker-pdf]  WARNING: The scripts marker, marker_chunk_convert, marker_extract, marker_gui, marker_server and marker_single are installed in '/app/data/.local/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 [marker-pdf]
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
reward-kit 0.4.1 requires openai==1.78.1, but you have openai 1.109.1 which is incompatible.
claudette 0.3.10 requires anthropic&gt;=0.52, but you have anthropic 0.46.0 which is incompatible.
Successfully installed Pillow-10.4.0 anthropic-0.46.0 cfgv-3.4.0 distlib-0.4.0 einops-0.8.1 ftfy-6.3.1 identify-2.6.15 markdown2-2.5.4 markdownify-1.2.2 marker-pdf-1.10.1 nodeenv-1.9.1 openai-1.109.1 opencv-python-headless-4.11.0.86 pdftext-0.6.3 pre-commit-4.4.0 pypdfium2-4.30.0 rapidfuzz-3.14.3 regex-2024.11.6 surya-ocr-0.17.0 virtualenv-20.35.4</code></pre>
</div>
</div>
<p>Is it possible to use python equivalent instead of cli command of marker_single</p>
</section>
<section id="reply-66" class="level5">
<h5 class="anchored" data-anchor-id="reply-66">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes! From the README I read earlier, you can use Marker directly in Python. Here‚Äôs the basic approach:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> marker.converters.pdf <span class="im">import</span> PdfConverter</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> marker.models <span class="im">import</span> create_model_dict</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>converter <span class="op">=</span> PdfConverter(</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>    artifact_dict<span class="op">=</span>create_model_dict(),</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>rendered <span class="op">=</span> converter(<span class="st">"wattbot_data/strubell2019.pdf"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>For chunks specifically, you‚Äôd need to specify the output format. Would you like to try this and see what properties the <code>rendered</code> object has?</p>
<div id="aaf707ca" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> marker.converters.pdf <span class="im">import</span> PdfConverter</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> marker.models <span class="im">import</span> create_model_dict</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm</code></pre>
</div>
</div>
<div id="4ccc185f" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a>converter <span class="op">=</span> PdfConverter(</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>    artifact_dict<span class="op">=</span>create_model_dict(),</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>rendered <span class="op">=</span> converter(<span class="st">"wattbot_data/strubell2019.pdf"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="37520798" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(rendered)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>lets try fitz, i want to know if we can give chunk size and overlap</p>
</section>
<section id="reply-67" class="level5">
<h5 class="anchored" data-anchor-id="reply-67">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>PyMuPDF (fitz) is primarily a PDF rendering library - it doesn‚Äôt have built-in chunking with overlap like some text processing libraries. However, you can:</p>
<ol type="1">
<li>Extract text from the PDF with fitz</li>
<li>Manually implement chunking with overlap on the extracted text</li>
</ol>
<p>Would you like to try extracting text with fitz first to see what you get, or would you prefer to continue exploring Marker‚Äôs chunking capabilities since it‚Äôs already designed for this use case?</p>
<p>lets try with fitz because marker is difficult to run, takes lot of resource. we can try marker if fitz does not workout</p>
</section>
<section id="reply-68" class="level5">
<h5 class="anchored" data-anchor-id="reply-68">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good call - fitz is much lighter weight. Let‚Äôs start by extracting text from the strubell2019 PDF with fitz:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fitz</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> fitz.<span class="bu">open</span>(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then you can extract text page by page or all at once. Would you like to try extracting from one page first to see what the output looks like?</p>
<div id="f4c650b7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fitz</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> fitz.<span class="bu">open</span>(data_path<span class="op">/</span><span class="st">'strubell2019.pdf'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>sure</p>
</section>
<section id="reply-69" class="level5">
<h5 class="anchored" data-anchor-id="reply-69">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Try extracting text from the first page:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>page <span class="op">=</span> doc[<span class="dv">0</span>]</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> page.get_text()</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will show you what fitz extracts. Take a look at the output and see how it compares to what you got with <code>rd.read_pdf</code>.</p>
<div id="03b99595" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>page <span class="op">=</span> doc[<span class="dv">0</span>]</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> page.get_text()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="3a3f9328" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>arXiv:1906.02243v1  [cs.CL]  5 Jun 2019
Energy and Policy Considerations for Deep Learning in NLP
Emma Strubell
Ananya Ganesh
Andrew McCallum
College of Information and Computer Sciences
University of Massachusetts Amherst
{strubell, aganesh, mccallum}@cs.umass.edu
Abstract
Recent progress in hardware and methodol-
ogy for training neural networks has ushered
in a new generation of large networks trained
on abundant data.
These models have ob-
tained notable gains in accuracy across many
NLP tasks. However, these accuracy improve-
ments depend on the availability of exception-
ally large computational resources that neces-
sitate similarly substantial energy consump-
tion. As a result these models are costly to
train and develop, both Ô¨Ånancially, due to the
cost of hardware and electricity or cloud com-
pute time, and environmentally, due to the car-
bon footprint required to fuel modern tensor
processing hardware. In this paper we bring
this issue to the attention of NLP researchers
by quantifying the approximate Ô¨Ånancial and
environmental costs of training a variety of re-
cently successful neural network models for
NLP. Based on these Ô¨Åndings, we propose ac-
tionable recommendations to reduce costs and
improve equity in NLP research and practice.
1
Introduction
Advances in techniques and hardware for train-
ing deep neural networks have recently en-
abled impressive accuracy improvements across
many fundamental NLP tasks (Bahdanau et al.,
2015; Luong et al., 2015; Dozat and Man-
ning, 2017; Vaswani et al., 2017), with the
most computationally-hungry models obtaining
the highest scores (Peters et al., 2018; Devlin et al.,
2019; Radford et al., 2019; So et al., 2019). As
a result, training a state-of-the-art model now re-
quires substantial computational resources which
demand considerable energy, along with the as-
sociated Ô¨Ånancial and environmental costs.
Re-
search and development of new models multiplies
these costs by thousands of times by requiring re-
training to experiment with model architectures
and hyperparameters. Whereas a decade ago most
Consumption
CO2e (lbs)
Air travel, 1 passenger, NY‚ÜîSF
1984
Human life, avg, 1 year
11,023
American life, avg, 1 year
36,156
Car, avg incl. fuel, 1 lifetime
126,000
Training one model (GPU)
NLP pipeline (parsing, SRL)
39
w/ tuning &amp; experimentation
78,468
Transformer (big)
192
w/ neural architecture search
626,155
Table 1: Estimated CO2 emissions from training com-
mon NLP models, compared to familiar consumption.1
NLP models could be trained and developed on
a commodity laptop or server, many now require
multiple instances of specialized hardware such as
GPUs or TPUs, therefore limiting access to these
highly accurate models on the basis of Ô¨Ånances.
Even when these expensive computational re-
sources are available, model training also incurs a
substantial cost to the environment due to the en-
ergy required to power this hardware for weeks or
months at a time. Though some of this energy may
come from renewable or carbon credit-offset re-
sources, the high energy demands of these models
are still a concern since (1) energy is not currently
derived from carbon-neural sources in many loca-
tions, and (2) when renewable energy is available,
it is still limited to the equipment we have to pro-
duce and store it, and energy spent training a neu-
ral network might better be allocated to heating a
family‚Äôs home. It is estimated that we must cut
carbon emissions by half over the next decade to
deter escalating rates of natural disaster, and based
on the estimated CO2 emissions listed in Table 1,
1Sources:
(1) Air travel and per-capita consump-
tion:
https://bit.ly/2Hw0xWc;
(2) car lifetime:
https://bit.ly/2Qbr0w1.
</code></pre>
</div>
</div>
<p>The table information is extracted but how to know rows and column</p>
</section>
<section id="reply-70" class="level5">
<h5 class="anchored" data-anchor-id="reply-70">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good observation! Fitz extracts the table as plain text without preserving the row/column structure. To get structured table data, you‚Äôd need to:</p>
<ol type="1">
<li><strong>Use fitz‚Äôs table detection</strong>: <code>page.find_tables()</code> - this can identify tables and extract them as structured data</li>
<li><strong>Parse the text manually</strong> - look for patterns in spacing/alignment</li>
</ol>
<p>Would you like to try <code>page.find_tables()</code> to see if it can detect the table structure on this page?</p>
<div id="b9d79176" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>tables <span class="op">=</span> page.find_tables()<span class="op">;</span> tables</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;pymupdf.table.TableFinder&gt;</code></pre>
</div>
</div>
<p>what is next?</p>
</section>
<section id="reply-71" class="level5">
<h5 class="anchored" data-anchor-id="reply-71">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>The <code>tables</code> object is a TableFinder. To see the actual tables, try:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a>tables.tables</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This will show you a list of detected tables. If there are any, you can then extract them as structured data.</p>
<div id="a13c82cb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>tables.tables</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>[]</code></pre>
</div>
</div>
<p>it is empty</p>
</section>
<section id="reply-72" class="level5">
<h5 class="anchored" data-anchor-id="reply-72">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Fitz‚Äôs table detection didn‚Äôt find the table on that page - probably because it‚Äôs formatted as plain text without borders or clear structure.</p>
<p>You have a few options:</p>
<ol type="1">
<li><strong>Try adjusting fitz‚Äôs table detection settings</strong> - it has parameters you can tune</li>
<li><strong>Extract tables as images and use OCR</strong> - fitz can extract page regions as images</li>
<li><strong>Go back to Marker</strong> - it uses ML models specifically trained for table detection</li>
<li><strong>Use a dedicated table extraction library</strong> like camelot or tabula</li>
</ol>
<p>Since fitz isn‚Äôt detecting the tables automatically, would you like to try one of these alternatives, or would you prefer to work with the text as-is and implement your own chunking strategy?</p>
<p>using marker will return markdown. Do you know how to chunk a markdow?</p>
</section>
<section id="reply-73" class="level5">
<h5 class="anchored" data-anchor-id="reply-73">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Yes! Markdown can be chunked in several ways:</p>
<ol type="1">
<li><strong>By headers</strong> - Split on <code>#</code>, <code>##</code>, <code>###</code> etc. to keep related sections together</li>
<li><strong>By paragraphs</strong> - Split on <code>\n\n</code> (double newlines)</li>
<li><strong>By semantic units</strong> - Use a library like <code>langchain</code> or <code>llama-index</code> that has markdown-aware chunkers</li>
<li><strong>Fixed size with overlap</strong> - Just like regular text, but preserve markdown formatting</li>
</ol>
<p>For tables specifically, Marker converts them to markdown table format, which you could: - Keep as complete units (don‚Äôt split tables across chunks) - Parse into structured data</p>
<p>Would you like to try getting Marker‚Äôs markdown output first and then explore how to chunk it effectively?</p>
<p>I have got access to datalab now. I want to use their api for the markdown extraction</p>
</section>
<section id="reply-74" class="level5">
<h5 class="anchored" data-anchor-id="reply-74">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†</p>
<p>Great! Datalab is the hosted API for Marker. Would you like me to look up the Datalab API documentation to see how to use it for markdown extraction?</p>
<p>Yes</p>
</section>
<section id="reply-75" class="level5">
<h5 class="anchored" data-anchor-id="reply-75">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! I found the Datalab API documentation. Here‚Äôs the key info:</p>
<p><a href="https://documentation.datalab.to/docs/welcome/api" title="API Overview - Datalab Documentation">*</a> The marker endpoint is at <code>/api/v1/marker</code>, and you can submit requests with parameters like <code>output_format</code> (set to ‚Äòmarkdown‚Äô), <code>force_ocr</code>, <code>use_llm</code>, etc.</p>
<p>Basic Python example:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://www.datalab.to/api/v1/marker"</span></span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>form_data <span class="op">=</span> {</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'file'</span>: (<span class="st">'test.pdf'</span>, <span class="bu">open</span>(<span class="st">'path/to/file.pdf'</span>, <span class="st">'rb'</span>), <span class="st">'application/pdf'</span>),</span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'output_format'</span>: (<span class="va">None</span>, <span class="st">'markdown'</span>),  <span class="co"># or 'chunks'</span></span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>headers <span class="op">=</span> {<span class="st">"X-Api-Key"</span>: <span class="st">"YOUR_API_KEY"</span>}</span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.post(url, files<span class="op">=</span>form_data, headers<span class="op">=</span>headers)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><a href="https://documentation.datalab.to/docs/welcome/api" title="API Overview - Datalab Documentation">*</a> This returns a <code>request_id</code> that you use to retrieve the result later.</p>
<p>Would you like to try calling the API with one of your PDFs?</p>
<p>I have the following code #| export class HybridSearch: def <strong>init</strong>(self, all_chunks, dense_embds, tokenized_corpus): fc.store_attr() self.bm25 = BM25Okapi(self.tokenized_corpus)</p>
<pre><code>def search(self, query, limit=5):
    query_emb = embedder.create_dembs(texts=[query])[0]
    tokenized_query = query.lower().split()
    results = self.bm25.get_top_n(tokenized_query, self.all_chunks, n=limit)
    scores = cosine_similarity([query_emb], self.dense_embds)[0]
    # print(len(scores), scores[0], len(scores[0]))
    idx = np.argsort(scores)[-limit:][::-1]
    results.extend(self.all_chunks[idx])
    return results</code></pre>
<p>I want to use qwen reranker, also I want to include the preceding and next chunk for each chunk in the results</p>
</section>
<section id="reply-76" class="level5">
<h5 class="anchored" data-anchor-id="reply-76">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Good improvements! Let me break this down into steps:</p>
<p><strong>For reranking with Qwen:</strong> You‚Äôll need to combine the BM25 and dense results first, then pass them to the reranker to get the final top-k.</p>
<p><strong>For including preceding/next chunks:</strong> You‚Äôll need to track chunk indices so you can grab adjacent chunks.</p>
<p>Which would you like to tackle first - adding the reranker or implementing the context window (prev/next chunks)?</p>
<p>Including the prev and next chunks</p>
</section>
<section id="reply-77" class="level5">
<h5 class="anchored" data-anchor-id="reply-77">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†</p>
<p>Good choice! To include prev/next chunks, you‚Äôll need to:</p>
<ol type="1">
<li>Track chunk indices in your results</li>
<li>Fetch adjacent chunks based on those indices</li>
<li>Handle edge cases (first/last chunks in a document)</li>
</ol>
<p>Here‚Äôs a helper method you could add to your <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#hybridsearch"><code>HybridSearch</code></a> class:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_context_chunks(<span class="va">self</span>, chunk_indices, window<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get chunks with surrounding context"""</span></span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a>    result_chunks <span class="op">=</span> []</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a>    seen <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> chunk_indices:</span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, idx <span class="op">-</span> window)</span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(<span class="va">self</span>.all_chunks), idx <span class="op">+</span> window <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb147-9"><a href="#cb147-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb147-10"><a href="#cb147-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(start, end):</span>
<span id="cb147-11"><a href="#cb147-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="kw">not</span> <span class="kw">in</span> seen:</span>
<span id="cb147-12"><a href="#cb147-12" aria-hidden="true" tabindex="-1"></a>                result_chunks.append((i, <span class="va">self</span>.all_chunks[i]))</span>
<span id="cb147-13"><a href="#cb147-13" aria-hidden="true" tabindex="-1"></a>                seen.add(i)</span>
<span id="cb147-14"><a href="#cb147-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb147-15"><a href="#cb147-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [chunk <span class="cf">for</span> _, chunk <span class="kw">in</span> <span class="bu">sorted</span>(result_chunks)]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>However, there‚Äôs one issue - you probably only want to include adjacent chunks from the <strong>same document</strong>. Should we add a check for <code>doc_id</code> to prevent mixing chunks from different documents?</p>
<p>Yes</p>
</section>
<section id="reply-78" class="level5">
<h5 class="anchored" data-anchor-id="reply-78">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Perfect! Here‚Äôs an updated version that respects document boundaries:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_context_chunks(<span class="va">self</span>, chunk_indices, window<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get chunks with surrounding context, respecting document boundaries"""</span></span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>    result_chunks <span class="op">=</span> []</span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a>    seen <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx <span class="kw">in</span> chunk_indices:</span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a>        current_doc_id <span class="op">=</span> <span class="va">self</span>.all_chunks[idx].doc_id</span>
<span id="cb148-8"><a href="#cb148-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb148-9"><a href="#cb148-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find adjacent chunks in same document</span></span>
<span id="cb148-10"><a href="#cb148-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> offset <span class="kw">in</span> <span class="bu">range</span>(<span class="op">-</span>window, window <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb148-11"><a href="#cb148-11" aria-hidden="true" tabindex="-1"></a>            new_idx <span class="op">=</span> idx <span class="op">+</span> offset</span>
<span id="cb148-12"><a href="#cb148-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb148-13"><a href="#cb148-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Check bounds and same document</span></span>
<span id="cb148-14"><a href="#cb148-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;=</span> new_idx <span class="op">&lt;</span> <span class="bu">len</span>(<span class="va">self</span>.all_chunks) <span class="kw">and</span> </span>
<span id="cb148-15"><a href="#cb148-15" aria-hidden="true" tabindex="-1"></a>                new_idx <span class="kw">not</span> <span class="kw">in</span> seen <span class="kw">and</span></span>
<span id="cb148-16"><a href="#cb148-16" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.all_chunks[new_idx].doc_id <span class="op">==</span> current_doc_id):</span>
<span id="cb148-17"><a href="#cb148-17" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb148-18"><a href="#cb148-18" aria-hidden="true" tabindex="-1"></a>                result_chunks.append((new_idx, <span class="va">self</span>.all_chunks[new_idx]))</span>
<span id="cb148-19"><a href="#cb148-19" aria-hidden="true" tabindex="-1"></a>                seen.add(new_idx)</span>
<span id="cb148-20"><a href="#cb148-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb148-21"><a href="#cb148-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [chunk <span class="cf">for</span> _, chunk <span class="kw">in</span> <span class="bu">sorted</span>(result_chunks)]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then update your <code>search</code> method to use it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> search(<span class="va">self</span>, query, limit<span class="op">=</span><span class="dv">5</span>, window<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... your existing search logic ...</span></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine and get top indices</span></span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>    combined_indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(bm25_indices <span class="op">+</span> dense_indices))[:limit]</span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get chunks with context</span></span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.get_context_chunks(combined_indices, window<span class="op">=</span>window)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Would you like to integrate this into your current code?</p>
<p>Please follow fastai coding style</p>
</section>
<section id="reply-79" class="level5">
<h5 class="anchored" data-anchor-id="reply-79">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†</p>
<p>Here‚Äôs the fastai-style version:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb150"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_context_chunks(<span class="va">self</span>, idxs, window<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get chunks with surrounding context, respecting doc boundaries"""</span></span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>    seen, res <span class="op">=</span> <span class="bu">set</span>(), []</span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> idxs:</span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>        doc_id <span class="op">=</span> <span class="va">self</span>.all_chunks[i].doc_id</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">max</span>(<span class="dv">0</span>, i<span class="op">-</span>window), <span class="bu">min</span>(<span class="bu">len</span>(<span class="va">self</span>.all_chunks), i<span class="op">+</span>window<span class="op">+</span><span class="dv">1</span>)):</span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> j <span class="kw">not</span> <span class="kw">in</span> seen <span class="kw">and</span> <span class="va">self</span>.all_chunks[j].doc_id <span class="op">==</span> doc_id:</span>
<span id="cb150-8"><a href="#cb150-8" aria-hidden="true" tabindex="-1"></a>                res.append((j, <span class="va">self</span>.all_chunks[j]))</span>
<span id="cb150-9"><a href="#cb150-9" aria-hidden="true" tabindex="-1"></a>                seen.add(j)</span>
<span id="cb150-10"><a href="#cb150-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fc.L(<span class="bu">sorted</span>(res)).itemgot(<span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>And the updated <code>search</code> method:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> search(<span class="va">self</span>, query, limit<span class="op">=</span><span class="dv">5</span>, window<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a>    q_emb <span class="op">=</span> embedder.create_dembs(texts<span class="op">=</span>[query])[<span class="dv">0</span>]</span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a>    tok_q <span class="op">=</span> query.lower().split()</span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb151-5"><a href="#cb151-5" aria-hidden="true" tabindex="-1"></a>    bm25_res <span class="op">=</span> <span class="va">self</span>.bm25.get_top_n(tok_q, <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.all_chunks)), n<span class="op">=</span>limit)</span>
<span id="cb151-6"><a href="#cb151-6" aria-hidden="true" tabindex="-1"></a>    dense_scores <span class="op">=</span> cosine_similarity([q_emb], <span class="va">self</span>.dense_embds)[<span class="dv">0</span>]</span>
<span id="cb151-7"><a href="#cb151-7" aria-hidden="true" tabindex="-1"></a>    dense_idxs <span class="op">=</span> np.argsort(dense_scores)[<span class="op">-</span>limit:][::<span class="op">-</span><span class="dv">1</span>].tolist()</span>
<span id="cb151-8"><a href="#cb151-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb151-9"><a href="#cb151-9" aria-hidden="true" tabindex="-1"></a>    idxs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(bm25_res <span class="op">+</span> dense_idxs))[:limit]</span>
<span id="cb151-10"><a href="#cb151-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.get_context_chunks(idxs, window<span class="op">=</span>window)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Note: I changed the BM25 call to return indices instead of chunks directly so we can use them for context lookup. Would you like to try this?</p>
<p>Can you please make the get_context_chunks simpler with fixed window of 1?</p>
</section>
<section id="reply-80" class="level5">
<h5 class="anchored" data-anchor-id="reply-80">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Sure! Here‚Äôs a simpler version with <code>window=1</code> fixed:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb152"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_context_chunks(<span class="va">self</span>, idxs):</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get chunks with prev/next context, respecting doc boundaries"""</span></span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>    seen, res <span class="op">=</span> <span class="bu">set</span>(), []</span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> idxs:</span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a>        doc_id <span class="op">=</span> <span class="va">self</span>.all_chunks[i].doc_id</span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> [i<span class="op">-</span><span class="dv">1</span>, i, i<span class="op">+</span><span class="dv">1</span>]:</span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;=</span> j <span class="op">&lt;</span> <span class="bu">len</span>(<span class="va">self</span>.all_chunks) <span class="kw">and</span> j <span class="kw">not</span> <span class="kw">in</span> seen <span class="kw">and</span> <span class="va">self</span>.all_chunks[j].doc_id <span class="op">==</span> doc_id:</span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a>                res.append((j, <span class="va">self</span>.all_chunks[j]))</span>
<span id="cb152-9"><a href="#cb152-9" aria-hidden="true" tabindex="-1"></a>                seen.add(j)</span>
<span id="cb152-10"><a href="#cb152-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fc.L(<span class="bu">sorted</span>(res)).itemgot(<span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Much cleaner! This checks only the immediate prev/current/next chunks for each result.</p>
<p>now lets use the qwen reranker</p>
</section>
<section id="reply-81" class="level5">
<h5 class="anchored" data-anchor-id="reply-81">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>Great! For Qwen reranker, you‚Äôll need to:</p>
<ol type="1">
<li>Install the model (if using locally) or use an API</li>
<li>Get initial candidates from BM25 + dense search (more than <code>limit</code>, e.g., <code>limit * 2</code>)</li>
<li>Pass candidates to reranker with the query</li>
<li>Return top <code>limit</code> reranked results</li>
</ol>
<p>Are you planning to use: - <strong>Local Qwen reranker</strong> (via transformers/sentence-transformers)? - <strong>API-based</strong> (like through Fireworks or another provider)?</p>
<p>Which approach would you prefer?</p>
<p>Yes</p>
</section>
<section id="reply-82" class="level5">
<h5 class="anchored" data-anchor-id="reply-82">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>üß†üß†üß†üß†üß†üß†üß†üß†</p>
<p>Found it! <a href="https://docs.fireworks.ai/guides/querying-embeddings-models" title="Embeddings &amp; Reranking - Fireworks AI Docs">*</a> The Fireworks reranking API uses the endpoint <code>https://api.fireworks.ai/inference/v1/rerank</code> with model <code>fireworks/qwen3-reranker-8b</code>.</p>
<p>Here‚Äôs how to add the reranker to your <a href="https://anubhavmaity.github.io/wattbot/xx_sdk_markdown.html#hybridsearch"><code>HybridSearch</code></a> class:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rerank(<span class="va">self</span>, query, chunks, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Rerank chunks using Fireworks Qwen3 reranker"""</span></span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a>    docs <span class="op">=</span> [c.text <span class="cf">for</span> c <span class="kw">in</span> chunks]</span>
<span id="cb153-6"><a href="#cb153-6" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> requests.post(</span>
<span id="cb153-7"><a href="#cb153-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"https://api.fireworks.ai/inference/v1/rerank"</span>,</span>
<span id="cb153-8"><a href="#cb153-8" aria-hidden="true" tabindex="-1"></a>        json<span class="op">=</span>{</span>
<span id="cb153-9"><a href="#cb153-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model"</span>: <span class="st">"fireworks/qwen3-reranker-8b"</span>,</span>
<span id="cb153-10"><a href="#cb153-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"query"</span>: query,</span>
<span id="cb153-11"><a href="#cb153-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">"documents"</span>: docs,</span>
<span id="cb153-12"><a href="#cb153-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"top_n"</span>: limit,</span>
<span id="cb153-13"><a href="#cb153-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"return_documents"</span>: <span class="va">True</span></span>
<span id="cb153-14"><a href="#cb153-14" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb153-15"><a href="#cb153-15" aria-hidden="true" tabindex="-1"></a>        headers<span class="op">=</span>{</span>
<span id="cb153-16"><a href="#cb153-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Authorization"</span>: <span class="ss">f"Bearer </span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">'FIREWORKS_API_KEY'</span>]<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb153-17"><a href="#cb153-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Content-Type"</span>: <span class="st">"application/json"</span></span>
<span id="cb153-18"><a href="#cb153-18" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb153-19"><a href="#cb153-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb153-20"><a href="#cb153-20" aria-hidden="true" tabindex="-1"></a>    ranked_idxs <span class="op">=</span> [r[<span class="st">'index'</span>] <span class="cf">for</span> r <span class="kw">in</span> res.json()[<span class="st">'results'</span>]]</span>
<span id="cb153-21"><a href="#cb153-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fc.L([chunks[i] <span class="cf">for</span> i <span class="kw">in</span> ranked_idxs])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Then update your <code>search</code> method to use it:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> search(<span class="va">self</span>, query, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get more candidates than needed</span></span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a>    candidates <span class="op">=</span> <span class="va">self</span>._get_candidates(query, limit<span class="op">=</span>limit<span class="op">*</span><span class="dv">3</span>)</span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rerank and return top results</span></span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.rerank(query, candidates, limit<span class="op">=</span>limit)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Would you like to integrate this into your class?</p>
<p>You can also use the openai spec instead of the request library</p>
</section>
<section id="reply-83" class="level5">
<h5 class="anchored" data-anchor-id="reply-83">ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c --></h5>
<p>üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†</p>
<p>Good point! Here‚Äôs the simpler version using the OpenAI client:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rerank(<span class="va">self</span>, query, chunks, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Rerank chunks using Fireworks Qwen3 reranker"""</span></span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>    docs <span class="op">=</span> [c.text <span class="cf">for</span> c <span class="kw">in</span> chunks]</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> openai_cli.post(</span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"/rerank"</span>,</span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a>        body<span class="op">=</span>{</span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model"</span>: <span class="st">"fireworks/qwen3-reranker-8b"</span>,</span>
<span id="cb155-8"><a href="#cb155-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"query"</span>: query,</span>
<span id="cb155-9"><a href="#cb155-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"documents"</span>: docs,</span>
<span id="cb155-10"><a href="#cb155-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"top_n"</span>: limit</span>
<span id="cb155-11"><a href="#cb155-11" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb155-12"><a href="#cb155-12" aria-hidden="true" tabindex="-1"></a>        cast_to<span class="op">=</span><span class="bu">dict</span></span>
<span id="cb155-13"><a href="#cb155-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb155-14"><a href="#cb155-14" aria-hidden="true" tabindex="-1"></a>    ranked_idxs <span class="op">=</span> [r[<span class="st">'index'</span>] <span class="cf">for</span> r <span class="kw">in</span> res[<span class="st">'results'</span>]]</span>
<span id="cb155-15"><a href="#cb155-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fc.L([chunks[i] <span class="cf">for</span> i <span class="kw">in</span> ranked_idxs])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Note: If the <code>.post()</code> method doesn‚Äôt work directly, you might need to use:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> httpx</span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rerank(<span class="va">self</span>, query, chunks, limit<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a>    docs <span class="op">=</span> [c.text <span class="cf">for</span> c <span class="kw">in</span> chunks]</span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> httpx.post(</span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"</span><span class="sc">{</span>openai_cli<span class="sc">.</span>base_url<span class="sc">}</span><span class="ss">rerank"</span>,</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a>        json<span class="op">=</span>{<span class="st">"model"</span>: <span class="st">"fireworks/qwen3-reranker-8b"</span>, <span class="st">"query"</span>: query, <span class="st">"documents"</span>: docs, <span class="st">"top_n"</span>: limit},</span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a>        headers<span class="op">=</span>{<span class="st">"Authorization"</span>: <span class="ss">f"Bearer </span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">'FIREWORKS_API_KEY'</span>]<span class="sc">}</span><span class="ss">"</span>}</span>
<span id="cb156-9"><a href="#cb156-9" aria-hidden="true" tabindex="-1"></a>    ).json()</span>
<span id="cb156-10"><a href="#cb156-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fc.L([chunks[r[<span class="st">'index'</span>]] <span class="cf">for</span> r <span class="kw">in</span> res[<span class="st">'results'</span>]])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Would you like to try one of these approaches?</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/anubhavmaity\.github\.io\/wattbot");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/anubhavmaity/wattbot/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>
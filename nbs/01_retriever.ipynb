{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c67b899c-aba0-4a9d-a8bd-efb16a5b9348",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f024657-be03-434d-b2fd-7c65f40dfd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be0b33-a018-4fb4-970b-e79ea0a363f1",
   "metadata": {},
   "source": [
    "# Retriever\n",
    "In this notebook we will extract content from pdf, chunk the content, index the chunks and retrieve chunk given the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535213b-062a-4f34-ac89-b873064461fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86386bb-3768-4bdb-851d-beb929196850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import toolslm as tlm\n",
    "import os\n",
    "import openai, numpy as np\n",
    "from wattbot import eda, utils\n",
    "import fastcore.all as fc\n",
    "import contextkit.read as rd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_text_splitters import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd064f33-0497-4041-b8c6-67ffb20706eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bbfdf8-a0a1-426e-bb55-eeed623ddb06",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Loading the files\n",
    "- metadata\n",
    "- train\n",
    "- test\n",
    "  \n",
    "and viewing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cd115-0e78-4a8e-b03d-aea780e12be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>citation</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon2023</td>\n",
       "      <td>report</td>\n",
       "      <td>2023 Amazon Sustainability Report</td>\n",
       "      <td>2023</td>\n",
       "      <td>Amazon Staff. (2023). Amazon Sustainability Re...</td>\n",
       "      <td>https://sustainability.aboutamazon.com/2023-am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chen2024</td>\n",
       "      <td>paper</td>\n",
       "      <td>Efficient Heterogeneous Large Language Model D...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...</td>\n",
       "      <td>https://arxiv.org/pdf/2405.01814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chung2025</td>\n",
       "      <td>paper</td>\n",
       "      <td>The ML.ENERGY Benchmark: Toward Automated Infe...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Jae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...</td>\n",
       "      <td>https://arxiv.org/pdf/2505.06371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cottier2024</td>\n",
       "      <td>paper</td>\n",
       "      <td>The Rising Costs of Training Frontier AI Models</td>\n",
       "      <td>2024</td>\n",
       "      <td>Ben Cottier, Robi Rahman, Loredana Fattorini, ...</td>\n",
       "      <td>https://arxiv.org/pdf/2405.21015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dodge2022</td>\n",
       "      <td>paper</td>\n",
       "      <td>Measuring the Carbon Intensity of AI in Cloud ...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Jesse Dodge, Taylor Prewitt, Remi Tachet Des C...</td>\n",
       "      <td>https://arxiv.org/pdf/2206.05229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    type                                              title  \\\n",
       "0   amazon2023  report                  2023 Amazon Sustainability Report   \n",
       "1     chen2024   paper  Efficient Heterogeneous Large Language Model D...   \n",
       "2    chung2025   paper  The ML.ENERGY Benchmark: Toward Automated Infe...   \n",
       "3  cottier2024   paper    The Rising Costs of Training Frontier AI Models   \n",
       "4    dodge2022   paper  Measuring the Carbon Intensity of AI in Cloud ...   \n",
       "\n",
       "   year                                           citation  \\\n",
       "0  2023  Amazon Staff. (2023). Amazon Sustainability Re...   \n",
       "1  2024  Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingx...   \n",
       "2  2025  Jae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan...   \n",
       "3  2024  Ben Cottier, Robi Rahman, Loredana Fattorini, ...   \n",
       "4  2022  Jesse Dodge, Taylor Prewitt, Remi Tachet Des C...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://sustainability.aboutamazon.com/2023-am...  \n",
       "1                   https://arxiv.org/pdf/2405.01814  \n",
       "2                   https://arxiv.org/pdf/2505.06371  \n",
       "3                   https://arxiv.org/pdf/2405.21015  \n",
       "4                   https://arxiv.org/pdf/2206.05229  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = eda.metadata()\n",
    "md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3878c-9cae-477c-a6e1-10e80cca8e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The ML.ENERGY Benchmark</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['chung2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2505.06371']</td>\n",
       "      <td>We present the ML.ENERGY Benchmark, a benchmar...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>4.3 tCO2e</td>\n",
       "      <td>4.3</td>\n",
       "      <td>tCO2e</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>\"Training GShard-600B used 24 MWh and produced...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>64.7 GB</td>\n",
       "      <td>64.7</td>\n",
       "      <td>GB</td>\n",
       "      <td>['chen2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2405.01814']</td>\n",
       "      <td>Table 3: Large language models used for evalua...</td>\n",
       "      <td>Table 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>MWh</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['wu2021b','patterson2021']</td>\n",
       "      <td>['https://arxiv.org/abs/2108.06738','https://a...</td>\n",
       "      <td>Wu 2021, body text near Fig. 1: \"…between trad...</td>\n",
       "      <td>The &gt;40% statement is explicit in Wu. Patterso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  q003  What is the name of the benchmark suite presen...   \n",
       "1  q009  What were the net CO2e emissions from training...   \n",
       "2  q054  What is the model size in gigabytes (GB) for t...   \n",
       "3  q062  What was the total electricity consumption of ...   \n",
       "4  q075  True or False: Hyperscale data centers in 2020...   \n",
       "\n",
       "                                              answer         answer_value  \\\n",
       "0                            The ML.ENERGY Benchmark  ML.ENERGY Benchmark   \n",
       "1                                          4.3 tCO2e                  4.3   \n",
       "2                                            64.7 GB                 64.7   \n",
       "3  Unable to answer with confidence based on the ...             is_blank   \n",
       "4                                               TRUE                    1   \n",
       "\n",
       "  answer_unit                       ref_id  \\\n",
       "0    is_blank                ['chung2025']   \n",
       "1       tCO2e            ['patterson2021']   \n",
       "2          GB                 ['chen2024']   \n",
       "3         MWh                     is_blank   \n",
       "4    is_blank  ['wu2021b','patterson2021']   \n",
       "\n",
       "                                             ref_url  \\\n",
       "0               ['https://arxiv.org/pdf/2505.06371']   \n",
       "1               ['https://arxiv.org/pdf/2104.10350']   \n",
       "2               ['https://arxiv.org/pdf/2405.01814']   \n",
       "3                                           is_blank   \n",
       "4  ['https://arxiv.org/abs/2108.06738','https://a...   \n",
       "\n",
       "                                supporting_materials  \\\n",
       "0  We present the ML.ENERGY Benchmark, a benchmar...   \n",
       "1  \"Training GShard-600B used 24 MWh and produced...   \n",
       "2  Table 3: Large language models used for evalua...   \n",
       "3                                           is_blank   \n",
       "4  Wu 2021, body text near Fig. 1: \"…between trad...   \n",
       "\n",
       "                                         explanation  \n",
       "0                                              Quote  \n",
       "1                                              Quote  \n",
       "2                                            Table 3  \n",
       "3                                           is_blank  \n",
       "4  The >40% statement is explicit in Wu. Patterso...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = eda.train()\n",
    "qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a205ae-0474-44cb-8407-2a44299f110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q001</td>\n",
       "      <td>What was the average increase in U.S. data cen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>percent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q002</td>\n",
       "      <td>In 2023, what was the estimated amount of cars...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q004</td>\n",
       "      <td>How many data centers did AWS begin using recy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data centers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q005</td>\n",
       "      <td>Since NVIDIA doesn't release the embodied carb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kg/GPU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q006</td>\n",
       "      <td>By what factor was the estimated amortized tra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ratio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  answer  \\\n",
       "0  q001  What was the average increase in U.S. data cen...     NaN   \n",
       "1  q002  In 2023, what was the estimated amount of cars...     NaN   \n",
       "2  q004  How many data centers did AWS begin using recy...     NaN   \n",
       "3  q005  Since NVIDIA doesn't release the embodied carb...     NaN   \n",
       "4  q006  By what factor was the estimated amortized tra...     NaN   \n",
       "\n",
       "   answer_value   answer_unit  ref_id  ref_url  supporting_materials  \\\n",
       "0           NaN       percent     NaN      NaN                   NaN   \n",
       "1           NaN          cars     NaN      NaN                   NaN   \n",
       "2           NaN  data centers     NaN      NaN                   NaN   \n",
       "3           NaN        kg/GPU     NaN      NaN                   NaN   \n",
       "4           NaN         ratio     NaN      NaN                   NaN   \n",
       "\n",
       "   explanation  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = eda.test()\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ff0d2-9710-417c-8c5d-bd42f9e8b6b3",
   "metadata": {},
   "source": [
    "We have to fill up the `answer`, `answer_value`, `answer_unit`, `ref_id`, `ref_url`, `supporting_materials` and `explanation` here.\n",
    "\n",
    "From the competition following values are expected \n",
    "\n",
    "- `answer`: A clear natural-language response (e.g., 1438 lbs, Water consumption, TRUE)'. If no answer is possible, use \"Unable to answer with confidence based on the provided documents.\"\n",
    "  \n",
    "- `answer_value`: The normalized numeric or categorical value (e.g., 1438, Water consumption, 1)\n",
    "  - If no answer is possible, use is_blank\n",
    "  - Ranges should be encoded as [low,high]\n",
    "  - Do not include symbols like <, >, ~ here. Those can be left in the clear natural language column.\n",
    "\n",
    "\n",
    "- `answer_unit`: Unit of measurement (e.g., lbs, kWh, gCO2, projects, is_blank).\n",
    "\n",
    "- `ref_id`: One or more document IDs from metadata.csv that support the answer.\n",
    "\n",
    "- `ref_url`: One or more URL(s) of the cited document(s).\n",
    "\n",
    "- `supporting_materials`: Verbatim justification from the cited document (quote, table reference, figure reference, etc.).\n",
    "\n",
    "- `explanation`: Short reasoning describing why the cited material supports the answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6828259-93cc-4469-8e23-7a043d55fbdc",
   "metadata": {},
   "source": [
    "## Read pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355433c-e981-4dd8-9532-fe7c99cdefe6",
   "metadata": {},
   "source": [
    "I already downloaded all the pdfs, please refer the notebook `00_eda`\n",
    "\n",
    "We will extract the content from the pdfs here using answerdotai's [`contextkit`](https://github.com/AnswerDotAI/ContextKit) library which uses [`pypdf`](https://github.com/py-pdf/pypdf) underneath.\n",
    "\n",
    "pypdf does a decent job of text extraction from pdf but it does not preserve the layouts, table structure and reading order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84952e2-70a2-4889-a13f-d675cc3aa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_metadata(doc_id:str) -> str:\n",
    "    \"\"\"Returns the metadata for a given doc_id\"\"\"\n",
    "    meta = eda.metadata()\n",
    "    return meta[meta['id'] == doc_id].iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c911b-9793-4999-b32d-e659af081729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chen2024',\n",
       " 'type': 'paper',\n",
       " 'title': 'Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n",
       " 'year': 2024,\n",
       " 'citation': 'Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n",
       " 'url': 'https://arxiv.org/pdf/2405.01814'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metadata('chen2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05471732-53c9-4f35-a817-3a14ef085760",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 'chen2024'\n",
    "fc.test_eq(get_metadata(doc_id)['id'], doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d3bb5-2bcd-43e8-b02a-d0d7f784b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def read_doc(doc_id:str) -> str: \n",
    "    \"\"\"Returns the content of the pdf along with its metadata for a given doc_id\"\"\"\n",
    "    meta = get_metadata(doc_id)\n",
    "    content = rd.read_pdf(fc.Path(eda.data_path)/f'{doc_id}.pdf')\n",
    "    return fc.NS(content=content, **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851b795-34b6-49b1-86b5-a6cdf5d8f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan C',\n",
       " 'chen2024')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = read_doc('chen2024')\n",
    "doc.content[:100], doc.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a7cf8-0042-4c2d-b488-77b2865e6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id='chen2024'\n",
    "doc = read_doc(doc_id)\n",
    "fc.test_ne(len(doc.content), 0)\n",
    "fc.test_eq(doc.id, doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d9b96-25fb-4539-8b65-24be7a179962",
   "metadata": {},
   "source": [
    "## Read Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66bfa4-a022-43a4-bc8e-295548c01ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def read_markdown(doc_id:str) -> str:\n",
    "    \"\"\"Returns the markdown content given the doc_id\"\"\"\n",
    "    meta = get_metadata(doc_id)\n",
    "    md = (fc.Path(eda.data_path)/f'markdown/{doc_id}.md').read_text()\n",
    "    return fc.NS(content=tlm.download.clean_md(md), **meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f47ecfa-d654-422b-bdfa-d5354bd3041d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75025, 69175)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_markdown('chen2024').content), len(read_doc('chen2024').content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38578b5c-5999-4567-bafe-30f7f86420f0",
   "metadata": {},
   "source": [
    "## Total content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66986cac-95f8-4374-ac84-f8879042660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2673613"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.L(eda.metadata()['id'].to_list()).map(lambda x: len(read_doc(x).content)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609750e-c7de-4cbf-b1df-1c92a48f4075",
   "metadata": {},
   "source": [
    "I dont think any open source models can handle that many characters in their context window as of November 2025. \n",
    "\n",
    "A RAG based system will be good where we chunk the content, retrieve the relavent chunk and generate answer with those relevant chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9592e34-d6d9-40a0-b18e-542dd97787d2",
   "metadata": {},
   "source": [
    "## Document Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27563f-4bac-4217-a0d8-bdace6f45e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_content_metadata(fn, doc_id):\n",
    "    doc = fn(doc_id)\n",
    "    content = doc.__dict__.pop('content')\n",
    "    return content, doc.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37bbbc4-260b-4ed3-94ed-857725d8b969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan C',\n",
       " {'id': 'chen2024',\n",
       "  'type': 'paper',\n",
       "  'title': 'Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n",
       "  'year': 2024,\n",
       "  'citation': 'Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n",
       "  'url': 'https://arxiv.org/pdf/2405.01814'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content, metadata = get_content_metadata(read_doc, 'chen2024')\n",
    "content[:100], metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a4a59-9bc7-44ee-b4b6-319a78ffb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def chunk_doc(doc_id:str, start_id:int=0, chunk_size:int=1500, step:int=1400) -> list:\n",
    "    \"\"\"Chunks the content of a doc given the doc_id\"\"\"\n",
    "    content, metadata = get_content_metadata(read_doc, doc_id)\n",
    "    def _chunk(x): return fc.NS(text = content[x[-1]: x[-1] + chunk_size],  chunk_id=x[0] + start_id, **metadata)\n",
    "    return fc.L.range(0, len(content), step).enumerate().map(_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5a54b-494c-45a2-8050-47157f047375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69175"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = read_doc('chen2024')\n",
    "len(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bff8a-0e84-4cf4-a30e-07c0b3559a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " ' performance and cost efficiency. Our com-\\nprehensive analysis and experiments confirm the viability\\nof splitting the attention computation over multiple devices.\\nAlso, the communication bandwidth req')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_doc('chen2024')\n",
    "len(chunks), chunks[0]['text'][-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb5749-baec-421d-b004-b9931a195d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 'chen2024'\n",
    "chunks = chunk_doc(doc_id)\n",
    "fc.test_ne(len(chunks), 0)\n",
    "fc.test_eq(chunks[0].id, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33642de1-bd06-49de-9e7c-380f85a3ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan Chen1 Wencong Xiao2 Yutong Lin1 Mingxing Zhang1 Yingdi Shan1 Jinlei Jiang1\\nKang Chen1 Yongwei Wu1\\n1Ts'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]['text'][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9797d60-f846-4103-86b3-d57f44d1c071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text='Efficient Heterogeneous Large Language Model Decoding\\nwith Model-Attention Disaggregation\\nShaoyuan Chen1 Wencong Xiao2 Yutong Lin1 Mingxing Zhang1 Yingdi Shan1 Jinlei Jiang1\\nKang Chen1 Yongwei Wu1\\n1Tsinghua University\\n2ByteDance\\nAbstract\\nTransformer-based large language models (LLMs) exhibit\\nimpressive performance in generative tasks but also intro-\\nduce significant challenges in real-world serving due to in-\\nefficient use of the expensive, computation-optimized accel-\\nerators. Although disaggregated serving architectures have\\nbeen proposed to split different phases of LLM inference, the\\nefficiency of decoding phase is still low. This is caused by\\nthe varying resource demands of different operators in the\\ntransformer-based LLMs. Specifically, the attention operator\\nis memory-intensive, exhibiting a memory access pattern that\\nclashes with the strengths of modern accelerators, especially\\nfor long context requests.\\nTo enhance the efficiency of LLM decoding, we introduce\\nmodel-attention disaggregation. This approach leverages a\\ncollection of cheap, memory-optimized devices for the atten-\\ntion operator while still utilizing high-end accelerators for\\nother parts of the model. This heterogeneous setup ensures\\nthat each component is tailored to its specific workload, max-\\nimizing overall performance and cost efficiency. Our com-\\nprehensive analysis and experiments confirm the viability\\nof splitting the attention computation over multiple devices.\\nAlso, the communication bandwidth req',\n",
       "          chunk_id=0,\n",
       "          id='chen2024',\n",
       "          type='paper',\n",
       "          title='Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n",
       "          year=2024,\n",
       "          citation='Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n",
       "          url='https://arxiv.org/pdf/2405.01814')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd66089-6eb7-417e-95b0-15f93520587a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text='i Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\\nand Luke Zettlemoyer. Opt: Open pre-trained trans-\\nformer language models, 2022.\\n[59] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,\\nYibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-\\nServe: Disaggregating prefill and decoding for goodput-\\noptimized large language model serving. In 18th\\nUSENIX Symposium on Operating Systems Design and\\nImplementation (OSDI 24), pages 193–210, 2024.\\n16',\n",
       "          chunk_id=49,\n",
       "          id='chen2024',\n",
       "          type='paper',\n",
       "          title='Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation',\n",
       "          year=2024,\n",
       "          citation='Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814',\n",
       "          url='https://arxiv.org/pdf/2405.01814')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acad0c0-e9ca-48aa-89c1-bd9a69946bf7",
   "metadata": {},
   "source": [
    "## Markdown Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d66b13-6ac2-4644-aa14-5213f51dbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 375\n",
    "chunk_overlap = 125\n",
    "md_splitter = MarkdownTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad726f-7193-4de7-9fe9-2245db797440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.markdown.MarkdownTextSplitter>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6b6b3-d5f4-4934-84f8-0cd3f12e0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 'chen2024'\n",
    "md_content = read_markdown(doc_id).content\n",
    "chunks = md_splitter.split_text(md_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822511d-16eb-4169-b3cf-eba68a00a358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation\\n\\nShaoyuan Chen<sup>1</sup> Wencong Xiao<sup>2</sup> Yutong Lin<sup>1</sup> Mingxing Zhang<sup>1</sup> Yingdi Shan<sup>1</sup> Jinlei Jiang<sup>1</sup>  \\nKang Chen<sup>1</sup> Yongwei Wu<sup>1</sup>\\n\\n<sup>1</sup>Tsinghua University\\n\\n<sup>2</sup>ByteDance\\n\\n## Abstract\\n\\nTransformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][-1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1073e4-cd32-4f91-af20-8fc2e1992b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.\\n\\nTo enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model. This heterogeneous setup ensures that each component is tailored to its specific workload, maximizing overall performance and cost efficiency. Our comprehensive analysis and experiments confirm the viability of splitting the attention computation over mult'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1][:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c39a3-916b-4b9b-bf79-166d322705f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def chunk_markdown(self:MarkdownTextSplitter, doc_id:str, start_id:int=0):\n",
    "    \"\"\"Chunks the markdown of a doc given the doc_id\"\"\"\n",
    "    content, metadata = get_content_metadata(read_markdown, doc_id)\n",
    "    chunks = fc.L(self.split_text(content))\n",
    "    return chunks.enumerate().map(lambda x: fc.NS(text = x[1],  chunk_id=x[0] + start_id, **metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48a573-adf0-4eee-85f3-ba7d1a7dd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = md_splitter.chunk_markdown(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66ba97-93a8-49de-9414-6739d2e63a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p> Yutong Lin<sup>1</sup> Mingxing Zhang<sup>1</sup> Yingdi Shan<sup>1</sup> Jinlei Jiang<sup>1</sup>  \\nKang Chen<sup>1</sup> Yongwei Wu<sup>1</sup>\\n\\n<sup>1</sup>Tsinghua University\\n\\n<sup>2</sup>ByteDance\\n\\n## Abstract\\n\\nTransformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].text[-900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f82648-a4c3-4857-ac64-53e698820819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests.\\n\\nTo enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1].text[:900]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6841b2-c6cd-40ab-a0f5-cd8eb3d22385",
   "metadata": {},
   "source": [
    "## Human readable Chunk\n",
    "\n",
    "This will help later in creating context for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e21fd-7810-4582-b61f-a735d4fade3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def Nugget(chunk:object, chunk_no:int=0) -> str:\n",
    "    \"\"\"Returns the chunk in a readable format\"\"\"\n",
    "    return f\"\"\"### Chunk {chunk_no}\n",
    "            Text: {chunk['text']}\n",
    "            Chunk Id: {chunk['chunk_id']}\n",
    "            Doc ID: {chunk['id']}\n",
    "            Type: {chunk['type']}\n",
    "            Title: {chunk['title']}\n",
    "            Year: {chunk['year']}\n",
    "            Citation: {chunk['citation']}\n",
    "            URL: {chunk['url']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4833b-c681-4564-8454-b534a75f0429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Chunk 0\n",
      "            Text: ![](_page_8_Figure_0.jpeg)\n",
      "\n",
      "(a) Request-level partition.\n",
      "\n",
      "(b) Head-level partition.\n",
      "\n",
      "Figure 9: Work partition methods of the attention operator.\n",
      "\n",
      "store the KV caches and compute the attention operators. As depicted in Figure 9, the attention operators can be parallelized among memory devices in various ways. One method is to distribute different requests across different devices; an alternative strategy is to partition and distribute the attention heads, which can also be computed independently, to different devices. The head-level partitioning approach ensures a balanced workload distribution, whereas the request-level partitioning may result in load imbalance due to the differences in sequence lengths and therefore the KV cache sizes among requests. However, head-level partitioning has limited flexibility, as it requires the number of memory devices to be divisible by the number of attention heads. We opt for head-level partitioning in Lamina, which offers optimal load balancing.\n",
      "\n",
      "## 6 Evaluation\n",
      "\n",
      "**Testbed.** We deploy Lamina on a real heterogeneous cluster with two kinds of GPU nodes. Each node consists of either eight H100 or H20 GPUs, and each GPU is paired with a dedicated ConnectX-7 NIC via PCIe switch. The GPU nodes are interconnected with 400 Gbps RoCE network. We use H100 as compute-optimized GPUs and H20 as memory-optimized GPUs for Lamina.\n",
      "            Chunk Id: 42\n",
      "            Doc ID: chen2024\n",
      "            Type: paper\n",
      "            Title: Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation\n",
      "            Year: 2024\n",
      "            Citation: Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814\n",
      "            URL: https://arxiv.org/pdf/2405.01814\n"
     ]
    }
   ],
   "source": [
    "print(Nugget(chunks[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c4fa3-0a2a-468c-94b8-0727dfa0c226",
   "metadata": {},
   "source": [
    "## Chunks all the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbdf95-e6f2-4f4c-9d11-4b85a65421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def chunk_all(fn) -> list:\n",
    "    \"\"\"Chunk contents of all docs\"\"\"\n",
    "    doc_ids = eda.metadata()['id'].tolist()\n",
    "    start_id, all_chunks = 0, fc.L()\n",
    "    for doc_id in doc_ids:\n",
    "        chunks = fn(doc_id, start_id=len(all_chunks))\n",
    "        all_chunks.extend(chunks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9345d64-1d6e-4f55-a4ff-c8910310090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1927"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks = chunk_all(chunk_doc)\n",
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93c9c7-3cdf-422c-bdc9-c5e040844fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text='Amazon \\nSustainability \\nReport\\n2023 Contents\\nOverview\\n3 Introduction\\n4 A Letter from Our Chief \\nSustainability Officer\\xa0\\n5 How We Work\\n6 Goals Summary\\n7 2023 Year in Review \\xa0\\nEnvironment\\n9 Carbon\\n24 Carbon-Free Energy\\n29 Packaging \\n34 Waste and Circularity\\n40 Water\\nValue Chain\\n45 Human Rights \\n50 Responsible Supply Chain\\n58 Sustainable Products and \\nMaterials \\n64 Supplier Diversity \\n67 Community Impact\\nPeople\\n75 Employee Experience\\n81 Health and Safety\\n86 Inclusive Experiences\\nAppendix\\n94  Sustainability Reporting Topic \\nAssessment\\n95  Endnotes\\n96 Assurance Statements \\n97 Disclaimer and Forward-Looking \\nStatements \\nOn the cover  \\nThe Baldy Mesa Solar and Storage Project (developed \\nand operated by AES), located in Adelanto, California. Employees inside one of our newest office buildings in Bellevue, \\nWashington.\\nIntroduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Letter\\nAbout This Report\\nThis is our sixth annual report detailing progress against \\nour goals\\xa0  and environmental, social, and governance \\ntopics. All financial figures are reported in U.S. dollars ($), \\nunless otherwise stated. The data within this report reflects \\nprogress from January 1 through December 31, 2023, unless \\notherwise indicated. This report includes information about \\nmany business units and subsidiaries including AWS, Devices, \\nFresh, Whole Foods Market, Amazon Private Brands, Twitch, \\nMGM Studios, and Ring.\\nOur 2023 Sustainability Report is structured into three \\nmain categories: Environment',\n",
       "          chunk_id=0,\n",
       "          id='amazon2023',\n",
       "          type='report',\n",
       "          title='2023 Amazon Sustainability Report',\n",
       "          year=2023,\n",
       "          citation='Amazon Staff. (2023). Amazon Sustainability Report. https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf',\n",
       "          url='https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acede449-ffe2-423c-a158-c9e94ecbee7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text='\\nWeidinger, L., Mellor, J., et al.: Ethical and social risks of harm from language models.\\narXiv preprint arXiv:2112.04359 (2021)\\n25',\n",
       "          chunk_id=1926,\n",
       "          id='zschache2025',\n",
       "          type='paper',\n",
       "          title='Comparing energy consumption and accuracy in text classification inference',\n",
       "          year=2025,\n",
       "          citation='Johannes Zschache, & Tilman Hartwig (2025). Comparing energy consumption and accuracy in text classification inference arXiv. https://arxiv.org/pdf/2508.14170',\n",
       "          url='https://arxiv.org/pdf/2508.14170 ')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e86e25-2051-45e6-b20b-826e9d8692c3",
   "metadata": {},
   "source": [
    "## Neighbour Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea4355-bfb4-4435-ab75-ae07eeb7cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Chunks:\n",
    "    def __init__(self, all_chunks):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def get_chunk(self, chunk_id):\n",
    "        return self.all_chunks.filter(lambda x: x.chunk_id==chunk_id)[0]\n",
    "\n",
    "    def get_neighbours(self, chunk_id):\n",
    "        left_chunk, right_chunk = None, None\n",
    "        left_chunk_id, right_chunk_id = chunk_id - 1, chunk_id + 1\n",
    "        if left_chunk_id >= 0: left_chunk = self.get_chunk(left_chunk_id)\n",
    "        if right_chunk_id < len(self.all_chunks): right_chunk = self.get_chunk(right_chunk_id)\n",
    "        return left_chunk, right_chunk\n",
    "\n",
    "    @staticmethod\n",
    "    def unique(chunks):\n",
    "        unique_chunk_ids = set()\n",
    "        ans = fc.L()\n",
    "        for c in chunks:\n",
    "            if c.chunk_id not in unique_chunk_ids:\n",
    "                unique_chunk_ids.add(c.chunk_id)\n",
    "                ans.append(c)\n",
    "        return ans\n",
    "\n",
    "    def include_neighbours(self, chunks):\n",
    "        ans = fc.L()\n",
    "        for chunk in chunks:\n",
    "            left_chunk, right_chunk = self.get_neighbours(chunk.chunk_id)\n",
    "            if left_chunk: ans.append(left_chunk)\n",
    "            ans.append(chunk)\n",
    "            if right_chunk: ans.append(right_chunk)\n",
    "        return self.unique(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb825d25-b2ac-4750-a431-98ff1854bc94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text='ne-tune the full BlackMamba model (i.e.,\\noriginal weight matrices), whereas employed QLoRA [15]\\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\\nGPU memory capacity budget. For QLoRA, we target the\\nMoE layers, including the routers, and set the rank of the\\nLoRA modules to 16. We enable FlashAttention2 [17] during\\nMixtral fine-tuning for enhanced efficiency. Moreover, we use\\ngradient checkpointing [18] to save memory usage.\\nDatasets. Our fine-tuning process is implemented in Py-\\nTorch using the LLaMA-Factory framework [19], with a\\nlearning rate of 5e-5 and 10 epochs. Both models were fine-\\ntuned on two datasets focused on different tasks: common-\\nsense 15k (CS) and Math 14k (MATH), which address com-\\nmonsense reasoning and arithmetic reasoning respectively\\n(provided by LLM-adapters [20]). The details of datasets\\nare used in Table II. For evaluation, we tested the models\\non GSM8K [21] for arithmetic reasoning and HE [22] for\\ncommonsense reasoning. Each dataset consists of thousands\\nof queries. We define a query as the concatenation of a\\nprompt and its ground-truth answer, which is feed to LLMs\\nfor fine-tuning.\\nProfiling experiments. We evaluate the fine-tuning pro-\\ncess from both software and hardware perspectives. The\\nsoftware evaluation includes an end-to-end assessment of\\nthe fine-tuning process and measures the performance of\\nthe two models on various tasks post-fine-tuning. Using\\nPyTorch, we provide essential algorithm-level information\\nsuch as test accuracy, t',\n",
       "          chunk_id=1850,\n",
       "          id='xia2024',\n",
       "          type='paper',\n",
       "          title='Understanding the Performance and Estimating the Cost of LLM Fine-Tuning',\n",
       "          year=2024,\n",
       "          citation='Yuchen Xia, Jiho Kim, Yuhan Chen, Haojie Ye, Souvik Kundu, Cong Hao, Nishil Talati. (2024). Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. arXiv. https://arxiv.org/pdf/2408.04693',\n",
       "          url='https://arxiv.org/pdf/2408.04693')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chunks(all_chunks).get_chunk(1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bcabc-29fe-431b-9cb0-3d5968d586c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [1849,1851]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_chunk, right_chunk = Chunks(all_chunks).get_neighbours(1850)\n",
    "fc.L(left_chunk, right_chunk).attrgot('chunk_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb608a-57ba-4454-993c-4e1a6601f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dup_chunks = fc.L(Chunks(all_chunks).get_chunk(cid) for cid in [1850, 1851, 1852, 1853, 1850, 1851, 1852])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfab02-890e-4761-a0ce-94bb30366315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [1850,1851,1852,1853]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chunks.unique(some_dup_chunks).attrgot('chunk_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb63dc-d114-49e8-9da6-decf06e8d482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6) [1849,1850,1851,1852,1853,1854]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = Chunks(all_chunks).include_neighbours(some_dup_chunks)\n",
    "ans.attrgot('chunk_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e6b45-8f27-412c-a519-a865770eca1f",
   "metadata": {},
   "source": [
    "## Lexical Search\n",
    "> We will use BM25 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e807cd-0585-492a-9bfc-3bb480e3f2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text='easoning modes\\n8 Discussion and Policy Implications\\n8.1 The Critical Role of Infrastructure in AI Sustainability\\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\\nDeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than\\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\\ngains can stem as much from datacenter design as from model optimization. These observations\\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\\nrenewable energy sources, and infrastructure-aware deployment strategies.\\n8.2 Rebound Effects and the Jevons Paradox\\nAlthough large language models consume significantly less energy, water, and carbon per task than\\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\\nAs p',\n",
       "          chunk_id=869,\n",
       "          id='jegham2025',\n",
       "          type='paper',\n",
       "          title='How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference',\n",
       "          year=2025,\n",
       "          citation='Nidhal Jegham, Marwan Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi (2025). How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference. arXiv. https://arxiv.org/pdf/2505.09598',\n",
       "          url='https://arxiv.org/pdf/2505.09598')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(all_chunks))\n",
    "query = all_chunks[idx].text\n",
    "all_chunks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446be0f3-b0c3-42fc-9a88-f7b87c6770ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(chunks): return all_chunks[np.random.randint(0, len(all_chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbb591-76d1-46c8-b69d-0012974b6224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eter, Compute and Data Trends in Machine Learning.https://epochai.org/data/epochdb/\\nvisualization, 2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_chunk(chunks).text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828419ae-a339-4da7-984f-97dfbcfe3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def tokenize(query): return query.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af583643-f2f7-4ddf-9678-5be2cd32e1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['easoning',\n",
       " 'modes',\n",
       " '8',\n",
       " 'discussion',\n",
       " 'and',\n",
       " 'policy',\n",
       " 'implications',\n",
       " '8.1',\n",
       " 'the',\n",
       " 'critical']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_query = tokenize(query)\n",
    "tokenized_query[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b90b9-1e47-4b5e-bd84-c3c4249296f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def bm25chunks(chunks:object) -> object: \n",
    "    \"\"\"Indexes the chunks to BM250kapi\"\"\"\n",
    "    return BM25Okapi([tokenize(t) for t in chunks.attrgot('text')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e6062-303f-40c5-b49e-574794619ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1927"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25 = bm25chunks(all_chunks)\n",
    "bm25.corpus_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660fbb8-979c-4271-b6b9-dabe9817f19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[namespace(text='easoning modes\\n8 Discussion and Policy Implications\\n8.1 The Critical Role of Infrastructure in AI Sustainability\\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\\nDeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than\\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\\ngains can stem as much from datacenter design as from model optimization. These observations\\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\\nrenewable energy sources, and infrastructure-aware deployment strategies.\\n8.2 Rebound Effects and the Jevons Paradox\\nAlthough large language models consume significantly less energy, water, and carbon per task than\\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\\nAs p',\n",
       "           chunk_id=869,\n",
       "           id='jegham2025',\n",
       "           type='paper',\n",
       "           title='How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference',\n",
       "           year=2025,\n",
       "           citation='Nidhal Jegham, Marwan Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi (2025). How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference. arXiv. https://arxiv.org/pdf/2505.09598',\n",
       "           url='https://arxiv.org/pdf/2505.09598')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(tokenized_query, all_chunks, n=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aefc93-31c2-417f-946a-4793085b3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LexicalSearch:\n",
    "    def __init__(self, chunks, tokenize_func=tokenize, neighbour_chunks=False):\n",
    "        fc.store_attr()\n",
    "        self.model = 'BM25Okapi'\n",
    "        self.bm25 = bm25chunks(chunks)\n",
    "        \n",
    "    def search(self, query, n=10):\n",
    "        ans = fc.L(self.bm25.get_top_n(self.tokenize_func(query), self.chunks, n=n))\n",
    "        if self.neighbour_chunks: ans = Chunks(self.chunks).include_neighbours(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74be7b-9b97-4753-8cc3-4a85b619c487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [NS(text='easoning modes\\n8 Discussion and Policy Implications\\n8.1 The Critical Role of Infrastructure in AI Sustainability\\nOur findings indicate that infrastructure is a crucial determinant of AI inference sustainability. While\\nmodel design enhances theoretical efficiency, real-world outcomes can substantially diverge based\\non deployment conditions and factors such as renewable energy usage and hardware efficiency.\\nFor instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more\\nenergy than GPT-4o on long queries due to reliance on older A100 GPU nodes. Similarly, DeepSeek\\nmodels highlight the profound impact of infrastructure: DeepSeek-R1 and DeepSeek-V3 deployed on\\nDeepSeek’s own servers exhibit water consumption and carbon emissions nearly six times higher than\\ntheir Azure-hosted counterparts. The Azure deployments benefit from better hardware, more efficient\\ncooling systems, lower carbon intensity, and tighter PUE control, demonstrating that sustainability\\ngains can stem as much from datacenter design as from model optimization. These observations\\nunderscore that true AI sustainability will hinge on coordinated progress in hardware efficiency,\\nrenewable energy sources, and infrastructure-aware deployment strategies.\\n8.2 Rebound Effects and the Jevons Paradox\\nAlthough large language models consume significantly less energy, water, and carbon per task than\\nhuman labor [ 75], these efficiency gains do not inherently reduce overall environmental impact.\\nAs p', chunk_id=869, id='jegham2025', type='paper', title='How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference', year=2025, citation='Nidhal Jegham, Marwan Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi (2025). How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference. arXiv. https://arxiv.org/pdf/2505.09598', url='https://arxiv.org/pdf/2505.09598')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LexicalSearch(all_chunks)\n",
    "lexical_res = ls.search(query, n=1)\n",
    "lexical_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01929f-361c-49dc-ad95-959c89852573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [868,869,870]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LexicalSearch(all_chunks, neighbour_chunks=True)\n",
    "ls.search(query, n=1).attrgot('chunk_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19a476-3053-44c1-a6d7-6986f8ac6fb3",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5b347-1422-4b3c-880a-99a4b5978c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = 'nomic-ai/nomic-embed-text-v1.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757a435-3bf7-43d8-94c7-3f5df3c36410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def embed(self:openai.OpenAI, model, texts, bs=256): \n",
    "    if type(texts) == str: texts = [texts]\n",
    "    texts_chunks = fc.chunked(texts, chunk_sz=bs)\n",
    "    data = fc.mapped(lambda o: self.embeddings.create(input=o, model=model), texts_chunks).attrgot('data')\n",
    "    return np.array(data.concat().attrgot('embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a4bbb-27ad-4d4e-a030-bde70b19d4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.fw().embed(embed_model, ['hi', 'anubhav']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fa88f-6ac5-47f3-9b8a-9229f9ed8031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1927,\n",
       " 'Amazon \\nSustainability \\nReport\\n2023 Contents\\nOverview\\n3 Introduction\\n4 A Letter from Our Chief \\nSust')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = all_chunks.attrgot('text')\n",
    "len(texts), texts[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126c4a9-b26f-4568-ba55-e1a30e9090b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = utils.fw().embed(embed_model, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a438b-df15-40a5-964b-5d37d7321c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1927, 768)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edea6fa-ba2f-4713-a794-ce71626460b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda.data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50476444-4ab9-4f61-b5e8-ddf20de8802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def embed_chunks(chunks, model='nomic-ai/nomic-embed-text-v1.5'):\n",
    "    texts = chunks.attrgot('text')\n",
    "    embeddings = utils.fw().embed(model, texts)\n",
    "    chunks_embeddings = fc.L(chunks, embeddings)\n",
    "    return chunks_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051f1fa-8796-4ad2-8d55-0b1465bd8665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1927, (1927, 768))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_embeddings = embed_chunks(all_chunks)    \n",
    "len(chunks_embeddings[0]), chunks_embeddings[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79f704-7fe4-4fd3-b382-c9cb38d468be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text=' Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language\\nUnderstanding. arXiv:1810.04805 [cs.CL]\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\\n[11] Michael Gillenwater. 2008. Redefining RECs—Part 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109–2119.\\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-\\ngoogle-cloud-region-with-the-lowest-co2\\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint\\narXiv:2006.06217 (2020).\\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\\nIEEE, 854–867.\\n[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dz',\n",
       "          chunk_id=530,\n",
       "          id='dodge2022',\n",
       "          type='paper',\n",
       "          title='Measuring the Carbon Intensity of AI in Cloud Instances',\n",
       "          year=2022,\n",
       "          citation=\"Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, Will Buchanan (2022). Measuring the Carbon Intensity of AI in Cloud Instances. FAccT '22. https://arxiv.org/pdf/2206.05229\",\n",
       "          url='https://arxiv.org/pdf/2206.05229')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_chunk = get_random_chunk(all_chunks)\n",
    "random_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c56479-efd7-4ad5-a9b3-7d2331ff0e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = utils.fw().embed(embed_model, random_chunk.text)\n",
    "query_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb58f8-539b-4070-90ab-058cb895f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "all_chunks, all_embeddings = chunks_embeddings\n",
    "scores = cosine_similarity(query_embedding, all_embeddings)\n",
    "best_k_ind = np.argsort(scores)[0].tolist()[::-1][:k]\n",
    "top_k_chunks = all_chunks[best_k_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726a0f8-f2e6-4efe-bd18-02baeefb61d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(text=' Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language\\nUnderstanding. arXiv:1810.04805 [cs.CL]\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\\n[11] Michael Gillenwater. 2008. Redefining RECs—Part 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109–2119.\\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-\\ngoogle-cloud-region-with-the-lowest-co2\\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint\\narXiv:2006.06217 (2020).\\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\\nIEEE, 854–867.\\n[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dz',\n",
       "          chunk_id=530,\n",
       "          id='dodge2022',\n",
       "          type='paper',\n",
       "          title='Measuring the Carbon Intensity of AI in Cloud Instances',\n",
       "          year=2022,\n",
       "          citation=\"Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, Will Buchanan (2022). Measuring the Carbon Intensity of AI in Cloud Instances. FAccT '22. https://arxiv.org/pdf/2206.05229\",\n",
       "          url='https://arxiv.org/pdf/2206.05229')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd235115-48b2-4e09-b0ec-f0c6433eb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SemanticSearch:\n",
    "    def __init__(self, chunks, model='nomic-ai/nomic-embed-text-v1.5', neighbour_chunks=False):\n",
    "        fc.store_attr()\n",
    "        self.chunks_embeddings = embed_chunks(chunks, model)\n",
    "\n",
    "    def search(self, query, n=1):\n",
    "        query_embedding = utils.fw().embed(self.model, query)\n",
    "        all_chunks, all_embeddings = self.chunks_embeddings\n",
    "        scores = cosine_similarity(query_embedding, all_embeddings)\n",
    "        best_k_ind = np.argsort(scores)[0].tolist()[::-1][:n]\n",
    "        ans = all_chunks[best_k_ind]\n",
    "        if self.neighbour_chunks: ans = Chunks(self.chunks).include_neighbours(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7c35d-b14d-4570-adb9-e8d44570933b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " namespace(text=' Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language\\nUnderstanding. arXiv:1810.04805 [cs.CL]\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929 (2020).\\n[10] Jim Gao. 2014. Machine learning applications for data center optimization. (2014).\\n[11] Michael Gillenwater. 2008. Redefining RECs—Part 1: untangling attributes and offsets. Energy Policy 36, 6 (2008), 2109–2119.\\n[12] Google. 2021. Carbon free energy for Google Cloud regions. https://cloud.google.com/sustainability/region-carbon\\n[13] Google. 2021. Helping you pick the greenest region for your Google Cloud resources. https://cloud.google.com/blog/topics/sustainability/pick-the-\\ngoogle-cloud-region-with-the-lowest-co2\\n[14] Abhishek Gupta, Camylle Lanteigne, and Sara Kingsley. 2020. SECure: A Social and Environmental Certificate for AI Systems. arXiv preprint\\narXiv:2006.06217 (2020).\\n[15] Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu. 2021. Chasing Carbon:\\nThe Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) .\\nIEEE, 854–867.\\n[16] K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dz',\n",
       "           chunk_id=530,\n",
       "           id='dodge2022',\n",
       "           type='paper',\n",
       "           title='Measuring the Carbon Intensity of AI in Cloud Instances',\n",
       "           year=2022,\n",
       "           citation=\"Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, Will Buchanan (2022). Measuring the Carbon Intensity of AI in Cloud Instances. FAccT '22. https://arxiv.org/pdf/2206.05229\",\n",
       "           url='https://arxiv.org/pdf/2206.05229'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SemanticSearch(all_chunks)\n",
    "semantic_res = ss.search(random_chunk.text, n=1)\n",
    "len(semantic_res), semantic_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c3458-6b59-438b-b132-77b46a209982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [529,530,531]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SemanticSearch(all_chunks, neighbour_chunks=True)\n",
    "ss.search(random_chunk.text, n=1).attrgot('chunk_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f668f5-ab74-4c0e-bf98-92105f6ea039",
   "metadata": {},
   "source": [
    "## Hybrid: Rerank\n",
    "\n",
    "Here we will rerank the outputs from semantic search and lexical search using a reranker model. \n",
    "\n",
    "There are other ways to mix the outputs from the above two searches like Reciprocal Rank Fusion (RRF), Linear Combination etc which you can try later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19beec8-0620-4ab0-ab93-d132f72cad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def combine_chunks(chunks1:list, chunks2:list) -> list:\n",
    "    \"Returns unique combination of chunks 1 and chunks 2\"\n",
    "    res = chunks1.copy()\n",
    "    seen_id = set(res.attrgot('chunk_id'))\n",
    "    for ele in chunks2:\n",
    "        if ele.chunk_id not in seen_id:\n",
    "            res.append(ele)\n",
    "            seen_id.add(ele.chunk_id)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a6857-7a2e-4347-993f-77edc0bcc9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_res = combine_chunks(semantic_res, lexical_res)\n",
    "len(combined_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bdcd2-9b5a-446f-bc90-7f7f8fd9fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [530,6]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_res.attrgot('chunk_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d7bcf-f412-442c-a468-1bad7d2e7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = utils.Reranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fa6de-d2cf-40e3-ac66-c3794058ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def rerank_chunks(self:utils.Reranker, query, chunks, n=10):\n",
    "    docs = chunks.attrgot('text')\n",
    "    ranked_ids = self.rank(query=query, docs=docs, n=n)\n",
    "    return chunks[ranked_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f34206-1ab9-4624-a180-813b71278cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Let'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617255e5-084e-481f-9cc7-9ef25c317877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Let'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_res[-1].text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50fa94-9262-4f3b-a8ff-680dfdda5a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We WorkCSO Let'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker.rerank_chunks(combined_res[-1].text, combined_res)[0].text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf62bc-f9ac-4793-8854-77c94e5d8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HybridSearch:\n",
    "    def __init__(self, lexical_search, semantic_search, neighbour_chunks=False):\n",
    "        fc.store_attr()\n",
    "        self.model = [lexical_search.model, semantic_search.model]\n",
    "        self.ranker = utils.Reranker()\n",
    "\n",
    "    def search(self, query, n=1):\n",
    "        lexical_res = self.lexical_search.search(query, n=2*n)\n",
    "        semantic_res = self.semantic_search.search(query, n=2*n)\n",
    "        combined_chunks = combine_chunks(lexical_res, semantic_res)\n",
    "        ans = self.ranker.rerank_chunks(query, combined_chunks,  n=n)\n",
    "        if self.neighbour_chunks: ans = Chunks(self.lexical_search.chunks).include_neighbours(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba3ec3-1025-4319-bc0c-6521b296ea95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23 Amazon Sustainability Report Value Chain Introduction 2023 Year in ReviewGoals SummaryHow We Work'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = HybridSearch(ls, ss)\n",
    "chunks_res = hs.search(combined_res[-1].text)\n",
    "chunks_res[0].text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee503c92-df4f-4aea-90bb-00c12116e24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [10,11,12]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = HybridSearch(ls, ss, neighbour_chunks=True)\n",
    "hs.search(combined_res[-1].text, n=1).attrgot('chunk_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c2c19-a0d1-4b65-bde0-7d7d95eace2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57d959-d7fb-4ed7-bd27-ae0cfddec104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wattbot",
   "language": "python",
   "name": "wattbot"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
